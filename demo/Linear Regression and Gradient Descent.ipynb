{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source: Sebastian Curi and Andreas Krause.\n",
    "\n",
    "# Python Notebook Commands\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Numerical Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = (10, 5)   # Change this if figures look ugly. \n",
    "\n",
    "# IPython Libraries\n",
    "import IPython\n",
    "import ipywidgets\n",
    "from ipywidgets import interact, interactive, interact_manual\n",
    "\n",
    "\n",
    "# Custom Libraries\n",
    "from utilities.load_data import polynomial_data\n",
    "from utilities import plot_helpers\n",
    "from utilities.regressors import LinearRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 100  # Number of training points.\n",
    "noise = 0.6  # Noise Level (needed for data generation).\n",
    "\n",
    "a_true = 3  # Slope.\n",
    "b_true = 1  # Intercept. \n",
    "w_true = np.array([a_true, b_true])\n",
    "\n",
    "X, Y = polynomial_data(num_points, noise, w_true)\n",
    "\n",
    "# Plot Data\n",
    "fig = plt.subplot(111);\n",
    "plot_opts = {'x_label': '$x$', 'y_label': '$y$', 'title': 'Generated Data', 'y_lim': [np.min(Y)-0.5, np.max(Y)+0.5]}\n",
    "plot_helpers.plot_data(X[:, 0], Y, fig=fig, options=plot_opts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed Form Solution:\n",
    "\n",
    "The closed form solution to the regression problem is given by:\n",
    "$$\\hat{w} = (X^\\top X)^{-1} X^T y.$$ \n",
    "\n",
    "Because there are only 2 parameters, the inverse of $X^\\top X$ is fast to do $\\approx O(d^3)$. Hence, the closed form can be computed with a total number of operations given by $\\approx O(d^3+nd^2)$ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = X.shape[1]\n",
    "reg = 0  # The regularizer is set to zero by now\n",
    "w_hat_closed_form = np.dot(np.linalg.pinv(np.dot(X.T, X) + reg * np.eye(dim)), np.dot(X.T, Y))\n",
    "fig = plt.subplot(111)\n",
    "plot_opts = {'x_label': '$x$', 'y_label': '$y$', 'title': 'Closed Form Solution', 'legend': True,\n",
    "             'y_lim': [np.min(Y)-0.5, np.max(Y)+0.5]}\n",
    "\n",
    "plot_helpers.plot_data(X[:, 0], Y, fig=fig, options=plot_opts)\n",
    "plot_helpers.plot_fit(X, w_hat_closed_form, fig=fig, options=plot_opts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Algorithm:\n",
    "\n",
    "When the number of dimensions or examples grows, the closed form solution becomes expensive to compute. Hence, the parameters $\\hat{w}$ can be updated via a gradient descent rule: \n",
    "\n",
    "$$ \\hat{w}_{k+1} \\gets \\hat{w}_k - \\eta_k \\left.\\frac{\\partial L}{\\partial w} \\right|_{w=w_k},$$\n",
    "\n",
    "where $\\eta_k$ is a parameter of the algorithm, $k$ is the iteration index, and $\\frac{\\partial L}{\\partial w}$\n",
    "\n",
    "The gradients of L with respect to the parameters are:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\hat{w}} = \\frac{1}{N} X^\\top(Xw - y) $$ \n",
    "In the *vanilla* gradient descent method, $\\eta(k)=\\eta_0$ is a constant. However other algorithms exists that modify this. We will discuss these later. \n",
    "\n",
    "The computational complexity of Gradient descent is $O(n_{\\text{iter}} \\cdot  n d)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(w0, loss_function, opts=dict()):\n",
    "    \"\"\"\n",
    "    Vanilla Gradient descent algorithm. \n",
    "    \n",
    "    w0: is the initial guess\n",
    "    loss_function: is the loss function you want to optimize. It should have the gradient method. \n",
    "    opts: a dictionary with the algorithm parameters \n",
    "    \"\"\"\n",
    "    w = w0\n",
    "    dim = w0.size\n",
    "    \n",
    "    # Parse the options. \n",
    "    eta = opts.get('eta0', 0.01)\n",
    "    n_iter = opts.get('n_iter', 10)\n",
    "    \n",
    "    # Trajectory of iterates w\n",
    "    trajectory = np.zeros((n_iter + 1, dim))\n",
    "    trajectory[0, :] = w\n",
    "    \n",
    "    for it in range(n_iter):\n",
    "        # Compute Gradient\n",
    "        gradient = loss_function.gradient(w)\n",
    "        \n",
    "        # Perform gradient step.\n",
    "        w = w - eta * gradient\n",
    "\n",
    "        # Save weights.\n",
    "        trajectory[it + 1, :] = w\n",
    "\n",
    "    return trajectory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_widget = ipywidgets.FloatSlider(value=1e-1, min=1e-1, max=2, step=1 * 1e-1, readout_format='.1f',\n",
    "                                   description='Learning rate:', style={'description_width': 'initial'},\n",
    "                                   continuous_update=False)\n",
    "n_iter_widget = ipywidgets.IntSlider(value=10, min=5, max=20, step=1, description='Number of iterations:',\n",
    "                                     style={'description_width': 'initial'}, continuous_update=False)\n",
    "\n",
    "def change_learning_params_gradient_descent(eta0, n_iter):\n",
    "    regressor = LinearRegressor(X, Y)\n",
    "    w0 = np.array([0., 0.])\n",
    "    opts = {'eta0': eta0, 'n_iter': n_iter}\n",
    "    trajectory = gradient_descent(w0, regressor, opts)\n",
    "\n",
    "    contourplot = plt.subplot(121)\n",
    "    dataplot = plt.subplot(122)\n",
    "    contour_opts = {'x_label': '$w_0$', 'y_label': '$w_1$', 'title': 'Weight trajectory', 'legend': False,\n",
    "                   }\n",
    "    data_opts = {'x_label': '$x$', 'y_label': '$y$', 'title': 'Regression trajectory', 'legend': False, \n",
    "                'y_lim': [np.min(Y)-0.5, np.max(Y)+0.5], 'sgd_point': False}\n",
    "    plot_opts = {'contour_opts': contour_opts, 'data_opts': data_opts}\n",
    "\n",
    "    plot_helpers.linear_regression_progression(X, Y, trajectory, np.arange(regressor.number_samples), \n",
    "                                               regressor.test_loss,\n",
    "                                               contourplot, dataplot, options=plot_opts)\n",
    "\n",
    "interact_manual(change_learning_params_gradient_descent, eta0=lr_widget, n_iter=n_iter_widget);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent:\n",
    "\n",
    "To compute the gradient in stochastic descent, the gradient for each data point must be individually computed and then add them up together. However, each of the data points provides an unbiased estimate (albiet with higher variance) of the expected value of the gradient, i. e.\n",
    "\n",
    "$$ \\mathbb{E}\\left[\\frac{\\partial L(x,y)}{\\partial \\hat{w}}\\right] \\approx \\frac{\\partial L(x_i, y_i)}{\\partial \\hat{w}} = x_i (x_i^\\top \\hat{w} - y_i). $$ \n",
    "\n",
    "As the gradient depends only on one sample, it is much more noisy and more iterations are needed for convergence. The computational complexity is $O(n_{\\text{iter}} \\cdot d)$. \n",
    "\n",
    "As each gradient step contains significant noise, a desirable property of the algorithm is that the learning rate becomes smaller as it is closer to convergence. If the sequence $eta_k$ satisfy Robbins-Monro condtion,  $\\sum_{k=0}^\\infty \\eta_k = \\infty$, $\\sum_{k=0}^\\infty \\eta_k^2 < \\infty$ then stochastic gradient descent is guaranteed to converge. For example $\\eta_k = \\frac{\\eta_0}{(k+1)^{0.6}}$ satisfy this requirements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(w0, loss_function, opts=dict()):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient descent algorithm. \n",
    "    \n",
    "    w0: is the initial guess\n",
    "    loss_function: is the loss function you want to optimize. It should have the gradient method. \n",
    "    opts: a dictionary with the algorithm parameters \n",
    "    \"\"\"\n",
    "    w = w0\n",
    "    dim = w0.size\n",
    "    \n",
    "    # Parse the options. \n",
    "    eta = opts.get('eta0', 0.01)\n",
    "    n_iter = opts.get('n_iter', 10)\n",
    "    \n",
    "    # Trajectory of iterates w\n",
    "    trajectory = np.zeros((n_iter + 1, dim))\n",
    "    trajectory[0, :] = w\n",
    "    \n",
    "    # Trajectory of indexes\n",
    "    index_traj = np.zeros((n_iter, 1), dtype=np.int)\n",
    "    \n",
    "    for it in range(n_iter):\n",
    "        # Sample index.\n",
    "        i = np.random.choice(loss_function.number_samples, 1)\n",
    "        index_traj[it, :] = i\n",
    "\n",
    "        # Compute Gradient.\n",
    "        gradient = loss_function.gradient(w, i) # The gradient is only with respect to this point. \n",
    "\n",
    "        # Perform gradient step.\n",
    "        w = w - eta * gradient\n",
    "\n",
    "        # Save weights.  \n",
    "        trajectory[it + 1, :] = w\n",
    "\n",
    "    return trajectory, index_traj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_widget = ipywidgets.FloatSlider(value=1e-1, min=1e-1, max=2, step=1 * 1e-1, readout_format='.1f',\n",
    "                                   description='Learning rate:', style={'description_width': 'initial'},\n",
    "                                   continuous_update=False)\n",
    "n_iter_widget = ipywidgets.IntSlider(value=20, min=5, max=200, step=1, description='Number of iterations:',\n",
    "                                     style={'description_width': 'initial'}, continuous_update=False)\n",
    "\n",
    "def change_learning_params_sgd(eta0, n_iter):\n",
    "    regressor = LinearRegressor(X, Y)\n",
    "    w0 = np.array([0., 0.])\n",
    "\n",
    "    opts = {'eta0': eta0,\n",
    "            'n_iter': n_iter,\n",
    "            }\n",
    "    trajectory, indexes = stochastic_gradient_descent(w0, regressor, opts)\n",
    "        \n",
    "    contourplot = plt.subplot(121)\n",
    "    dataplot = plt.subplot(122)\n",
    "    contour_opts = {'x_label': '$w_0$', 'y_label': '$w_1$', 'title': 'Weight trajectory', 'legend': False}\n",
    "    data_opts = {'x_label': '$x$', 'y_label': '$y$', 'title': 'Regression trajectory', 'legend': False,\n",
    "                'y_lim': [np.min(Y)-0.5, np.max(Y)+0.5], 'sgd_point': True}\n",
    "    plot_opts = {'contour_opts': contour_opts, 'data_opts': data_opts}\n",
    "\n",
    "    plot_helpers.linear_regression_progression(X, Y, trajectory, indexes, regressor.test_loss,\n",
    "                                               contourplot, dataplot, options=plot_opts)\n",
    "\n",
    "\n",
    "interact_manual(change_learning_params_sgd, eta0=lr_widget, n_iter=n_iter_widget);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent:\n",
    "\n",
    "A trade-off between full gradient descent (higher cost, lower variance), and stochastic gradient descent (lower cost, higher variance), is done by computing the gradient of a mini-batch of points i.e.\n",
    "$$ \\mathbb{E}\\left[\\frac{\\partial L(x,y)}{\\partial \\hat{w}}\\right] \\approx \\frac{1}{|B|} \\sum_{i\\in B} \\frac{\\partial L(x_i, y_i)}{\\partial \\hat{w}} = \\frac{1}{|B|} \\sum_{i\\in B} x_i (x_i^\\top \\hat{w} - y_i). $$ \n",
    "\n",
    "The computational complexity is $O(n_{\\text{iter}} \\cdot |B| d)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gradient_descent(w0, loss_function, opts=dict()):\n",
    "    \"\"\"\n",
    "    Mini-Batch Stochastic Gradient descent algorithm. \n",
    "    \n",
    "    w0: is the initial guess\n",
    "    loss_function: is the loss function you want to optimize. It should have the gradient method. \n",
    "    opts: a dictionary with the algorithm parameters \n",
    "    \"\"\"\n",
    "    w = w0\n",
    "    dim = w0.size\n",
    "    \n",
    "    # Parse the options. \n",
    "    eta = opts.get('eta0', 0.01)\n",
    "    n_iter = opts.get('n_iter', 10)\n",
    "    batch_size = opts.get('batch_size', 1)\n",
    "    \n",
    "    # Trajectory of iterates w\n",
    "    trajectory = np.zeros((n_iter + 1, dim))\n",
    "    trajectory[0, :] = w\n",
    "    \n",
    "    # Trajectory of indexes\n",
    "    index_traj = np.zeros((n_iter, batch_size), dtype=np.int)\n",
    "    \n",
    "    for it in range(n_iter):\n",
    "        # Sample index.\n",
    "        i = np.random.choice(loss_function.number_samples, batch_size, replace=True)\n",
    "        # If replace=False the estimate becomes BIASED, but its still used in practice when the dataset is large. \n",
    "        # Instead of using np.random.choice, the index set is shuffled and then iterate over this shuffled \n",
    "        # version sequentially. \n",
    "        \n",
    "        index_traj[it, :] = i\n",
    "\n",
    "        # Compute Gradient.\n",
    "        gradient = loss_function.gradient(w, i) # The gradient is only with respect to this point. \n",
    "\n",
    "        # Perform gradient step.\n",
    "        w = w - eta * gradient\n",
    "\n",
    "        # Save weights.  \n",
    "        trajectory[it + 1, :] = w\n",
    "\n",
    "    return trajectory, index_traj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_widget = ipywidgets.FloatSlider(value=1, min=1e-1, max=2, step=1 * 1e-1, readout_format='.1f',\n",
    "                                   description='Learning rate:', style={'description_width': 'initial'},\n",
    "                                   continuous_update=False)\n",
    "n_iter_widget = ipywidgets.IntSlider(value=10, min=5, max=200, step=1, description='Number of iterations:',\n",
    "                                     style={'description_width': 'initial'}, continuous_update=False)\n",
    "bs_widget = ipywidgets.IntSlider(value=1, min=1, max=100, step=1, description='Batch Size:',\n",
    "                                 style={'description_width': 'initial'}, continuous_update=False)\n",
    "def change_batch_size(eta0, n_iter, batch_size):\n",
    "    regressor = LinearRegressor(X, Y)\n",
    "    w0 = np.array([0., 0.])\n",
    "\n",
    "    opts = {'eta0': eta0,\n",
    "            'n_iter': n_iter,\n",
    "            'batch_size': batch_size,\n",
    "            }\n",
    "    trajectory, indexes = mini_batch_gradient_descent(w0, regressor, opts)\n",
    "\n",
    "    contourplot = plt.subplot(121)\n",
    "    dataplot = plt.subplot(122)\n",
    "    contour_opts = {'x_label': '$w_0$', 'y_label': '$w_1$', 'title': 'Weight trajectory', 'legend': False}\n",
    "    data_opts = {'x_label': '$x$', 'y_label': '$y$', 'title': 'Regression trajectory', 'legend': False,\n",
    "                'y_lim': [np.min(Y)-0.5, np.max(Y)+0.5], 'sgd_point': True}\n",
    "    plot_opts = {'contour_opts': contour_opts, 'data_opts': data_opts}\n",
    "\n",
    "    plot_helpers.linear_regression_progression(X, Y, trajectory, indexes, regressor.test_loss,\n",
    "                                               contourplot, dataplot, options=plot_opts)\n",
    "\n",
    "interact_manual(change_batch_size, eta0=lr_widget, n_iter=n_iter_widget, batch_size=bs_widget);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristics to change learning rate\n",
    "\n",
    "* Bold driver heuristic:\n",
    "\n",
    "    If $L(x, y, w_k) > L(x, y, w_{k+1})$, \n",
    "    \n",
    "    $\\eta_{k+1} = \\eta_{k}/5$\n",
    "    \n",
    "    else\n",
    "    \n",
    "    $\\eta_{k+1} \\gets \\eta_{k}*1.1$\n",
    "\n",
    "    \n",
    "* AdaGrad:\n",
    "\n",
    "    $\\eta_k = \\frac{\\eta_0}{\\sqrt{\\sum_{j=0}^k g_j^2}}$, where $g_j$ is the 2-norm gradient of $L$ at time $j$. \n",
    "\n",
    "\n",
    "* Annealing (From Stochastic Approximation) \n",
    "\n",
    "    $\\eta_{k} = \\frac{\\eta_0}{(k+1)^\\alpha}$, with $\\alpha \\in (0.5, 1]$\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_learning_rate(eta, opts=dict()):\n",
    "    learning_rate_scheduling = opts.get('learning_rate_scheduling', None)\n",
    "    eta0 = opts.get('eta0', eta)\n",
    "    f_increased = opts.get('f_increased', False)\n",
    "    grad_sum = opts.get('grad_sum', 0)\n",
    "    it = opts.get('it', 0)\n",
    "    if learning_rate_scheduling == None:\n",
    "        eta = eta0  # keep it constant. \n",
    "    elif learning_rate_scheduling == 'Annealing':\n",
    "        eta = eta0 / np.power(it + 1, 0.6)\n",
    "    elif learning_rate_scheduling == 'Bold driver':\n",
    "        eta = (eta / 5) if (f_increased) else (eta * 1.1)\n",
    "    elif learning_rate_scheduling == 'AdaGrad':\n",
    "        eta = eta0 / np.sqrt(grad_sum)\n",
    "    elif learning_rate_scheduling == 'Annealing2':\n",
    "        eta = min([eta0, 100. / (it + 1.)])\n",
    "    else:\n",
    "        raise ValueError('Learning rate scheduling {} not understood'.format(method))\n",
    "    return eta\n",
    "\n",
    "\n",
    "def gradient_descent(w0, loss_function, opts=dict()):\n",
    "    \"\"\"\n",
    "    Mini-Batch Stochastic Gradient descent algorithm. \n",
    "    \n",
    "    w0: is the initial guess\n",
    "    loss_function: is the loss function you want to optimize. It should have the gradient and loss method. \n",
    "    opts: a dictionary with the algorithm parameters \n",
    "    \"\"\"\n",
    "    w = w0\n",
    "    dim = w0.size\n",
    "    \n",
    "    # Parse the options. \n",
    "    eta = opts.get('eta0', 0.01)\n",
    "    eta0 = eta\n",
    "    n_iter = opts.get('n_iter', 10)\n",
    "    batch_size = opts.get('batch_size', 1)\n",
    "    learning_rate_scheduling = opts.get('learning_rate_scheduling', None) \n",
    "    \n",
    "    # Trajectory of iterates w\n",
    "    trajectory = np.zeros((n_iter + 1, dim))\n",
    "    trajectory[0, :] = w\n",
    "    \n",
    "    # Trajectory of indexes\n",
    "    index_traj = np.zeros((n_iter, batch_size), dtype=np.int)\n",
    "    \n",
    "    # Needed for learning rate schedule. \n",
    "    f_val = loss_function.loss(w)\n",
    "    f_old = f_val\n",
    "    grad_sum = 0\n",
    "    \n",
    "    for it in range(n_iter):\n",
    "        # Sample index.\n",
    "        i = np.random.choice(loss_function.number_samples, batch_size, replace=True)\n",
    "        # If replace=False the estimate becomes BIASED, but its still used in practice when the dataset is large. \n",
    "        # Instead of using np.random.choice, the index set is shuffled and then iterate over this shuffled \n",
    "        # version sequentially. \n",
    "        \n",
    "        index_traj[it, :] = i\n",
    "\n",
    "        # Compute Gradient.\n",
    "        gradient = loss_function.gradient(w, i) # The gradient is only with respect to this point. \n",
    "        grad_sum += np.sum(np.square(gradient)) # For AdaGrad\n",
    "        \n",
    "        # Update learning rate.\n",
    "        learning_rate_opts = {'learning_rate_scheduling': learning_rate_scheduling,\n",
    "                              'eta0': eta0,\n",
    "                              'it': it,\n",
    "                              'f_increased': (f_val > f_old),\n",
    "                              'grad_sum': grad_sum}\n",
    "        # Note it is recursive but eta0 is passed in the options. \n",
    "        eta = compute_learning_rate(eta, learning_rate_opts)  \n",
    "        \n",
    "        # Perform gradient step.\n",
    "        w = w - eta * gradient\n",
    "\n",
    "        # Save weights.  \n",
    "        trajectory[it + 1, :] = w\n",
    "        \n",
    "        # Compute new cost.\n",
    "        f_old = f_val\n",
    "        f_val = loss_function.loss(w)\n",
    "\n",
    "    return trajectory, index_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule_widget = ipywidgets.RadioButtons(options=['Bold driver', 'AdaGrad', 'Annealing', 'None'], \n",
    "                                          value='Bold driver',  description='Learning rate heuristics:', \n",
    "                                          style={'description_width': 'initial'})\n",
    "\n",
    "def change_heuristics(learning_rate_scheduling=None):\n",
    "    regressor = LinearRegressor(X, Y)\n",
    "    w0 = np.array([0., 0.])\n",
    "    if learning_rate_scheduling == 'None':\n",
    "        learning_rate_scheduling = None\n",
    "\n",
    "    opts = {'eta0': 2,\n",
    "            'n_iter': 10,\n",
    "            'batch_size': 16,\n",
    "            'n_samples': X.shape[0],\n",
    "            'learning_rate_scheduling': learning_rate_scheduling\n",
    "            }\n",
    "    trajectory, indexes = gradient_descent(w0, regressor, opts)\n",
    "\n",
    "    contourplot = plt.subplot(121)\n",
    "    dataplot = plt.subplot(122)\n",
    "    contour_opts = {'x_label': '$w_0$', 'y_label': '$w_1$', 'title': 'Weight trajectory', 'legend': False}\n",
    "    data_opts = {'x_label': '$x$', 'y_label': '$y$', 'title': 'Regression trajectory', 'legend': False,\n",
    "                'y_lim': [np.min(Y)-0.5, np.max(Y)+0.5]}\n",
    "    plot_opts = {'contour_opts': contour_opts, 'data_opts': data_opts}\n",
    "\n",
    "    plot_helpers.linear_regression_progression(X, Y, trajectory, indexes, regressor.test_loss,\n",
    "                                               contourplot, dataplot, options=plot_opts)\n",
    "\n",
    "interact_manual(change_heuristics, learning_rate_scheduling=schedule_widget);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
