{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import ipywidgets\n",
    "from ipywidgets import interact, interactive, interact_manual\n",
    "import IPython\n",
    "from IPython.display import display, clear_output\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10, 5)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utilities.util import gradient_descent\n",
    "from utilities.load_data import linear_separable_data, circular_separable_data\n",
    "from utilities import plot_helpers \n",
    "from utilities.classifiers import Perceptron, SVM, Logistic\n",
    "from utilities.regularizers import L1Regularizer, L2Regularizer\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "import sklearn\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import Lasso, SGDClassifier, Ridge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def laplacian_kernel(X, Y, bw):\n",
    "    rows = X.shape[0]\n",
    "    cols = Y.shape[0]\n",
    "    K = np.zeros((rows, cols))\n",
    "    for col in range(cols):\n",
    "        dist = bw * np.linalg.norm(X - Y[col, :], ord=1, axis=1)\n",
    "        K[:, col] = np.exp(-dist)\n",
    "    return K\n",
    "\n",
    "# Our dataset and targets\n",
    "n_samples = 200  # Number of points per class\n",
    "tol = 1e-1\n",
    "\n",
    "def svm_features(dataset, features, reg, bw, deg, noise):\n",
    "    if dataset is 'blobs':\n",
    "        X, Y = datasets.make_blobs(n_samples=n_samples, centers=2, random_state=3, cluster_std=10*noise)\n",
    "    elif dataset is 'circles':\n",
    "        X, Y = datasets.make_circles(n_samples=n_samples, factor=.5, noise=noise, random_state=42)\n",
    "    elif dataset is 'moons':\n",
    "        X, Y = datasets.make_moons(n_samples=n_samples, noise=noise, random_state=42)\n",
    "    elif dataset == 'xor':\n",
    "        np.random.seed(42)\n",
    "        step = int(n_samples/4)\n",
    "        \n",
    "        X = np.zeros((n_samples, 2))\n",
    "        Y = np.zeros(n_samples)\n",
    "        \n",
    "        X[0*step:1*step, :] = noise * np.random.randn(step, 2)\n",
    "        Y[0*step:1*step] = 1\n",
    "        X[1*step:2*step, :] = np.array([1, 1]) + noise * np.random.randn(step, 2)\n",
    "        Y[1*step:2*step] = 1\n",
    "        \n",
    "        X[2*step:3*step, :] = np.array([0, 1]) + noise * np.random.randn(step, 2)\n",
    "        Y[2*step:3*step] = -1\n",
    "        X[3*step:4*step, :] = np.array([1, 0]) + noise * np.random.randn(step, 2)\n",
    "        Y[3*step:4*step] = -1\n",
    "    \n",
    "    elif dataset == 'periodic':\n",
    "        np.random.seed(42)\n",
    "        step = int(n_samples/4)\n",
    "        \n",
    "        X = np.zeros((n_samples, 2))\n",
    "        Y = np.zeros(n_samples)\n",
    "        \n",
    "        X[0*step:1*step, :] = noise * np.random.randn(step, 2)\n",
    "        Y[0*step:1*step] = 1\n",
    "        X[1*step:2*step, :] = np.array([0, 2]) + noise * np.random.randn(step, 2)\n",
    "        Y[1*step:2*step] = 1\n",
    "        \n",
    "        X[2*step:3*step, :] = np.array([0, 1]) + noise * np.random.randn(step, 2)\n",
    "        Y[2*step:3*step] = -1\n",
    "        X[3*step:4*step, :] = np.array([0, 3]) + noise * np.random.randn(step, 2)\n",
    "        Y[3*step:4*step] = -1\n",
    "        \n",
    "    X = X[Y <= 1, :]\n",
    "    Y = Y[Y <=1 ]\n",
    "    Y[Y==0] = -1\n",
    "        \n",
    "    # Add the 1 feature.  \n",
    "    X = np.concatenate((X, np.ones((X.shape[0], 1))), axis=1)\n",
    "    plot_support = True\n",
    "    kernel = features\n",
    "    if kernel == 'poly':\n",
    "        gamma = 1\n",
    "        coef0 = 0\n",
    "    elif kernel == 'sigmoid':\n",
    "        gamma = np.power(10., bw)\n",
    "        coef0 = 0\n",
    "    elif kernel == 'rbf':\n",
    "        gamma = np.power(10., -bw)\n",
    "        coef0 = 0\n",
    "    elif kernel == 'laplacian':\n",
    "        gamma = np.power(10., -bw)\n",
    "        coef0 = 0\n",
    "        kernel = lambda X, Y: laplacian_kernel(X, Y, gamma)\n",
    "        plot_support = False\n",
    "\n",
    "    classifier = svm.SVC(kernel=kernel, C=np.power(10., -reg), gamma=gamma, degree=deg, coef0=coef0, tol=tol)\n",
    "    classifier.fit(X, Y)\n",
    "\n",
    "    # plot the line, the points, and the nearest vectors to the plane\n",
    "    plt.figure()\n",
    "    plt.clf()\n",
    "    fig = plt.axes()\n",
    "    opt = {'marker': 'r*', 'label': '+'}\n",
    "    plot_helpers.plot_data(X[np.where(Y == 1)[0], 0], X[np.where(Y == 1)[0], 1], fig=fig, options=opt)\n",
    "    opt = {'marker': 'bs', 'label': '-'}\n",
    "    plot_helpers.plot_data(X[np.where(Y == -1)[0], 0], X[np.where(Y == -1)[0], 1], fig=fig, options=opt)\n",
    "    \n",
    "    if plot_support:\n",
    "        plt.scatter(classifier.support_vectors_[:, 0], classifier.support_vectors_[:, 1], s=80,\n",
    "                    facecolors='none', edgecolors='k')\n",
    "\n",
    "    mins = np.min(X, 0)\n",
    "    maxs = np.max(X, 0)\n",
    "    x_min = mins[0] - 1\n",
    "    x_max = maxs[0] + 1\n",
    "    y_min = mins[1] - 1\n",
    "    y_max = maxs[1] + 1\n",
    "\n",
    "    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]  \n",
    "    Xtest = np.c_[XX.ravel(), YY.ravel(), np.ones_like(XX.ravel())]\n",
    "    Z = classifier.decision_function(Xtest)\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(XX.shape)\n",
    "    plt.contourf(XX, YY, Z > 0, cmap=plt.cm.jet, alpha=0.3)\n",
    "    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'], levels=[-.99, 0, .99])\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "\n",
    "\n",
    "interact(svm_features, \n",
    "         dataset=['blobs', 'circles', 'moons', 'xor', 'periodic'],\n",
    "         features=['poly', 'rbf', 'laplacian'], \n",
    "         reg=ipywidgets.FloatSlider(value=-3,\n",
    "                                    min=-3,\n",
    "                                    max=3,\n",
    "                                    step=0.5,\n",
    "                                    readout_format='.1f',\n",
    "                                    description='Regularization 10^:',\n",
    "                                    style={'description_width': 'initial'},\n",
    "                                    continuous_update=False),\n",
    "         bw=ipywidgets.FloatSlider(value=-1,\n",
    "                                    min=-3,\n",
    "                                    max=3,\n",
    "                                    step=0.1,\n",
    "                                    readout_format='.1f',\n",
    "                                    description='Bandwidth 10^:',\n",
    "                                    style={'description_width': 'initial'},\n",
    "                                    continuous_update=False),  \n",
    "         deg=ipywidgets.IntSlider(\n",
    "                         value=1,\n",
    "                         min=1,\n",
    "                         max=10, \n",
    "                         step=1,\n",
    "                         description='Degree of Poly:',\n",
    "                         style={'description_width': 'initial'}),\n",
    "         noise=ipywidgets.FloatSlider(value=0.05,\n",
    "                                    min=0.01,\n",
    "                                    max=0.3,\n",
    "                                    step=0.01,\n",
    "                                    readout_format='.2f',\n",
    "                                    description='Noise level:',\n",
    "                                    style={'description_width': 'initial'},\n",
    "                                    continuous_update=False),  \n",
    "        );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Selection\n",
    "\n",
    "Data are generated according to the following generative model: $$x \\sim \\mathcal{N}(0, 1), \\qquad y = 1 + x + 0.3 \\sin(10x).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "num_points = 100\n",
    "noise = 0.\n",
    "train_fraction = 0.7  # Train/Validation split = 0.7/0.3\n",
    "\n",
    "X = np.random.randn(num_points)\n",
    "Y = 1 + X + 0.3 * np.sin(10 * X) + noise * np.random.randn(num_points)\n",
    "Xtest = np.linspace(-3, 3, 100)\n",
    "Ytest = 1 + Xtest + 0.3 * np.sin(10 * Xtest)  # Noiseless\n",
    "\n",
    "\n",
    "# Split Train into train and validation.\n",
    "idx = np.arange(0, num_points)\n",
    "np.random.shuffle(idx)\n",
    "num_train = int(train_fraction * num_points)\n",
    "\n",
    "Xtrain, Ytrain = X[idx[:num_train]], Y[idx[:num_train]]\n",
    "Xval, Yval = X[idx[num_train:]], Y[idx[num_train:]]\n",
    "\n",
    "\n",
    "fig = plt.subplot(111);\n",
    "plot_opts = {'x_label': '$x$', 'y_label': '$y$', 'title': 'Generated Data', 'y_lim': [np.min(Y)-0.5, np.max(Y)+0.5]}\n",
    "plot_helpers.plot_data(X, Y, fig=fig, options=plot_opts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Features:\n",
    "We define polynomials $x^p$, with $p \\in \\left\\{0, 1, 2, 3 \\right\\}$; sine functions $\\sin(\\omega x)$, with $\\omega \\in \\left\\{1, 2, 5, 10 \\right\\}$; and exponential functions $e^{\\alpha x}$, with $\\alpha \\in \\left\\{1, 2, 5, 10 \\right\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features \n",
    "poly = [] \n",
    "sin = []\n",
    "exp = []\n",
    "for i in [0, 1, 2, 3]:\n",
    "    poly.append(lambda x, n=i: x**n)\n",
    "for i in [1, 2, 5, 10]: \n",
    "    sin.append(lambda x, n=i: np.sin(n * x))\n",
    "    exp.append(lambda x, n=i: np.exp(n * x))\n",
    "               \n",
    "def build_features(x, feature_list):\n",
    "    func_list = parse_list(feature_list)\n",
    "    return np.atleast_2d(np.stack( tuple(feature(x) for feature in func_list) )).T\n",
    "\n",
    "def get_coefficient(x_train, y_train, feature_list):\n",
    "    phi_train = build_features(x_train, feature_list)    \n",
    "    dim = phi_train.shape[-1]\n",
    "    w_hat = np.dot(np.linalg.pinv(np.dot(phi_train.T, phi_train) + 1e-16 * np.eye(dim)), \n",
    "                   np.dot(phi_train.T, y_train))\n",
    "    return w_hat\n",
    "\n",
    "def evaluate_features(x_test, y_test, feature_list, w_hat):\n",
    "    phi_test = build_features(x_test, feature_list)\n",
    "    error = np.power(y_test - phi_test @ w_hat, 2).mean()\n",
    "    return error \n",
    "\n",
    "def cross_validation(x, y, feature_list, num_folds=5):\n",
    "    num_points = len(x)\n",
    "    \n",
    "    idx = np.arange(num_points)\n",
    "    folds = np.split(idx, num_folds)  # Careful, this must have equal division.\n",
    "    error = 0 \n",
    "    for i_fold in range(num_folds):\n",
    "        train_idx = np.concatenate(tuple(folds[i] for i in range(num_folds) if i != i_fold))\n",
    "        val_idx = folds[i_fold]\n",
    "        \n",
    "        x_train, y_train = x[train_idx], y[train_idx]\n",
    "        x_val, y_val = x[val_idx], y[val_idx]\n",
    "        \n",
    "        w_hat = get_coefficient(x_train, y_train, feature_list)\n",
    "        error += evaluate_features(x_val, y_val, feature_list, w_hat)\n",
    "    \n",
    "    return error / num_folds\n",
    "\n",
    "\n",
    "def parse_list(feature_list):\n",
    "    \n",
    "    ans = []\n",
    "    if 'Constant' in feature_list:\n",
    "        ans.append(poly[0])\n",
    "    if 'Linear' in feature_list: \n",
    "        ans.append(poly[1])\n",
    "    if 'Squared' in feature_list: \n",
    "        ans.append(poly[2])\n",
    "    if 'Cubic' in feature_list: \n",
    "        ans.append(poly[3])\n",
    "    \n",
    "    if 'Sin(x)' in feature_list:\n",
    "        ans.append(sin[0])\n",
    "    if 'Sin(2x)' in feature_list: \n",
    "        ans.append(sin[1])\n",
    "    if 'Sin(5x)' in feature_list: \n",
    "        ans.append(sin[2])\n",
    "    if 'Sin(10x)' in feature_list: \n",
    "        ans.append(sin[3])\n",
    "    \n",
    "    if 'Exp(x)' in feature_list:\n",
    "        ans.append(exp[0])\n",
    "    if 'Exp(2x)' in feature_list: \n",
    "        ans.append(exp[1])\n",
    "    if 'Exp(5x)' in feature_list: \n",
    "        ans.append(exp[2])\n",
    "    if 'Exp(10x)' in feature_list: \n",
    "        ans.append(exp[3])\n",
    "        \n",
    "    return ans \n",
    "\n",
    "features=['Constant', 'Linear', 'Squared', 'Cubic',\n",
    "         'Sin(x)', 'Sin(2x)', 'Sin(5x)', 'Sin(10x)',\n",
    "         'Exp(x)', 'Exp(2x)', 'Exp(5x)', 'Exp(10x)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection by Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_widget = ipywidgets.SelectMultiple(options=features,\n",
    "    value=['Constant'],\n",
    "    rows=len(features),\n",
    "    description='Features',\n",
    "    disabled=False\n",
    ")\n",
    "def feature_selection(feature_list):\n",
    "    print(feature_list)\n",
    "    w_hat = get_coefficient(Xtrain, Ytrain, feature_list)\n",
    "    \n",
    "    \n",
    "    fig = plt.subplot(111);\n",
    "    plot_opts = {'x_label': '$x$', 'y_label': '$y$', 'title': 'Generated Data', 'y_lim': [np.min(Y)-0.5, np.max(Y)+0.5]}\n",
    "    plot_helpers.plot_data(X, Y, fig=fig, options=plot_opts)\n",
    "    \n",
    "    Phival = build_features(Xval, feature_list)\n",
    "    Phitest = build_features(Xtest, feature_list)\n",
    "    plt.plot(Xtest, Phitest @ w_hat, 'r-')\n",
    "    \n",
    "    print('Coefficients:', w_hat)\n",
    "    print('Validation Error:', evaluate_features(Xval, Yval, feature_list, w_hat))\n",
    "    print('Test MSE Error:', evaluate_features(Xtest, Ytest, feature_list, w_hat))\n",
    "    \n",
    "        \n",
    "interact(feature_selection, feature_list=feature_widget);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Greedy Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Generate data\n",
    "num_points = 100\n",
    "noise = 0.\n",
    "num_folds = 5\n",
    "\n",
    "X = np.random.randn(num_points)\n",
    "Y = 1 + X + 0.3 * np.sin(10 * X) + noise * np.random.randn(num_points)\n",
    "Xtest = np.linspace(-3, 3, 100)\n",
    "Ytest = 1 + Xtest + 0.3 * np.sin(10 * Xtest)  # Noiseless\n",
    "\n",
    "# Define features\n",
    "features = {'Constant', 'Linear', 'Squared', 'Cubic', \n",
    "            'Sin(x)', 'Sin(2x)', 'Sin(5x)', 'Sin(10x)',\n",
    "            'Exp(x)', 'Exp(2x)', 'Exp(5x)', 'Exp(10x)'}\n",
    "\n",
    "feature_set = set()\n",
    "num_features = 0 \n",
    "valid_error = float('+Inf')\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    global features, feature_set, num_features, valid_error\n",
    "    best_feature = None\n",
    "    best_feature_error = float('+Inf')\n",
    "    \n",
    "    # Find Best Feature\n",
    "    for feature in features:\n",
    "        feature_set.add(feature)\n",
    "        w_hat = get_coefficient(X, Y, feature_set)\n",
    "        current_error = cross_validation(X, Y, feature_set, num_folds)\n",
    "        \n",
    "        if current_error < best_feature_error:\n",
    "            best_feature = feature \n",
    "            best_feature_error = current_error\n",
    "        \n",
    "        feature_set.remove(feature) # Remove feature \n",
    "    \n",
    "    # If validation error increases\n",
    "    if best_feature_error > valid_error or len(features) == 0:\n",
    "        print('Finished! Feature set:', feature_set)\n",
    "        print('Validation error:', valid_error) \n",
    "        w_hat = get_coefficient(X, Y, feature_set)\n",
    "        print('Coefficients', w_hat)\n",
    "    else:\n",
    "        # Add feature to feature set. \n",
    "        num_features += 1 \n",
    "        feature_set.add(best_feature)\n",
    "        valid_error = best_feature_error\n",
    "        features.remove(best_feature)\n",
    "\n",
    "        Phitest = build_features(Xtest, feature_set)        \n",
    "        w_hat = get_coefficient(X, Y, feature_set)\n",
    "        \n",
    "        plt.clf()\n",
    "        clear_output()\n",
    "        display(button)\n",
    "        plot_opts = {'x_label': '$x$', 'y_label': '$y$', 'title': 'Num Features {}'.format(num_features), 'y_lim': [np.min(Y)-0.5, np.max(Y)+0.5]}\n",
    "        plot_helpers.plot_data(X, Y, options=plot_opts)\n",
    "        plt.plot(Xtest, Phitest @ w_hat, 'r-')\n",
    "        print('Added Feature', best_feature)\n",
    "        print('Feature set:', list(feature_set))\n",
    "        print('Coefficients', w_hat)\n",
    "        \n",
    "\n",
    "button = ipywidgets.Button(description=\"Next Feature\")\n",
    "button.on_click(on_button_clicked)\n",
    "display(button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Greedy Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Generate data\n",
    "num_points = 100\n",
    "noise = 0\n",
    "num_folds = 5\n",
    "\n",
    "X = np.random.randn(num_points)\n",
    "Y = 1 + X + 0.3 * np.sin(10 * X) + noise * np.random.randn(num_points)\n",
    "Xtest = np.linspace(-3, 3, 100)\n",
    "Ytest = 1 + Xtest + 0.3 * np.sin(10 * Xtest)  # Noiseless\n",
    "\n",
    "\n",
    "# Define features\n",
    "feature_set = {'Constant', 'Linear', 'Squared', 'Cubic', \n",
    "            'Sin(x)', 'Sin(2x)', 'Sin(5x)', 'Sin(10x)',\n",
    "            'Exp(x)', 'Exp(2x)', 'Exp(5x)', 'Exp(10x)'}\n",
    "#             'Log(|x|)', 'Log(2|x|)', 'Log(5|x|)', 'Log(10|x|)'}\n",
    "\n",
    "w_hat = get_coefficient(X, Y, feature_set)\n",
    "valid_error = cross_validation(X, Y, feature_set, num_folds)\n",
    "\n",
    "num_features = len(feature_set)\n",
    "\n",
    "plt.clf()\n",
    "clear_output()\n",
    "plot_opts = {'x_label': '$x$', 'y_label': '$y$', 'title': 'Num Features {}'.format(num_features), 'y_lim': [np.min(Y)-0.5, np.max(Y)+0.5]}\n",
    "plot_helpers.plot_data(X, Y, options=plot_opts)\n",
    "\n",
    "Phitest = build_features(Xtest, feature_set)\n",
    "plt.plot(Xtest, Phitest @ w_hat, 'r-')\n",
    "\n",
    "print('Feature set:', list(feature_set))\n",
    "print('Coefficients', w_hat)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    global features, feature_set, num_features, valid_error\n",
    "    worst_feature = None\n",
    "    worst_feature_error = float('+Inf')\n",
    "    \n",
    "    # Find Worst Feature\n",
    "    for feature in feature_set:\n",
    "        feature_set.remove(feature)\n",
    "        \n",
    "        w_hat = get_coefficient(X, Y, feature_set)\n",
    "        current_error = cross_validation(X, Y, feature_set, num_folds)\n",
    "        \n",
    "        if current_error < worst_feature_error:\n",
    "            worst_feature = feature \n",
    "            worst_feature_error = current_error\n",
    "        \n",
    "        feature_set.add(feature) # Append feature \n",
    "    \n",
    "    # If validation error increases\n",
    "    if worst_feature_error > valid_error or len(feature_set) == 1:\n",
    "        print('Finished! Feature set:', feature_set)\n",
    "        print('Validation error:', valid_error)\n",
    "        w_hat = get_coefficient(Xtrain, Ytrain, feature_set)\n",
    "        print('Coefficients', w_hat)\n",
    "    else:\n",
    "        # Remove feature to feature set. \n",
    "        num_features -= 1 \n",
    "        feature_set.remove(worst_feature)\n",
    "        valid_error = worst_feature_error\n",
    "\n",
    "        w_hat = get_coefficient(X, Y, feature_set)\n",
    "\n",
    "        plt.clf()\n",
    "        clear_output()\n",
    "        display(button)\n",
    "        plot_opts = {'x_label': '$x$', 'y_label': '$y$', 'title': 'Num Features {}'.format(num_features), 'y_lim': [np.min(Y)-0.5, np.max(Y)+0.5]}\n",
    "        plot_helpers.plot_data(X, Y, options=plot_opts)\n",
    "        \n",
    "        Phitest = build_features(Xtest, feature_set)\n",
    "        plt.plot(Xtest, Phitest @ w_hat, 'r-')\n",
    "        print('Feature removed', worst_feature)\n",
    "        print('Feature set:', list(feature_set))\n",
    "        print('Coefficients', w_hat)\n",
    "        \n",
    "\n",
    "button = ipywidgets.Button(description=\"Remove Feature Feature\")\n",
    "button.on_click(on_button_clicked)\n",
    "display(button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization: Sparsity Inducing\n",
    "\n",
    "The L-1 regularization method uses a regularizer of the form $R(w) = ||w||_1$ which is non-differentiable. However, the subgradient exits and is:\n",
    "\n",
    "$$\\partial(||w|||_1) = \\left\\{\\begin{array}{cc} \\text{sign} (w)& \\text{if } w \\neq 0 \\\\ [-1, 1]  & \\text{if } w = 0 \\end{array} \\right. $$\n",
    "\n",
    "This regularization method penalizes weights and induces sparsity in the solutions. That is, most of the entries of the solution $w^\\star$ will be zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_widget = ipywidgets.FloatSlider(value=-12,min=-12,max=2,step=1,  description='Regularizer 10^:',\n",
    "                                    style={'description_width': 'initial'}, continuous_update=False)\n",
    "\n",
    "features= ['Constant', 'Linear', 'Squared', 'Cubic',\n",
    "          'Sin(x)', 'Sin(2x)', 'Sin(5x)', 'Sin(10x)',\n",
    "          'Exp(x)', 'Exp(2x)', 'Exp(5x)', 'Exp(10x)']\n",
    "feature_widget = ipywidgets.SelectMultiple(options=features,\n",
    "    value=['Constant', 'Linear', 'Sin(2x)', 'Sin(5x)', 'Sin(10x)'],\n",
    "    rows=len(features),\n",
    "    description='Features',\n",
    "    disabled=False\n",
    ")\n",
    "def lasso_reg(reg, feature_list):\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Generate data\n",
    "    num_points = 100\n",
    "    noise = 0\n",
    "    num_folds = 5\n",
    "\n",
    "    X = np.random.randn(num_points)\n",
    "    Y = 1 + X + 0.3 * np.sin(10 * X) + noise * np.random.randn(num_points)\n",
    "    Xtest = np.linspace(-3, 3, 100)\n",
    "    Ytest = 1 + Xtest + 0.3 * np.sin(10 * Xtest)  # Noiseless\n",
    "\n",
    "    Phi = build_features(X, feature_list)\n",
    "    Phi_test = build_features(Xtest, feature_list)\n",
    "\n",
    "    regressor = Lasso(alpha=10 ** reg, fit_intercept=False, max_iter=1e5)\n",
    "    regressor.fit(Phi, Y)\n",
    "\n",
    "    w = regressor.coef_\n",
    "    print('Features:', feature_list)\n",
    "    print('Regressor Coefficients:', w)\n",
    "    fig = plt.subplot(111);\n",
    "    plot_opts = {'x_label': '$x$', 'y_label': '$y$', 'title': 'Lasso-Regression', 'y_lim': [np.min(Y)-0.5, np.max(Y)+0.5]}\n",
    "    plot_helpers.plot_data(X, Y, fig=fig, options=plot_opts)\n",
    "    plt.plot(Xtest, regressor.predict(Phi_test), 'r-', linewidth=4);\n",
    "\n",
    "interact(lasso_reg, reg=reg_widget, feature_list=feature_widget);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization: (Lasso-)Regression \n",
    "\n",
    "The dataset has 10 features, but only one informative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_widget = ipywidgets.FloatSlider(value=1,min=0,max=3,step=0.5,readout_format='.1f',\n",
    "                                      description='Noise level:', \n",
    "                                      style={'description_width': 'initial'}, continuous_update=False)\n",
    "n_features_widget = ipywidgets.IntSlider(value=10, min=1, max=10, step=1,  description='Number of Features:', \n",
    "                                         style={'description_width': 'initial'}, continuous_update=False)\n",
    "n_informative_widget = ipywidgets.IntSlider(value=1, min=1, max=10, step=1,\n",
    "                                            description='Informative Features:', \n",
    "                                            style={'description_width': 'initial'}, continuous_update=False)\n",
    "reg_widget = ipywidgets.FloatSlider(value=-6,min=-6,max=1,step=0.5,  description='Regularizer 10^:',\n",
    "                                    style={'description_width': 'initial'}, continuous_update=False)\n",
    "\n",
    "def lasso_regression(n_features, n_informative, reg, noise):\n",
    "    X, Y, w = make_regression(n_features=n_features, n_informative=n_informative, coef=True, random_state=42, noise=noise, \n",
    "                              bias=1)\n",
    "\n",
    "    regressor = Lasso(alpha=max(10**reg, 1e-12))\n",
    "    regressor.fit(X, Y)\n",
    "    \n",
    "    w = regressor.coef_\n",
    "    idx = np.argmax(np.abs(w))\n",
    "    Xtest = np.random.randn(100, n_features)\n",
    "    Xtest[:, idx] = np.linspace(-3, 3, 100)\n",
    "    Ytest = regressor.predict(Xtest)\n",
    "    \n",
    "    # Plot Data\n",
    "    fig = plt.subplot(111);\n",
    "    plot_opts = {'x_label': '$x$', 'y_label': '$y$', 'title': 'Lasso-Regression', 'y_lim': [np.min(Y)-0.5, np.max(Y)+0.5]}\n",
    "    plot_helpers.plot_data(X[:, idx], Y, fig=fig, options=plot_opts)\n",
    "    plt.plot(Xtest[:, idx], Ytest, 'r-', linewidth=4);\n",
    "    print('Regressor Coefficients:', w)\n",
    "\n",
    "interact(lasso_regression, n_features=n_features_widget, n_informative=n_informative_widget, \n",
    "         reg=reg_widget, noise=noise_widget);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, w = make_regression(n_samples=10, n_features=10, coef=True, random_state=1, bias=3.5)\n",
    "regs = dict(ridge=Ridge(), lasso=Lasso())\n",
    "\n",
    "alphas = np.logspace(-6, 6, 200)\n",
    "coefs = dict(ridge=[], lasso=[])\n",
    "for a in alphas:\n",
    "    for name, reg in regs.items():\n",
    "        reg.set_params(alpha=a, max_iter=10000)\n",
    "        reg.fit(X, y)\n",
    "        coefs[name].append(reg.coef_)\n",
    "\n",
    "fig = plt.figure(1)\n",
    "plt.plot(alphas, coefs['ridge'])\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Ridge coefficients as a function of the regularization');\n",
    "\n",
    "fig = plt.figure(2)\n",
    "plt.plot(alphas, coefs['lasso'])\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Lasso coefficients as a function of the regularization');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Classificaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 100  # Number of points per class\n",
    "noise = 0.2  # Noise Level (needed for data generation).\n",
    "TEST_FRACTION = .80\n",
    "np.random.seed(42)\n",
    "X, Y = linear_separable_data(num_points, noise=noise, dim=2)\n",
    "\n",
    "fig = plt.subplot(111)\n",
    "opt = {'marker': 'ro', 'label': '+', 'size': 8}\n",
    "plot_helpers.plot_data(X[np.where(Y == 1)[0], 0], X[np.where(Y == 1)[0], 1], fig=fig, options=opt)\n",
    "opt = {'marker': 'bs', 'label': '-', 'x_label': '$x$', 'y_label': '$y$', 'size': 8, 'legend': True}\n",
    "plot_helpers.plot_data(X[np.where(Y == -1)[0], 0], X[np.where(Y == -1)[0], 1], fig=fig, options=opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into train and test sets!\n",
    "indexes = np.arange(0, 2 * num_points, 1)\n",
    "np.random.shuffle(indexes)\n",
    "num_train = int(np.ceil(2 * TEST_FRACTION * num_points))\n",
    "\n",
    "X_train = X[indexes[:num_train]]\n",
    "Y_train = Y[indexes[:num_train]]\n",
    "\n",
    "X_test = X[indexes[num_train:]]\n",
    "Y_test = Y[indexes[num_train:]]\n",
    "\n",
    "fig = plt.subplot(111)\n",
    "\n",
    "opt = {'marker': 'ro', 'fillstyle': 'full', 'label': '+ Train', 'size': 8}\n",
    "plot_helpers.plot_data(X_train[np.where(Y_train == 1)[0], 0], X_train[np.where(Y_train == 1)[0], 1], fig=fig, options=opt)\n",
    "opt = {'marker': 'bs', 'fillstyle': 'full', 'label': '- Train', 'size': 8}\n",
    "plot_helpers.plot_data(X_train[np.where(Y_train == -1)[0], 0], X_train[np.where(Y_train == -1)[0], 1], fig=fig, options=opt)\n",
    "\n",
    "opt = {'marker': 'ro', 'fillstyle': 'none', 'label': '+ Test', 'size': 8}\n",
    "plot_helpers.plot_data(X_test[np.where(Y_test == 1)[0], 0], X_test[np.where(Y_test == 1)[0], 1], fig=fig, options=opt)\n",
    "opt = {'marker': 'bs', 'fillstyle': 'none', 'label': '- Test', 'size': 8, \n",
    "       'x_label': '$x$', 'y_label': '$y$', 'legend': True}\n",
    "plot_helpers.plot_data(X_test[np.where(Y_test == -1)[0], 0], X_test[np.where(Y_test == -1)[0], 1], fig=fig, options=opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_positive = 50  # Number of points per class\n",
    "num_negative = 50  # Number of points per class\n",
    "\n",
    "noise = 0.3  # Noise Level (needed for data generation).\n",
    "\n",
    "X, Y = linear_separable_data(num_positive, num_negative, offset=np.array([1, .2]), noise=noise, dim=2)\n",
    "X = X - np.mean(X, axis=0)\n",
    "\n",
    "\n",
    "def regularization(regularizer, reg):\n",
    "    np.random.seed(42)\n",
    "    classifier = SGDClassifier(loss='perceptron', penalty=regularizer, alpha = np.power(10., reg), random_state=1)\n",
    "    classifier.fit(X[:,:2], Y)\n",
    "    \n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    h = .02\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    fig = plt.subplot(111)\n",
    "    contour = plot_helpers.plot_contours(fig, classifier, xx, yy, cmap=plt.cm.jet, alpha=0.3)\n",
    "    plt.colorbar(contour)\n",
    "    opt = {'marker': 'r*', 'label': '+'}\n",
    "    plot_helpers.plot_data(X[np.where(Y == 1)[0], 0], X[np.where(Y == 1)[0], 1], fig=fig, options=opt)\n",
    "    opt = {'marker': 'bs', 'label': '-', 'legend': True}\n",
    "    plot_helpers.plot_data(X[np.where(Y == -1)[0], 0], X[np.where(Y == -1)[0], 1], fig=fig, options=opt)\n",
    "    \n",
    "interact(regularization,\n",
    "         regularizer=ipywidgets.RadioButtons(\n",
    "             options=['l1', 'l2'],\n",
    "             value='l1',\n",
    "             description='Algorithm:',\n",
    "             style={'description_width': 'initial'}),\n",
    "         reg=ipywidgets.FloatSlider(\n",
    "             value=-3,\n",
    "             min=-3,\n",
    "             max=0,\n",
    "             step=0.5,\n",
    "             description='Regularizer 10^:',\n",
    "             style={'description_width': 'initial'},\n",
    "             continuous_update=False)\n",
    "         );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "widgets": {
   "state": {
    "1dbcd274dcfe470b91708912892ed69f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "2cfdd1a03116452aa5629c3d90011059": {
     "views": [
      {
       "cell_index": 13
      }
     ]
    },
    "4e37636106d041fab7a6ff19b8cb36c7": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    },
    "674c84c930124a6f95d134bf447ea1c1": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "85cd0d5f4f1e4750ab3b284e95bc5242": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "a44fb013255548a7b20262200fd53b86": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
