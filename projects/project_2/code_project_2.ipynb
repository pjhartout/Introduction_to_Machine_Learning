{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the solution to project 2 from scratch\n",
    "To note:\n",
    "- For clarity the df_test was renamed to df_val as the test word was used when splitting the labeled data into train and test. \n",
    "    - Val stands for validation\n",
    "\n",
    "To try out:\n",
    "- Preprocessing\n",
    "    - Tweak data imputer\n",
    "    - Tweak scaler (Robust scaler, minmax, etc..)\n",
    "    - Tweak feature selection parameter\n",
    "    - Tweak order of operations above to see the effect\n",
    "- Modelling\n",
    "    - XGBoost\n",
    "    - SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import zipfile\n",
    "import time\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import ADASYN, SMOTE\n",
    "from imblearn.under_sampling import ClusterCentroids, RandomUnderSampler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, chi2, f_classif\n",
    "from sklearn.metrics import f1_score, mean_squared_error, accuracy_score, r2_score, roc_auc_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import label_ranking_average_precision_score as LRAPS\n",
    "from sklearn.metrics import label_ranking_loss as LRL\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Global variables\n",
    "IDENTIFIERS = [\"pid\", \"Time\"]\n",
    "MEDICAL_TESTS = [\n",
    "    \"LABEL_BaseExcess\",\n",
    "    \"LABEL_Fibrinogen\",\n",
    "    \"LABEL_AST\",\n",
    "    \"LABEL_Alkalinephos\",\n",
    "    \"LABEL_Bilirubin_total\",\n",
    "    \"LABEL_Lactate\",\n",
    "    \"LABEL_TroponinI\",\n",
    "    \"LABEL_SaO2\",\n",
    "    \"LABEL_Bilirubin_direct\",\n",
    "    \"LABEL_EtCO2\",\n",
    "]\n",
    "VITAL_SIGNS = [\"LABEL_RRate\", \"LABEL_ABPm\", \"LABEL_SpO2\", \"LABEL_Heartrate\"]\n",
    "SEPSIS = [\"LABEL_Sepsis\"]\n",
    "ESTIMATOR = {\"bayesian\": BayesianRidge(), \"decisiontree\": DecisionTreeRegressor(max_features=\"sqrt\", random_state=0), \n",
    "                \"extratree\": ExtraTreesRegressor(n_estimators=10, random_state=0), \n",
    "                \"knn\": KNeighborsRegressor(n_neighbors=10, weights=\"distance\")}\n",
    "\n",
    "FEATURES_MNAR = [\"EtCO2\", \"PTT\", \"BUN\", \"Lactate\", \"Hgb\", \"HCO3\", \"BaseExcess\",\n",
    "                          \"Fibrinogen\", \"Phosphate\", \"WBC\", \"Creatinine\", \"PaCO2\", \"AST\",\n",
    "                          \"FiO2\", \"Platelets\", \"SaO2\", \"Glucose\", \"Magnesium\", \"Potassium\",\n",
    "                          \"Calcium\", \"Alkalinephos\", \"Bilirubin_direct\", \"Chloride\", \"Hct\",\n",
    "                          \"Bilirubin_total\", \"TroponinI\", \"pH\"]\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_f(x):\n",
    "    \"\"\"To get predictions as confidence level, the model predicts for all 12 sets of measures for\n",
    "    each patient a distance to the hyperplane ; it is then transformed into a confidence level using\n",
    "    the sigmoid function ; the confidence level reported is the mean of all confidence levels for a\n",
    "    single patient\n",
    "\n",
    "    Args:\n",
    "        x (float): input of the sigmoid function\n",
    "\n",
    "    Returns:\n",
    "       float: result of the sigmoid computation.\n",
    "\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r\"data/train_features.csv\")\n",
    "df_train_label = pd.read_csv(r\"data/train_labels.csv\")\n",
    "df_val = pd.read_csv(r\"data/test_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data imputation methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done adding features about MNAR features\n"
     ]
    }
   ],
   "source": [
    "# Adding engineered features\n",
    "mnar_columns = [\n",
    "        sub + \"_presence\" for sub in FEATURES_MNAR\n",
    "    ]\n",
    "pid = df_train[\"pid\"].unique()\n",
    "patient_count = -1\n",
    "for patient in pid:\n",
    "    patient_count += 1\n",
    "    print(patient_count/len(pid)*100, end=\"\\r\")\n",
    "    for column in FEATURES_MNAR:\n",
    "        presence = int(df_train.loc[\n",
    "            df_train[\"pid\"] == patient\n",
    "            ][column].any())\n",
    "        df_train.at[patient, column] = presence\n",
    "print(\"Done adding features about MNAR features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done adding features about MNAR features\n"
     ]
    }
   ],
   "source": [
    "# Adding engineered features\n",
    "pid = df_val[\"pid\"].unique()\n",
    "patient_count = -1\n",
    "for patient in pid:\n",
    "    patient_count += 1\n",
    "    print(patient_count/len(pid)*100, end=\"\\r\")\n",
    "    for column in FEATURES_MNAR:\n",
    "        presence = int(df_val.loc[\n",
    "            df_val[\"pid\"] == patient\n",
    "            ][column].any())\n",
    "        df_val.at[patient, column] = presence\n",
    "print(\"Done adding features about MNAR features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.04735456699135544\r"
     ]
    }
   ],
   "source": [
    "columns_for_regression = [\"Temp\", \"Hgb\", \"RRate\", \"BaseExcess\", \"WBC\", \"PaCO2\", \"FiO2\", \"Glucose\", \"ABPm\", \"ABPd\", \"SpO2\", \"Hct\", \"Heartrate\", \"ABPs\", \"pH\"]\n",
    "columns_for_regression_trend = [\n",
    "        sub + \"_trend\" for sub in columns_for_regression\n",
    "    ]\n",
    "columns_for_regression_std = [\n",
    "        sub + \"_std\" for sub in columns_for_regression\n",
    "    ]\n",
    "columns_for_regression_min = [\n",
    "        sub + \"_min\" for sub in columns_for_regression\n",
    "    ]\n",
    "columns_for_regression_max = [\n",
    "        sub + \"_max\" for sub in columns_for_regression\n",
    "    ]\n",
    "cols_to_add = columns_for_regression_trend + columns_for_regression_std + columns_for_regression_min + columns_for_regression_max\n",
    "patient_count = 0\n",
    "\n",
    "df_train = df_train.reindex(\n",
    "        df_train.columns.tolist() + cols_to_add,\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "pid = df_train[\"pid\"].unique()\n",
    "\n",
    "for patient in pid:\n",
    "    patient_count += 1\n",
    "    print(patient_count/len(pid)*100, end=\"\\r\")\n",
    "    for column in columns_for_regression:\n",
    "        if df_train.loc[df_train[\"pid\"] == patient][column].isna().sum() <= 8:\n",
    "            series = df_train.loc[df_train[\"pid\"] == patient][column]\n",
    "            # Fill missing values between two non nans with their average\n",
    "            series = (series.ffill() + series.bfill()) / 2\n",
    "            # Drop the rest of the value\n",
    "            series = series.dropna()\n",
    "            standard_deviation = series.std()\n",
    "            minimum = series.min()\n",
    "            maximum = series.max()\n",
    "            X = [i for i in range(0, len(series))]\n",
    "            X = np.reshape(X, (len(X), 1))\n",
    "            y = series\n",
    "            model = LinearRegression()\n",
    "            try:\n",
    "                model.fit(X, y)\n",
    "                df_train.at[patient, column + \"_trend\"] = model.coef_\n",
    "            except ValueError:\n",
    "                df_train.at[patient, column + \"_trend\"] = 0\n",
    "            df_train.at[patient, column + \"_std\"] = standard_deviation\n",
    "            df_train.at[patient, column + \"_min\"] = minimum\n",
    "            df_train.at[patient, column + \"_max\"] = maximum\n",
    "\n",
    "    # fill rest of values with 0 for trends col umns\n",
    "    df_train[columns_for_regression_trend] = df_train[\n",
    "        columns_for_regression_trend\n",
    "    ].fillna(value=0)\n",
    "    df_train[columns_for_regression_std] = df_train[\n",
    "        columns_for_regression_std\n",
    "    ].fillna(value=0)\n",
    "    df_train[columns_for_regression_min] = df_train[\n",
    "        columns_for_regression_min\n",
    "    ].fillna(value=0)\n",
    "    df_train[columns_for_regression_max] = df_train[\n",
    "        columns_for_regression_max\n",
    "    ].fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.04735456699135544\r"
     ]
    }
   ],
   "source": [
    "patient_count = 0\n",
    "\n",
    "for patient in pid:\n",
    "    patient_count += 1\n",
    "    print(patient_count/len(pid)*100, end=\"\\r\")\n",
    "    for column in columns_for_regression:\n",
    "        if df_val.loc[df_val[\"pid\"] == patient][column].isna().sum() <= 8:\n",
    "            series = df_val.loc[df_train[\"pid\"] == patient][column]\n",
    "            # Fill missing values between two non nans with their average\n",
    "            series = (series.ffill() + series.bfill()) / 2\n",
    "            # Drop the rest of the value\n",
    "            series = series.dropna()\n",
    "            standard_deviation = series.std()\n",
    "            minimum = series.min()\n",
    "            maximum = series.max()\n",
    "            X = [i for i in range(0, len(series))]\n",
    "            X = np.reshape(X, (len(X), 1))\n",
    "            y = series\n",
    "            model = LinearRegression()\n",
    "            try:\n",
    "                model.fit(X, y)\n",
    "                df_train.at[patient, column + \"_trend\"] = model.coef_\n",
    "            except ValueError:\n",
    "                df_train.at[patient, column + \"_trend\"] = 0\n",
    "            df_val.at[patient, column + \"_std\"] = standard_deviation\n",
    "            df_val.at[patient, column + \"_min\"] = minimum\n",
    "            df_val.at[patient, column + \"_max\"] = maximum\n",
    "\n",
    "    # fill rest of values with 0 for trends col umns\n",
    "    df_val[columns_for_regression_trend] = df_val[\n",
    "        columns_for_regression_trend\n",
    "    ].fillna(value=0)\n",
    "    df_val[columns_for_regression_std] = df_val[\n",
    "        columns_for_regression_std\n",
    "    ].fillna(value=0)\n",
    "    df_val[columns_for_regression_min] = df_val[\n",
    "        columns_for_regression_min\n",
    "    ].fillna(value=0)\n",
    "    df_val[columns_for_regression_max] = df_val[\n",
    "        columns_for_regression_max\n",
    "    ].fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philiphartout/anaconda3/envs/iml/lib/python3.7/site-packages/sklearn/impute/_iterative.py:638: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "imputer = IterativeImputer()\n",
    "columns = df_train.columns\n",
    "df_train = imputer.fit_transform(df_train.values)\n",
    "df_train = pd.DataFrame(df_train, columns=columns)\n",
    "df_train = df_train.groupby([\"pid\"], as_index=False).mean()\n",
    "\n",
    "# Tranform test data according to same imputer\n",
    "pid_val = df_val[\"pid\"].unique()\n",
    "columns = df_val.columns\n",
    "df_val = pd.DataFrame(columns=columns, index=pid_val)\n",
    "df_val = imputer.transform(df_val.values)\n",
    "df_val = pd.DataFrame(df_val, columns=columns)\n",
    "df_val = df_val.groupby([\"pid\"], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"df_train_philip.csv\")\n",
    "df_val.to_csv(\"df_val_philip.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_preprocessed = pd.read_csv(\"df_train_philip.csv\")\n",
    "df_val_preprocessed = pd.read_csv(\"df_val_philip.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "val_pids = np.unique(df_val_preprocessed[\"pid\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_preprocessed = df_train_preprocessed.sort_values(by=[\"pid\"])\n",
    "df_train_preprocessed = df_train_preprocessed.drop(columns=IDENTIFIERS)\n",
    "df_val_preprocessed = df_val_preprocessed.sort_values(by=[\"pid\"])\n",
    "df_val_preprocessed = df_val_preprocessed.drop(columns=IDENTIFIERS)\n",
    "df_train_label = df_train_label.sort_values(by=[\"pid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a list of labels for each medical test\n",
      "Creating a list of labels for sepsis\n",
      "Creating a list of labels for each vital sign\n"
     ]
    }
   ],
   "source": [
    "# Data formatting\n",
    "X_train = df_train_preprocessed.values\n",
    "X_val = df_val_preprocessed.values\n",
    "# Create list with different label for each medical test\n",
    "print(\"Creating a list of labels for each medical test\")\n",
    "y_train_medical_tests = []\n",
    "for test in MEDICAL_TESTS:\n",
    "    y_train_medical_tests.append(df_train_label[test].astype(int).values)\n",
    "\n",
    "# Create list with different label for sepsis\n",
    "print(\"Creating a list of labels for sepsis\")\n",
    "y_train_sepsis = []\n",
    "for sepsis in SEPSIS:\n",
    "    y_train_sepsis.append(df_train_label[sepsis].astype(int).values)\n",
    "\n",
    "# Create list with different label for each vital sign\n",
    "print(\"Creating a list of labels for each vital sign\")\n",
    "y_train_vital_signs = []\n",
    "for sign in VITAL_SIGNS:\n",
    "    y_train_vital_signs.append(df_train_label[sign].astype(int).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data \n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling medical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Modelling of medical tests using logistic regression with cross validation\n",
    "# models = []\n",
    "# losses = []\n",
    "# columns_medical_tests = []\n",
    "# for i, test in enumerate(MEDICAL_TESTS):\n",
    "#     print(f\"Fitting model for {test}.\")\n",
    "\n",
    "#     print(\"Applying feature selection\")\n",
    "#     feature_selector = SelectKBest(score_func=f_classif, k=3)\n",
    "#     X_train = feature_selector.fit_transform(X_train, y_train_medical_tests[i])\n",
    "#     X_test = feature_selector.transform(X_test)\n",
    "#     columns = feature_selector.get_support(indices=True)\n",
    "#     columns_medical_tests.append(columns)\n",
    "\n",
    "#     print(\"Fitting model\")\n",
    "#     clf = LogisticRegressionCV(cv=5, random_state=42).fit(X_train, y_train_medical_tests[i])\n",
    "#     models.append(clf)\n",
    "#     print(roc_auc_score(y_test, clf.predict_proba(X_test)[:,1]))\n",
    "#     print(f\"Finished test for medical tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READY TO ROLL\n"
     ]
    }
   ],
   "source": [
    "print(\"READY TO ROLL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_cuda_tensor(X_train, X_test, y_train, y_test, device):\n",
    "    \"\"\"Converts a number of np.ndarrays to tensors placed on the device specified.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): (n_samples, n_features) array containing training features\n",
    "        X_test (np.ndarray): (n_samples, n_features) array containing testing features\n",
    "        y_train (np.ndarray): (n_samples,) array containing training labels\n",
    "        y_test (np.ndarray): (n_samples,) array containing testing labels\n",
    "        device (torch.device): device on which the tensors should be placed (CPU/CUDA GPU)\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    return (\n",
    "        torch.from_numpy(X_train).to(device).float(),\n",
    "        torch.from_numpy(X_test).to(device).float(),\n",
    "        torch.from_numpy(y_train).to(device).float(),\n",
    "        torch.from_numpy(y_test).to(device).float(),\n",
    "    )\n",
    "\n",
    "\n",
    "class Feedforward(torch.nn.Module):\n",
    "    \"\"\" Definition of the feedfoward neural network. It currently has three layers which can be\n",
    "    modified in the function where the network is trained.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, subtask, p=0.2):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.subtask = subtask\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout = torch.nn.Dropout(p=p)\n",
    "        self.bn = torch.nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        torch.nn.init.xavier_normal_(self.fc1.weight)\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        torch.nn.init.xavier_normal_(self.fc2.weight)\n",
    "        self.fc3 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        torch.nn.init.xavier_normal_(self.fc3.weight)\n",
    "        self.fcout = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "        torch.nn.init.xavier_normal_(self.fcout.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Function where the forward pass is defined. The backward pass is deternmined by the\n",
    "            autograd function built into PyTorch.\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor (n_samples,n_features) tensor containing training input\n",
    "                features\n",
    "            subtask (int): subtask performed (choice: 1,2,3)\n",
    "        Returns:\n",
    "            output (torch.Tensor): (n_samples,n_features) tensor containing\n",
    "                the predicted output for each sample.\n",
    "        \"\"\"\n",
    "        assert self.subtask in [1, 2, 3]\n",
    "        hidden = self.fc1(x)\n",
    "        hidden_bn = self.bn(hidden)\n",
    "        relu = self.sigmoid(hidden_bn)\n",
    "        hidden = self.dropout(self.fc2(relu))\n",
    "        hidden_bn = self.bn(hidden)\n",
    "        relu = self.sigmoid(hidden_bn)\n",
    "        hidden = self.dropout(self.fc3(relu))\n",
    "        hidden_bn = self.bn(hidden)\n",
    "        relu = self.sigmoid(hidden_bn)\n",
    "        output = self.fcout(relu)\n",
    "        if self.subtask == 1 or self.subtask == 2:\n",
    "            output = self.sigmoid(output)\n",
    "        else:\n",
    "            output = self.relu(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Data(Dataset):\n",
    "    \"\"\" Class used to load the data in minibatches to control the neural network stability during\n",
    "        training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.len = self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Modelling using extreme gradient boosting\n",
    "# models = []\n",
    "# losses = []\n",
    "# feature_selector_medical_tests = []\n",
    "# for i, test in enumerate(MEDICAL_TESTS):\n",
    "#     print(f\"Fitting model for {test}.\")\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X_train_scaled, y_train_medical_tests[i], test_size=0.10, random_state=42, shuffle=True\n",
    "#     )\n",
    "#     # Coarse parameter grid not optimized at all yet\n",
    "    \n",
    "#     print(\"Resampling\")\n",
    "#     sampler = RandomUnderSampler(random_state=42)\n",
    "#     X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "#     print(\"Applying feature selection\")\n",
    "#     feature_selector = SelectKBest(score_func=f_classif, k=10)\n",
    "#     X_train = feature_selector.fit_transform(X_train_res, y_train_res)\n",
    "#     X_test = feature_selector.transform(X_test)\n",
    "#     feature_selector_medical_tests.append(feature_selector)\n",
    "    \n",
    "    \n",
    "#     X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = convert_to_cuda_tensor(\n",
    "#             X_train, X_test, y_train_res, y_test, DEVICE\n",
    "#     )\n",
    "    \n",
    "#     model = Feedforward(\n",
    "#             input_size=X_train_tensor.shape[1],\n",
    "#             hidden_size=100,\n",
    "#             output_size=1,\n",
    "#             subtask=1,\n",
    "#             p=0.1,\n",
    "#     )\n",
    "    \n",
    "#     criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# #     if optim == \"SGD\":\n",
    "# #         optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "# #     elif optim == \"Adam\":\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "#     dataset = Data(X_train_tensor, y_train_tensor)\n",
    "#     batch_size = 1024  # Ideally we want powers of 2\n",
    "#     trainloader = DataLoader(dataset=dataset, batch_size=batch_size)\n",
    "\n",
    "#     if torch.cuda.is_available():\n",
    "#         model.cuda()\n",
    "#     model.float()\n",
    "    \n",
    "    \n",
    "#     dirpath = os.path.join(os.getcwd(), \"runs\")\n",
    "#     fileList = os.listdir(dirpath)\n",
    "#     for fileName in fileList:\n",
    "#         shutil.rmtree(dirpath + \"/\" + fileName)\n",
    "    \n",
    "#     now = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "#     writer = SummaryWriter(\n",
    "#         log_dir=f\"runs/ann_network_runs_{200}_epochs_{now}_{test}\"\n",
    "#     )\n",
    "#     for epoch in list(range(500)):\n",
    "#         LOSS = []\n",
    "#         for x, y in trainloader:\n",
    "#             yhat = model(x)\n",
    "#             loss = criterion(yhat.float(), y.reshape((y.shape[0], 1)))\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             LOSS.append(loss)\n",
    "#         X_test_tensor = X_test_tensor.to(DEVICE)\n",
    "#         y_test_pred = model(X_test_tensor).cpu().detach().numpy()\n",
    "#         y_train_pred = model(X_train_tensor).cpu().detach().numpy()\n",
    "#         loss_average = sum(LOSS) / len(LOSS)\n",
    "#         writer.add_scalar(\"Training_loss\", loss_average, epoch)\n",
    "#         ROC_train = roc_auc_score(y_train_res, y_train_pred)\n",
    "#         ROC_test = roc_auc_score(y_test, y_test_pred)\n",
    "# #         writer.add_scalar(\"ROC train\", ROC_train, epoch)\n",
    "# #         writer.add_scalar(\"ROC test\", ROC_test, epoch)\n",
    "#         print(f\"Epoch: {epoch} - ROC train: {ROC_train} - ROC test: {ROC_test}\", end=\"\\r\")\n",
    "#     print(\"\")\n",
    "        \n",
    "# #     models.append(model)\n",
    "# #     model.eval()\n",
    "# #     print(f\"ROC score on test set {roc_auc_score(y_test, model(X_test_tensor))}\")\n",
    "# #     print(f\"CV score {coarse_search.best_score_}\")\n",
    "# print(f\"Finished test for medical tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model for LABEL_BaseExcess.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   30.7s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed: 11.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed: 17.1min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed: 20.1min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed: 23.9min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed: 28.2min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed: 28.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92124844 0.11344181 0.29810002 ... 0.91302997 0.27503392 0.8153258 ]\n",
      "ROC score on test set 0.879664081103796\n",
      "CV score 0.8627901854812372\n",
      "ROC score on test set 0.879664081103796\n",
      "CV score 0.8627901854812372\n",
      "Fitting model for LABEL_Fibrinogen.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:   30.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:   47.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:  6.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.718125   0.5770305  0.52418727 ... 0.4888774  0.32086784 0.36808202]\n",
      "ROC score on test set 0.7243470422535212\n",
      "CV score 0.7329693651574803\n",
      "ROC score on test set 0.7243470422535212\n",
      "CV score 0.7329693651574803\n",
      "Fitting model for LABEL_AST.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   25.7s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed: 15.7min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed: 18.6min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed: 21.7min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed: 22.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8374758  0.6264091  0.33403337 ... 0.58397895 0.38864544 0.4827148 ]\n",
      "ROC score on test set 0.7368214609952413\n",
      "CV score 0.7073899464604403\n",
      "ROC score on test set 0.7368214609952413\n",
      "CV score 0.7073899464604403\n",
      "Fitting model for LABEL_Alkalinephos.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed: 15.5min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed: 18.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed: 22.1min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed: 22.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8032056  0.64130074 0.33422276 ... 0.59836924 0.4610252  0.46021613]\n",
      "ROC score on test set 0.7403043216253443\n",
      "CV score 0.7150990828758821\n",
      "ROC score on test set 0.7403043216253443\n",
      "CV score 0.7150990828758821\n",
      "Fitting model for LABEL_Bilirubin_total.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   21.6s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   51.4s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed: 14.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed: 17.0min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed: 19.7min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed: 22.6min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed: 23.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87397355 0.6922115  0.32102206 ... 0.5634515  0.4186712  0.5080465 ]\n",
      "ROC score on test set 0.7409208937198067\n",
      "CV score 0.7041963403010875\n",
      "ROC score on test set 0.7409208937198067\n",
      "CV score 0.7041963403010875\n",
      "Fitting model for LABEL_Lactate.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   19.3s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   55.3s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed: 17.1min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed: 20.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8033025  0.20566778 0.28735828 ... 0.62399405 0.2143497  0.6532622 ]\n",
      "ROC score on test set 0.7627646524669035\n",
      "CV score 0.750527370586176\n",
      "ROC score on test set 0.7627646524669035\n",
      "CV score 0.750527370586176\n",
      "Fitting model for LABEL_TroponinI.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   28.5s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:   48.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:  9.7min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12231846 0.75927013 0.74577814 ... 0.25730664 0.6032266  0.17911664]\n",
      "ROC score on test set 0.7227693581780539\n",
      "CV score 0.742208626284478\n",
      "ROC score on test set 0.7227693581780539\n",
      "CV score 0.742208626284478\n",
      "Fitting model for LABEL_SaO2.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   52.0s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed: 17.2min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed: 20.0min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed: 23.0min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed: 23.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74861014 0.28393912 0.42730027 ... 0.5937572  0.26378438 0.4900812 ]\n",
      "ROC score on test set 0.7633056578207291\n",
      "CV score 0.7569968114539521\n",
      "ROC score on test set 0.7633056578207291\n",
      "CV score 0.7569968114539521\n",
      "Fitting model for LABEL_Bilirubin_direct.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1528 tasks      | elapsed:   22.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2428 tasks      | elapsed:   38.0s\n",
      "[Parallel(n_jobs=-1)]: Done 3072 tasks      | elapsed:   48.3s\n",
      "[Parallel(n_jobs=-1)]: Done 4132 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5632 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 7332 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 9232 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:  2.7min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8458845  0.62316173 0.28457513 ... 0.4416283  0.3965993  0.6129491 ]\n",
      "ROC score on test set 0.7264207650273226\n",
      "CV score 0.7335745216034641\n",
      "ROC score on test set 0.7264207650273226\n",
      "CV score 0.7335745216034641\n",
      "Fitting model for LABEL_EtCO2.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 248 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=-1)]: Done 498 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=-1)]: Done 848 tasks      | elapsed:   32.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1298 tasks      | elapsed:   48.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1848 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2498 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3248 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4098 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 5048 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6098 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 7248 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 8498 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 9848 tasks      | elapsed:  5.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62657565 0.3886174  0.32917774 ... 0.61635196 0.25950286 0.5672045 ]\n",
      "ROC score on test set 0.8425446428571428\n",
      "CV score 0.801199280844551\n",
      "ROC score on test set 0.8425446428571428\n",
      "CV score 0.801199280844551\n",
      "Finished test for medical tests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:  5.8min finished\n"
     ]
    }
   ],
   "source": [
    "# Modelling using extreme gradient boosting\n",
    "clf = xgb.XGBClassifier(objective=\"binary:logistic\", n_thread=-1)\n",
    "models = []\n",
    "losses = []\n",
    "feature_selector_medical_tests = []\n",
    "for i, test in enumerate(MEDICAL_TESTS):\n",
    "    print(f\"Fitting model for {test}.\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train_scaled, y_train_medical_tests[i], test_size=0.10, random_state=42, shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(\"Resampling\")\n",
    "    sampler = RandomUnderSampler(random_state=42)\n",
    "    X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(\"Applying feature selection\")\n",
    "    feature_selector = SelectKBest(score_func=f_classif, k=5)\n",
    "    X_train_selected = feature_selector.fit_transform(X_train_res, y_train_res)\n",
    "    X_test = feature_selector.transform(X_test)\n",
    "    feature_selector_medical_tests.append(feature_selector)\n",
    "    \n",
    "    print(\"Fitting coarse model\")\n",
    "    # Coarse parameter grid not optimized at all yet\n",
    "    coarse_param_grid = {\n",
    "        \"booster\": [\"dart\"],\n",
    "        \"eta\": np.arange(0,1,0.1),\n",
    "        \"min_child_weight\": range(1, 10, 1),\n",
    "        \"max_depth\": range(4, 10, 1),\n",
    "        \"gamma\": range(0, 100, 1),\n",
    "        \"max_delta_step\": range(1, 10, 1),\n",
    "        \"subsample\": np.arange(0.1, 1, 0.05),\n",
    "        \"colsample_bytree\": np.arange(0.3, 1, 0.05),\n",
    "        \"n_estimators\": range(50, 150, 1),\n",
    "        \"scale_pos_weight\": [1],\n",
    "        \"reg_lambda\": [0, 1], # Ridge regularization\n",
    "        \"reg_alpha\": [0, 1], # Lasso regularization\n",
    "        \"eval_metric\": [\"error\"],\n",
    "        \"verbosity\": [1]\n",
    "    }\n",
    "    coarse_search = RandomizedSearchCV(estimator=clf,\n",
    "            param_distributions=coarse_param_grid, scoring=\"roc_auc\",\n",
    "            n_jobs=-1, cv=10, n_iter=1000, verbose=1)\n",
    "    coarse_search.fit(X_train_selected, y_train_res)\n",
    "    print(coarse_search.best_estimator_.predict_proba(X_test)[:,1])\n",
    "    print(f\"ROC score on test set {roc_auc_score(y_test, coarse_search.best_estimator_.predict_proba(X_test)[:,1])}\")\n",
    "    print(f\"CV score {coarse_search.best_score_}\")\n",
    "    best_params = coarse_search.best_params_\n",
    "#     print(best_params)\n",
    "#     print(\"Fitting fine model\")\n",
    "    # Fine parameter grid not optimized at all yet\n",
    "#     fine_param_grid = {\n",
    "#         \"booster\": best_params[\"booster\"],\n",
    "#         \"eta\": np.arange(best_params[\"eta\"]-0.05, best_params[\"eta\"]+0.05, 0.003),\n",
    "#         \"min_child_weight\": [best_params[\"min_child_weight\"]],\n",
    "#         \"max_depth\": [best_params[\"max_depth\"]],\n",
    "#         \"gamma\": np.arange(best_params[\"gamma\"]-3, best_params[\"gamma\"]+3, 0.05),\n",
    "#         \"max_delta_step\": [best_params[\"max_delta_step\"]],\n",
    "#         \"subsample\": np.arange(best_params[\"subsample\"]-0.05, best_params[\"subsample\"]+0.05, 0.002),\n",
    "#         \"colsample_bytree\": [best_params[\"colsample_bytree\"]],\n",
    "#         \"n_estimators\": range(best_params[\"n_estimators\"]-10, best_params[\"n_estimators\"]+10, 1),\n",
    "#         \"scale_pos_weight\": [1],\n",
    "#         \"reg_lambda\": [best_params[\"reg_lambda\"]], # Ridge regularization\n",
    "#         \"reg_alpha\": [best_params[\"reg_alpha\"]], # Lasso regularization\n",
    "#         \"eval_metric\": [\"error\"],\n",
    "#         \"verbosity\": [2]\n",
    "#     }\n",
    "#     fine_search = RandomizedSearchCV(estimator=clf,\n",
    "#             param_distributions=fine_param_grid, scoring=\"roc_auc\",\n",
    "#             n_jobs=-1, cv=10, n_iter=10, verbose=1)\n",
    "#     fine_search.fit(X_train_selected, y_train_res)\n",
    "#     print(coarse_search.best_estimator_.predict_proba(X_test)[:,1])\n",
    "    print(f\"ROC score on test set {roc_auc_score(y_test, coarse_search.best_estimator_.predict_proba(X_test)[:,1])}\")\n",
    "    print(f\"CV score {coarse_search.best_score_}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    models.append(coarse_search.best_estimator_)\n",
    "    \n",
    "print(f\"Finished test for medical tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "for i, model in enumerate(models):\n",
    "    joblib.dump(models[i], f\"xgboost_fine_{MEDICAL_TESTS[i]}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Get predictions for medical tests\n",
    "df_pred_medical_test = pd.DataFrame(index=val_pids, columns=MEDICAL_TESTS)\n",
    "for i, test in enumerate(MEDICAL_TESTS):\n",
    "    feature_selector = feature_selector_medical_tests[i]\n",
    "    X_val_vital_sign = feature_selector.transform(X_val_scaled)\n",
    "    model_for_test = models[i]\n",
    "#     print(model_for_test.predict_proba(X_val_vital_sign))\n",
    "    y_pred = model_for_test.predict_proba(X_val_vital_sign)[:, 1]\n",
    "    df_pred_medical_test[test] = y_pred\n",
    "\n",
    "df_pred_medical_test = df_pred_medical_test.reset_index().rename(columns={\"index\": \"pid\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>LABEL_BaseExcess</th>\n",
       "      <th>LABEL_Fibrinogen</th>\n",
       "      <th>LABEL_AST</th>\n",
       "      <th>LABEL_Alkalinephos</th>\n",
       "      <th>LABEL_Bilirubin_total</th>\n",
       "      <th>LABEL_Lactate</th>\n",
       "      <th>LABEL_TroponinI</th>\n",
       "      <th>LABEL_SaO2</th>\n",
       "      <th>LABEL_Bilirubin_direct</th>\n",
       "      <th>LABEL_EtCO2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.832393</td>\n",
       "      <td>0.429798</td>\n",
       "      <td>0.896758</td>\n",
       "      <td>0.961712</td>\n",
       "      <td>0.913878</td>\n",
       "      <td>0.849364</td>\n",
       "      <td>0.150986</td>\n",
       "      <td>0.603449</td>\n",
       "      <td>0.647068</td>\n",
       "      <td>0.434431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.131074</td>\n",
       "      <td>0.304217</td>\n",
       "      <td>0.463766</td>\n",
       "      <td>0.480553</td>\n",
       "      <td>0.439553</td>\n",
       "      <td>0.238957</td>\n",
       "      <td>0.698675</td>\n",
       "      <td>0.274292</td>\n",
       "      <td>0.446310</td>\n",
       "      <td>0.232469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.417147</td>\n",
       "      <td>0.304217</td>\n",
       "      <td>0.443913</td>\n",
       "      <td>0.433482</td>\n",
       "      <td>0.425904</td>\n",
       "      <td>0.277688</td>\n",
       "      <td>0.473744</td>\n",
       "      <td>0.404472</td>\n",
       "      <td>0.446310</td>\n",
       "      <td>0.218275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.959387</td>\n",
       "      <td>0.911051</td>\n",
       "      <td>0.944069</td>\n",
       "      <td>0.961004</td>\n",
       "      <td>0.867512</td>\n",
       "      <td>0.672507</td>\n",
       "      <td>0.352497</td>\n",
       "      <td>0.647528</td>\n",
       "      <td>0.911838</td>\n",
       "      <td>0.269971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.384726</td>\n",
       "      <td>0.298570</td>\n",
       "      <td>0.566566</td>\n",
       "      <td>0.562898</td>\n",
       "      <td>0.542512</td>\n",
       "      <td>0.396905</td>\n",
       "      <td>0.221129</td>\n",
       "      <td>0.206536</td>\n",
       "      <td>0.570115</td>\n",
       "      <td>0.324562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12659</th>\n",
       "      <td>31647.0</td>\n",
       "      <td>0.338738</td>\n",
       "      <td>0.304217</td>\n",
       "      <td>0.406793</td>\n",
       "      <td>0.373973</td>\n",
       "      <td>0.411888</td>\n",
       "      <td>0.300359</td>\n",
       "      <td>0.094801</td>\n",
       "      <td>0.290623</td>\n",
       "      <td>0.446310</td>\n",
       "      <td>0.232469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12660</th>\n",
       "      <td>31649.0</td>\n",
       "      <td>0.175901</td>\n",
       "      <td>0.608352</td>\n",
       "      <td>0.760341</td>\n",
       "      <td>0.811736</td>\n",
       "      <td>0.611820</td>\n",
       "      <td>0.340112</td>\n",
       "      <td>0.677199</td>\n",
       "      <td>0.271969</td>\n",
       "      <td>0.739103</td>\n",
       "      <td>0.281824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12661</th>\n",
       "      <td>31651.0</td>\n",
       "      <td>0.929130</td>\n",
       "      <td>0.325646</td>\n",
       "      <td>0.474042</td>\n",
       "      <td>0.461187</td>\n",
       "      <td>0.443320</td>\n",
       "      <td>0.697765</td>\n",
       "      <td>0.273953</td>\n",
       "      <td>0.724276</td>\n",
       "      <td>0.364428</td>\n",
       "      <td>0.437264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12662</th>\n",
       "      <td>31652.0</td>\n",
       "      <td>0.082532</td>\n",
       "      <td>0.317979</td>\n",
       "      <td>0.525401</td>\n",
       "      <td>0.593110</td>\n",
       "      <td>0.526376</td>\n",
       "      <td>0.207084</td>\n",
       "      <td>0.635182</td>\n",
       "      <td>0.299705</td>\n",
       "      <td>0.243108</td>\n",
       "      <td>0.341546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12663</th>\n",
       "      <td>31655.0</td>\n",
       "      <td>0.117321</td>\n",
       "      <td>0.323369</td>\n",
       "      <td>0.573651</td>\n",
       "      <td>0.594063</td>\n",
       "      <td>0.588049</td>\n",
       "      <td>0.225370</td>\n",
       "      <td>0.345621</td>\n",
       "      <td>0.274292</td>\n",
       "      <td>0.446310</td>\n",
       "      <td>0.413635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12664 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           pid  LABEL_BaseExcess  LABEL_Fibrinogen  LABEL_AST  \\\n",
       "0          0.0          0.832393          0.429798   0.896758   \n",
       "1          3.0          0.131074          0.304217   0.463766   \n",
       "2          5.0          0.417147          0.304217   0.443913   \n",
       "3          7.0          0.959387          0.911051   0.944069   \n",
       "4          9.0          0.384726          0.298570   0.566566   \n",
       "...        ...               ...               ...        ...   \n",
       "12659  31647.0          0.338738          0.304217   0.406793   \n",
       "12660  31649.0          0.175901          0.608352   0.760341   \n",
       "12661  31651.0          0.929130          0.325646   0.474042   \n",
       "12662  31652.0          0.082532          0.317979   0.525401   \n",
       "12663  31655.0          0.117321          0.323369   0.573651   \n",
       "\n",
       "       LABEL_Alkalinephos  LABEL_Bilirubin_total  LABEL_Lactate  \\\n",
       "0                0.961712               0.913878       0.849364   \n",
       "1                0.480553               0.439553       0.238957   \n",
       "2                0.433482               0.425904       0.277688   \n",
       "3                0.961004               0.867512       0.672507   \n",
       "4                0.562898               0.542512       0.396905   \n",
       "...                   ...                    ...            ...   \n",
       "12659            0.373973               0.411888       0.300359   \n",
       "12660            0.811736               0.611820       0.340112   \n",
       "12661            0.461187               0.443320       0.697765   \n",
       "12662            0.593110               0.526376       0.207084   \n",
       "12663            0.594063               0.588049       0.225370   \n",
       "\n",
       "       LABEL_TroponinI  LABEL_SaO2  LABEL_Bilirubin_direct  LABEL_EtCO2  \n",
       "0             0.150986    0.603449                0.647068     0.434431  \n",
       "1             0.698675    0.274292                0.446310     0.232469  \n",
       "2             0.473744    0.404472                0.446310     0.218275  \n",
       "3             0.352497    0.647528                0.911838     0.269971  \n",
       "4             0.221129    0.206536                0.570115     0.324562  \n",
       "...                ...         ...                     ...          ...  \n",
       "12659         0.094801    0.290623                0.446310     0.232469  \n",
       "12660         0.677199    0.271969                0.739103     0.281824  \n",
       "12661         0.273953    0.724276                0.364428     0.437264  \n",
       "12662         0.635182    0.299705                0.243108     0.341546  \n",
       "12663         0.345621    0.274292                0.446310     0.413635  \n",
       "\n",
       "[12664 rows x 11 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred_medical_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling sepsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>EtCO2</th>\n",
       "      <th>PTT</th>\n",
       "      <th>BUN</th>\n",
       "      <th>Lactate</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Hgb</th>\n",
       "      <th>HCO3</th>\n",
       "      <th>BaseExcess</th>\n",
       "      <th>RRate</th>\n",
       "      <th>...</th>\n",
       "      <th>PaCO2_max</th>\n",
       "      <th>FiO2_max</th>\n",
       "      <th>Glucose_max</th>\n",
       "      <th>ABPm_max</th>\n",
       "      <th>ABPd_max</th>\n",
       "      <th>SpO2_max</th>\n",
       "      <th>Hct_max</th>\n",
       "      <th>Heartrate_max</th>\n",
       "      <th>ABPs_max</th>\n",
       "      <th>pH_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.0</td>\n",
       "      <td>10.457671</td>\n",
       "      <td>19.379766</td>\n",
       "      <td>8.085624</td>\n",
       "      <td>1.211180</td>\n",
       "      <td>36.853339</td>\n",
       "      <td>4.444106</td>\n",
       "      <td>10.845164</td>\n",
       "      <td>-0.222014</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>0.25</td>\n",
       "      <td>27.125</td>\n",
       "      <td>59.666667</td>\n",
       "      <td>42.333333</td>\n",
       "      <td>49.75</td>\n",
       "      <td>8.225</td>\n",
       "      <td>42.333333</td>\n",
       "      <td>79.916667</td>\n",
       "      <td>1.315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86.0</td>\n",
       "      <td>14.616151</td>\n",
       "      <td>39.361886</td>\n",
       "      <td>22.099093</td>\n",
       "      <td>1.971963</td>\n",
       "      <td>36.309616</td>\n",
       "      <td>8.466490</td>\n",
       "      <td>19.535822</td>\n",
       "      <td>-0.846701</td>\n",
       "      <td>18.023750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66.0</td>\n",
       "      <td>15.791465</td>\n",
       "      <td>37.113623</td>\n",
       "      <td>16.386752</td>\n",
       "      <td>1.874285</td>\n",
       "      <td>36.809351</td>\n",
       "      <td>8.364038</td>\n",
       "      <td>19.664941</td>\n",
       "      <td>-0.539940</td>\n",
       "      <td>14.931951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66.0</td>\n",
       "      <td>15.622747</td>\n",
       "      <td>38.511719</td>\n",
       "      <td>18.504078</td>\n",
       "      <td>2.037579</td>\n",
       "      <td>37.166667</td>\n",
       "      <td>10.195928</td>\n",
       "      <td>19.913676</td>\n",
       "      <td>-2.301866</td>\n",
       "      <td>15.833333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42.0</td>\n",
       "      <td>15.473433</td>\n",
       "      <td>36.413614</td>\n",
       "      <td>15.481142</td>\n",
       "      <td>1.825637</td>\n",
       "      <td>36.512461</td>\n",
       "      <td>8.253512</td>\n",
       "      <td>19.023452</td>\n",
       "      <td>-0.351043</td>\n",
       "      <td>17.255125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18990</th>\n",
       "      <td>52.0</td>\n",
       "      <td>17.530511</td>\n",
       "      <td>38.407358</td>\n",
       "      <td>16.974113</td>\n",
       "      <td>1.817651</td>\n",
       "      <td>37.262373</td>\n",
       "      <td>8.862815</td>\n",
       "      <td>20.913225</td>\n",
       "      <td>-0.813232</td>\n",
       "      <td>15.767475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18991</th>\n",
       "      <td>66.0</td>\n",
       "      <td>14.685580</td>\n",
       "      <td>37.877243</td>\n",
       "      <td>21.038035</td>\n",
       "      <td>1.679079</td>\n",
       "      <td>36.705730</td>\n",
       "      <td>8.602569</td>\n",
       "      <td>19.593550</td>\n",
       "      <td>-0.446349</td>\n",
       "      <td>17.023402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18992</th>\n",
       "      <td>44.0</td>\n",
       "      <td>16.590601</td>\n",
       "      <td>41.245585</td>\n",
       "      <td>18.441434</td>\n",
       "      <td>2.324881</td>\n",
       "      <td>37.162000</td>\n",
       "      <td>8.638379</td>\n",
       "      <td>19.648159</td>\n",
       "      <td>-1.735771</td>\n",
       "      <td>23.791434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18993</th>\n",
       "      <td>70.0</td>\n",
       "      <td>15.375473</td>\n",
       "      <td>37.376038</td>\n",
       "      <td>20.542817</td>\n",
       "      <td>2.067433</td>\n",
       "      <td>36.492035</td>\n",
       "      <td>8.199492</td>\n",
       "      <td>19.450276</td>\n",
       "      <td>-0.858656</td>\n",
       "      <td>16.415617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18994</th>\n",
       "      <td>60.0</td>\n",
       "      <td>15.324444</td>\n",
       "      <td>37.754309</td>\n",
       "      <td>18.638508</td>\n",
       "      <td>2.076506</td>\n",
       "      <td>36.656467</td>\n",
       "      <td>8.751419</td>\n",
       "      <td>19.700195</td>\n",
       "      <td>-0.664748</td>\n",
       "      <td>17.913271</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18995 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age      EtCO2        PTT        BUN   Lactate       Temp        Hgb  \\\n",
       "0      34.0  10.457671  19.379766   8.085624  1.211180  36.853339   4.444106   \n",
       "1      86.0  14.616151  39.361886  22.099093  1.971963  36.309616   8.466490   \n",
       "2      66.0  15.791465  37.113623  16.386752  1.874285  36.809351   8.364038   \n",
       "3      66.0  15.622747  38.511719  18.504078  2.037579  37.166667  10.195928   \n",
       "4      42.0  15.473433  36.413614  15.481142  1.825637  36.512461   8.253512   \n",
       "...     ...        ...        ...        ...       ...        ...        ...   \n",
       "18990  52.0  17.530511  38.407358  16.974113  1.817651  37.262373   8.862815   \n",
       "18991  66.0  14.685580  37.877243  21.038035  1.679079  36.705730   8.602569   \n",
       "18992  44.0  16.590601  41.245585  18.441434  2.324881  37.162000   8.638379   \n",
       "18993  70.0  15.375473  37.376038  20.542817  2.067433  36.492035   8.199492   \n",
       "18994  60.0  15.324444  37.754309  18.638508  2.076506  36.656467   8.751419   \n",
       "\n",
       "            HCO3  BaseExcess      RRate  ...  PaCO2_max  FiO2_max  \\\n",
       "0      10.845164   -0.222014  17.000000  ...   7.333333      0.25   \n",
       "1      19.535822   -0.846701  18.023750  ...   0.000000      0.00   \n",
       "2      19.664941   -0.539940  14.931951  ...   0.000000      0.00   \n",
       "3      19.913676   -2.301866  15.833333  ...   0.000000      0.00   \n",
       "4      19.023452   -0.351043  17.255125  ...   0.000000      0.00   \n",
       "...          ...         ...        ...  ...        ...       ...   \n",
       "18990  20.913225   -0.813232  15.767475  ...   0.000000      0.00   \n",
       "18991  19.593550   -0.446349  17.023402  ...   0.000000      0.00   \n",
       "18992  19.648159   -1.735771  23.791434  ...   0.000000      0.00   \n",
       "18993  19.450276   -0.858656  16.415617  ...   0.000000      0.00   \n",
       "18994  19.700195   -0.664748  17.913271  ...   0.000000      0.00   \n",
       "\n",
       "       Glucose_max   ABPm_max   ABPd_max  SpO2_max  Hct_max  Heartrate_max  \\\n",
       "0           27.125  59.666667  42.333333     49.75    8.225      42.333333   \n",
       "1            0.000   0.000000   0.000000      0.00    0.000       0.000000   \n",
       "2            0.000   0.000000   0.000000      0.00    0.000       0.000000   \n",
       "3            0.000   0.000000   0.000000      0.00    0.000       0.000000   \n",
       "4            0.000   0.000000   0.000000      0.00    0.000       0.000000   \n",
       "...            ...        ...        ...       ...      ...            ...   \n",
       "18990        0.000   0.000000   0.000000      0.00    0.000       0.000000   \n",
       "18991        0.000   0.000000   0.000000      0.00    0.000       0.000000   \n",
       "18992        0.000   0.000000   0.000000      0.00    0.000       0.000000   \n",
       "18993        0.000   0.000000   0.000000      0.00    0.000       0.000000   \n",
       "18994        0.000   0.000000   0.000000      0.00    0.000       0.000000   \n",
       "\n",
       "        ABPs_max  pH_max  \n",
       "0      79.916667   1.315  \n",
       "1       0.000000   0.000  \n",
       "2       0.000000   0.000  \n",
       "3       0.000000   0.000  \n",
       "4       0.000000   0.000  \n",
       "...          ...     ...  \n",
       "18990   0.000000   0.000  \n",
       "18991   0.000000   0.000  \n",
       "18992   0.000000   0.000  \n",
       "18993   0.000000   0.000  \n",
       "18994   0.000000   0.000  \n",
       "\n",
       "[18995 rows x 95 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9  8 23 20  4 33 15 30 16  1 27 12 25  3  2 13 53  5 83 32 46 28 62 44\n",
      " 26 63 18 38 74 60 79 89 43 14 36 21 59 24 48 71 93 52 87 70 69 34 73 88\n",
      " 78 92 58 65 22 80 94 72 57 90 77 82 75 11 37 67 19 85 68 39 17  0 66 10\n",
      " 47 81 45 41 76 35  7 91 84 50  6 56 86 55 61 51 42 40 31 29 54 49 64]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 7200x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feature extraction\n",
    "feature_selector = SelectKBest(score_func=f_regression, k=X_train_scaled.shape[1])\n",
    "fit = feature_selector.fit(X_train_scaled, y_train_sepsis[0])\n",
    "\n",
    "# Get the indices sorted by most important to least important\n",
    "indices = np.argsort(fit.scores_)[::-1]\n",
    "\n",
    "print(indices)\n",
    "# To get your top 10 feature names\n",
    "features = []\n",
    "for i in range(X_train_scaled.shape[1]):\n",
    "    features.append(df_train.columns[indices[i]])\n",
    "\n",
    "# Now plot\n",
    "plt.rcParams['figure.figsize'] = 100, 10\n",
    "plt.figure()\n",
    "plt.bar(features, fit.scores_[indices[range(X_train_scaled.shape[1])]], color='r', align='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting model\n",
      "[0 0 0 ... 1 1 1]\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   22.4s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:   36.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC score on test set 0.6795178031877602\n",
      "CV score 0.6437475757575757\n",
      "Finished test for medical tests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   46.3s finished\n"
     ]
    }
   ],
   "source": [
    "# Model and predict sepsis\n",
    "\n",
    "clf = xgb.XGBClassifier(objective=\"binary:logistic\", n_thread=-1)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train_scaled, y_train_sepsis[0], test_size=0.10, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "        \"booster\": [\"dart\"],\n",
    "        \"eta\": np.arange(0,1,0.1),\n",
    "        \"min_child_weight\": range(1, 10, 1),\n",
    "        \"max_depth\": range(4, 10, 1),\n",
    "        \"gamma\": range(0, 100, 1),\n",
    "        \"max_delta_step\": range(1, 10, 1),\n",
    "        \"subsample\": np.arange(0.1, 1, 0.05),\n",
    "        \"colsample_bytree\": np.arange(0.3, 1, 0.05),\n",
    "        \"n_estimators\": range(50, 150, 1),\n",
    "        \"scale_pos_weight\": [1],\n",
    "        \"reg_lambda\": [0, 1], # Ridge regularization\n",
    "        \"reg_alpha\": [0, 1], # Lasso regularization\n",
    "        \"eval_metric\": [\"error\"],\n",
    "        \"verbosity\": [1]\n",
    "    }\n",
    "\n",
    "print(\"Resampling\")\n",
    "sampler = RandomUnderSampler()\n",
    "X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Applying feature selection\")\n",
    "feature_selector = SelectKBest(score_func=f_classif, k=10)\n",
    "X_train = feature_selector.fit_transform(X_train_res, y_train_res)\n",
    "X_test = feature_selector.transform(X_test)\n",
    "\n",
    "\n",
    "print(\"Fitting model\")\n",
    "coarse_search = RandomizedSearchCV(estimator=clf,\n",
    "        param_distributions=param_grid, scoring=\"roc_auc\",\n",
    "        n_jobs=-1, cv=10, n_iter=100, verbose=1)\n",
    "print(y_train_res)\n",
    "coarse_search.fit(X_train, y_train_res)\n",
    "\n",
    "sepsis_model = coarse_search.best_estimator_\n",
    "print(f\"ROC score on test set {roc_auc_score(y_test, coarse_search.best_estimator_.predict_proba(X_test)[:,1])}\")\n",
    "print(f\"CV score {coarse_search.best_score_}\")\n",
    "print(f\"Finished test for medical tests.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling\n",
      "Applying feature selection\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-634f414c4ecf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0msvc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0msepsis_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ROC score on test set {roc_auc_score(y_test, sigmoid_f(clf.best_estimator_.decision_function(X_test)))}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train_scaled, y_train_sepsis[0], test_size=0.10, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Resampling\")\n",
    "sampler = ADASYN()\n",
    "X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Applying feature selection\")\n",
    "feature_selector = SelectKBest(score_func=f_classif, k=60)\n",
    "X_train = feature_selector.fit_transform(X_train_res, y_train_res)\n",
    "X_test = feature_selector.transform(X_test)\n",
    "\n",
    "parameters = {\n",
    "        \"C\": np.linspace(0.1, 10, num=3),\n",
    "        \"kernel\": [\"rbf\", \"sigmoid\"],\n",
    "        \"gamma\": np.linspace(0.1, 10, num=3),  # for poly or rbf kernel\n",
    "        \"coef0\": [0],\n",
    "        \"shrinking\": [True],\n",
    "        \"probability\": [False],\n",
    "        \"cache_size\": [1000],\n",
    "        \"class_weight\": [None],\n",
    "        \"verbose\": [2],\n",
    "        \"decision_function_shape\": [\"ovo\"],  # only binary variables are set\n",
    "        \"random_state\": [42],\n",
    "        \"max_iter\": [2000]\n",
    "    }\n",
    "\n",
    "svc = SVC()\n",
    "clf = GridSearchCV(svc, parameters, cv=2, n_jobs=-1)\n",
    "clf.fit(X_train, y_train_res)\n",
    "sepsis_model = clf.best_estimator_\n",
    "print(f\"ROC score on test set {roc_auc_score(y_test, sigmoid_f(clf.best_estimator_.decision_function(X_test)))}\")\n",
    "print(f\"CV score {clf.best_score_}\")\n",
    "print(f\"Finished test for medical tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "    class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_ANN_sepsis(X_train_scaled,y_train_sepsis,n_layers,hidden_units):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_train_scaled, y_train_sepsis[0], test_size=0.10, random_state=42, shuffle=True\n",
    "    )   \n",
    "    \n",
    "    print(\"Resampling\")\n",
    "    sampler = ADASYN()\n",
    "    X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_units, activation=\"relu\", input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dropout(0.5))\n",
    "    for i in range(1,n_layers):\n",
    "        model.add(Dense(hidden_units, activation=\"relu\"))\n",
    "        model.add(Dropout(0.5))\n",
    "    model.add(Dense(1,activation=\"sigmoid\"))\n",
    "    \n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model.fit(X_train_res,y_train_res,epochs=40,batch_size=64)\n",
    "    score = model.evaluate(X_test, y_test, batch_size=64)\n",
    "    print(\"Accuracy : \",score)\n",
    "    y_pred = model.predict(X_test)\n",
    "    rauc = roc_auc_score(y_test, y_pred)\n",
    "    print(\"Roc auc : \",rauc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling\n",
      "Epoch 1/40\n",
      "32011/32011 [==============================] - 1s 32us/step - loss: 0.6947 - accuracy: 0.5507\n",
      "Epoch 2/40\n",
      "32011/32011 [==============================] - 1s 27us/step - loss: 0.6677 - accuracy: 0.5946\n",
      "Epoch 3/40\n",
      "32011/32011 [==============================] - 1s 27us/step - loss: 0.6561 - accuracy: 0.6201\n",
      "Epoch 4/40\n",
      "32011/32011 [==============================] - 1s 28us/step - loss: 0.6443 - accuracy: 0.6317\n",
      "Epoch 5/40\n",
      "32011/32011 [==============================] - 1s 28us/step - loss: 0.6364 - accuracy: 0.6491\n",
      "Epoch 6/40\n",
      "32011/32011 [==============================] - 1s 29us/step - loss: 0.6251 - accuracy: 0.6607\n",
      "Epoch 7/40\n",
      "32011/32011 [==============================] - 1s 29us/step - loss: 0.6175 - accuracy: 0.6688\n",
      "Epoch 8/40\n",
      "32011/32011 [==============================] - 1s 30us/step - loss: 0.6081 - accuracy: 0.6808\n",
      "Epoch 9/40\n",
      "32011/32011 [==============================] - 1s 29us/step - loss: 0.5957 - accuracy: 0.6877\n",
      "Epoch 10/40\n",
      "32011/32011 [==============================] - 1s 30us/step - loss: 0.5899 - accuracy: 0.6956\n",
      "Epoch 11/40\n",
      "32011/32011 [==============================] - 1s 32us/step - loss: 0.5840 - accuracy: 0.6995\n",
      "Epoch 12/40\n",
      "32011/32011 [==============================] - 1s 30us/step - loss: 0.5779 - accuracy: 0.7042\n",
      "Epoch 13/40\n",
      "32011/32011 [==============================] - 1s 33us/step - loss: 0.5735 - accuracy: 0.7106\n",
      "Epoch 14/40\n",
      "32011/32011 [==============================] - 1s 38us/step - loss: 0.5663 - accuracy: 0.7156\n",
      "Epoch 15/40\n",
      "32011/32011 [==============================] - 1s 42us/step - loss: 0.5641 - accuracy: 0.7170\n",
      "Epoch 16/40\n",
      "32011/32011 [==============================] - 1s 37us/step - loss: 0.5565 - accuracy: 0.7190\n",
      "Epoch 17/40\n",
      "32011/32011 [==============================] - 1s 35us/step - loss: 0.5510 - accuracy: 0.7257\n",
      "Epoch 18/40\n",
      "32011/32011 [==============================] - 1s 38us/step - loss: 0.5486 - accuracy: 0.7301\n",
      "Epoch 19/40\n",
      "32011/32011 [==============================] - 1s 37us/step - loss: 0.5441 - accuracy: 0.7317\n",
      "Epoch 20/40\n",
      "32011/32011 [==============================] - 1s 35us/step - loss: 0.5414 - accuracy: 0.7363\n",
      "Epoch 21/40\n",
      "32011/32011 [==============================] - 1s 40us/step - loss: 0.5369 - accuracy: 0.7372\n",
      "Epoch 22/40\n",
      "32011/32011 [==============================] - 1s 36us/step - loss: 0.5367 - accuracy: 0.7391\n",
      "Epoch 23/40\n",
      "32011/32011 [==============================] - 1s 36us/step - loss: 0.5337 - accuracy: 0.7407\n",
      "Epoch 24/40\n",
      "32011/32011 [==============================] - 1s 34us/step - loss: 0.5263 - accuracy: 0.7420\n",
      "Epoch 25/40\n",
      "32011/32011 [==============================] - 1s 32us/step - loss: 0.5313 - accuracy: 0.7459\n",
      "Epoch 26/40\n",
      "32011/32011 [==============================] - 1s 43us/step - loss: 0.5253 - accuracy: 0.7458\n",
      "Epoch 27/40\n",
      "32011/32011 [==============================] - 1s 41us/step - loss: 0.5266 - accuracy: 0.7465\n",
      "Epoch 28/40\n",
      "32011/32011 [==============================] - 1s 37us/step - loss: 0.5173 - accuracy: 0.7514\n",
      "Epoch 29/40\n",
      "32011/32011 [==============================] - 1s 36us/step - loss: 0.5144 - accuracy: 0.7563\n",
      "Epoch 30/40\n",
      "32011/32011 [==============================] - 1s 35us/step - loss: 0.5160 - accuracy: 0.7571\n",
      "Epoch 31/40\n",
      "32011/32011 [==============================] - 1s 34us/step - loss: 0.5113 - accuracy: 0.7561\n",
      "Epoch 32/40\n",
      "32011/32011 [==============================] - 1s 34us/step - loss: 0.5130 - accuracy: 0.7586\n",
      "Epoch 33/40\n",
      "32011/32011 [==============================] - 1s 33us/step - loss: 0.5066 - accuracy: 0.7595\n",
      "Epoch 34/40\n",
      "32011/32011 [==============================] - 1s 33us/step - loss: 0.5143 - accuracy: 0.7561\n",
      "Epoch 35/40\n",
      "32011/32011 [==============================] - 1s 34us/step - loss: 0.5070 - accuracy: 0.7637\n",
      "Epoch 36/40\n",
      "32011/32011 [==============================] - 1s 33us/step - loss: 0.5022 - accuracy: 0.7632\n",
      "Epoch 37/40\n",
      "32011/32011 [==============================] - 1s 33us/step - loss: 0.5074 - accuracy: 0.7624\n",
      "Epoch 38/40\n",
      "32011/32011 [==============================] - 1s 33us/step - loss: 0.4996 - accuracy: 0.7640\n",
      "Epoch 39/40\n",
      "32011/32011 [==============================] - 1s 33us/step - loss: 0.5035 - accuracy: 0.7619\n",
      "Epoch 40/40\n",
      "32011/32011 [==============================] - 1s 33us/step - loss: 0.5036 - accuracy: 0.7645\n",
      "1900/1900 [==============================] - 0s 21us/step\n",
      "Accuracy :  [0.4998707559861635, 0.7636842131614685]\n",
      "Roc auc :  0.5982541149901662\n"
     ]
    }
   ],
   "source": [
    "get_keras_ANN_sepsis(X_train_scaled, y_train_sepsis, 3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgboost_fine_sepsis.pkl']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(sepsis_model, f\"xgboost_fine_sepsis.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_sepsis = feature_selector.transform(X_val_scaled)\n",
    "y_pred = sepsis_model.predict_proba(X_val_sepsis)[:,1]\n",
    "df_pred_sepsis = pd.DataFrame(y_pred, index=val_pids, columns=SEPSIS)\n",
    "df_pred_sepsis = df_pred_sepsis.reset_index().rename(columns={\"index\": \"pid\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling vital signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model for LABEL_RRate.\n",
      "Applying feature selection\n",
      "Fitting model\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   18.6s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  8.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score 0.3762834615674595\n",
      "Test score is 0.3651876778962392\n",
      "Finished test for medical tests.\n",
      "Fitting model for LABEL_ABPm.\n",
      "Applying feature selection\n",
      "Fitting model\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  6.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score 0.5772479021545291\n",
      "Test score is 0.5894346373011152\n",
      "Finished test for medical tests.\n",
      "Fitting model for LABEL_SpO2.\n",
      "Applying feature selection\n",
      "Fitting model\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   49.5s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  4.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score 0.30896617018979444\n",
      "Test score is 0.3392388962178333\n",
      "Finished test for medical tests.\n",
      "Fitting model for LABEL_Heartrate.\n",
      "Applying feature selection\n",
      "Fitting model\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   18.3s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  8.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score 0.5974856981684256\n",
      "Test score is 0.6043524041430639\n",
      "Finished test for medical tests.\n"
     ]
    }
   ],
   "source": [
    "# Modelling of vital signs\n",
    "models = []\n",
    "losses = []\n",
    "feature_selectors_vital_signs = []\n",
    "clf = xgb.XGBRegressor(objective=\"reg:squarederror\", n_thread=-1)\n",
    "\n",
    "for i, sign in enumerate(VITAL_SIGNS):\n",
    "    print(f\"Fitting model for {sign}.\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_train_scaled, y_train_vital_signs[i], test_size=0.10, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    print(\"Applying feature selection\")\n",
    "    feature_selector = SelectKBest(score_func=f_classif, k=5)\n",
    "    X_train_selected = feature_selector.fit_transform(X_train, y_train)\n",
    "    X_test_selected = feature_selector.transform(X_test)\n",
    "    feature_selectors_vital_signs.append(feature_selector)\n",
    "\n",
    "    print(\"Fitting model\")\n",
    "    \n",
    "    param_grid = {\n",
    "        \"booster\": [\"dart\"],\n",
    "        \"eta\": np.arange(0,1,0.1),\n",
    "        \"min_child_weight\": range(1, 10, 1),\n",
    "        \"max_depth\": range(4, 10, 1),\n",
    "        \"gamma\": range(0, 100, 1),\n",
    "        \"max_delta_step\": range(1, 10, 1),\n",
    "        \"subsample\": np.arange(0.1, 1, 0.05),\n",
    "        \"colsample_bytree\": np.arange(0.3, 1, 0.05),\n",
    "        \"n_estimators\": range(50, 150, 1),\n",
    "        \"scale_pos_weight\": [1],\n",
    "        \"reg_lambda\": [0, 1], # Ridge regularization\n",
    "        \"reg_alpha\": [0, 1], # Lasso regularization\n",
    "        \"eval_metric\": [\"error\"],\n",
    "        \"verbosity\": [1]\n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "    coarse_search = RandomizedSearchCV(estimator=clf,\n",
    "            param_distributions=param_grid, scoring=\"r2\",\n",
    "            n_jobs=-1, cv=10, n_iter=100, verbose=1)\n",
    "    coarse_search.fit(X_train_selected, y_train)\n",
    "    models.append(coarse_search.best_estimator_)\n",
    "    print(f\"CV score {coarse_search.best_score_}\")\n",
    "    print(f\"Test score is {r2_score(y_test, coarse_search.best_estimator_.predict(X_test_selected))}\")\n",
    "    print(f\"Finished test for medical tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(models):\n",
    "    joblib.dump(models[i], f\"xgboost_fine_{VITAL_SIGNS[i]}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_keras(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_ANN_vital_signs(ind_vital_sign,X_train_scaled,y_train_vital_signs,n_layers,hidden_units,epochs):\n",
    "    print(f\"For vital sign {VITAL_SIGNS[ind_vital_sign]}\")\n",
    "    y_train_processed = []\n",
    "    for i in range(len(y_train_vital_signs[ind_vital_sign])):\n",
    "        y_train_processed+=[y_train_vital_signs[ind_vital_sign][i]]*12\n",
    "    y_train_processed = np.array(y_train_processed)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_train_scaled, y_train_processed, test_size=0.10, random_state=42, shuffle=True\n",
    "    )   \n",
    "    score_vs,models_vs = [],[]\n",
    "    for epoch in epochs:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hidden_units, activation=\"relu\", input_shape=(X_train.shape[1],)))\n",
    "        model.add(Dropout(0.5))\n",
    "        for i in range(1,n_layers):\n",
    "            model.add(Dense(hidden_units, activation=\"relu\"))\n",
    "            model.add(Dropout(0.5))\n",
    "        model.add(Dense(1,activation=\"relu\"))\n",
    "        opt = optimizers.Adam()\n",
    "        model.compile(optimizer=opt, loss=\"mse\", metrics=[r2_keras])\n",
    "        model.fit(X_train,y_train,epochs=int(epoch),batch_size=64)\n",
    "        y_pred = model.predict(X_test, batch_size=64)\n",
    "        r2s = r2_score(y_test,y_pred)\n",
    "        print(\"final r2 : \",r2s)\n",
    "        score_vs.append(r2s)\n",
    "        models_vs.append(model)\n",
    "    best_model = models_vs[np.argmax(score_vs)]\n",
    "    print(\"Appended model with r2 score \",np.max(score_vs))\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For vital sign LABEL_RRate\n",
      "[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]\n",
      "Epoch 1/50\n",
      "205146/205146 [==============================] - 11s 56us/step - loss: 33.3013 - r2_keras: -1.8540\n",
      "Epoch 2/50\n",
      "205146/205146 [==============================] - 12s 56us/step - loss: 17.6482 - r2_keras: -0.4773\n",
      "Epoch 3/50\n",
      "205146/205146 [==============================] - 12s 59us/step - loss: 16.0369 - r2_keras: -0.3383\n",
      "Epoch 4/50\n",
      "205146/205146 [==============================] - 12s 58us/step - loss: 15.3659 - r2_keras: -0.2795\n",
      "Epoch 5/50\n",
      "205146/205146 [==============================] - 12s 56us/step - loss: 14.9129 - r2_keras: -0.2397\n",
      "Epoch 6/50\n",
      "205146/205146 [==============================] - 12s 58us/step - loss: 14.4199 - r2_keras: -0.1974\n",
      "Epoch 7/50\n",
      "205146/205146 [==============================] - 12s 58us/step - loss: 14.0649 - r2_keras: -0.1650\n",
      "Epoch 8/50\n",
      "205146/205146 [==============================] - 12s 57us/step - loss: 13.7796 - r2_keras: -0.1411\n",
      "Epoch 9/50\n",
      "205146/205146 [==============================] - 12s 58us/step - loss: 13.5146 - r2_keras: -0.1174\n",
      "Epoch 10/50\n",
      "205146/205146 [==============================] - 12s 57us/step - loss: 13.3296 - r2_keras: -0.1011\n",
      "Epoch 11/50\n",
      "205146/205146 [==============================] - 12s 59us/step - loss: 13.1504 - r2_keras: -0.0851\n",
      "Epoch 12/50\n",
      "205146/205146 [==============================] - 12s 58us/step - loss: 12.9957 - r2_keras: -0.0717\n",
      "Epoch 13/50\n",
      "205146/205146 [==============================] - 12s 59us/step - loss: 12.9058 - r2_keras: -0.0645\n",
      "Epoch 14/50\n",
      "205146/205146 [==============================] - 12s 61us/step - loss: 12.8029 - r2_keras: -0.0550\n",
      "Epoch 15/50\n",
      "205146/205146 [==============================] - 12s 58us/step - loss: 12.7197 - r2_keras: -0.0476\n",
      "Epoch 16/50\n",
      "205146/205146 [==============================] - 12s 58us/step - loss: 12.6620 - r2_keras: -0.0426\n",
      "Epoch 17/50\n",
      "205146/205146 [==============================] - 12s 57us/step - loss: 12.6085 - r2_keras: -0.0378\n",
      "Epoch 18/50\n",
      "205146/205146 [==============================] - 12s 59us/step - loss: 12.5821 - r2_keras: -0.0356\n",
      "Epoch 19/50\n",
      "205146/205146 [==============================] - 12s 58us/step - loss: 12.5409 - r2_keras: -0.0319\n",
      "Epoch 20/50\n",
      "205146/205146 [==============================] - 12s 60us/step - loss: 12.5123 - r2_keras: -0.0297\n",
      "Epoch 21/50\n",
      "205146/205146 [==============================] - 13s 61us/step - loss: 12.4897 - r2_keras: -0.0281\n",
      "Epoch 22/50\n",
      "205146/205146 [==============================] - 12s 61us/step - loss: 12.4679 - r2_keras: -0.0261\n",
      "Epoch 23/50\n",
      "205146/205146 [==============================] - 12s 60us/step - loss: 12.4417 - r2_keras: -0.0233\n",
      "Epoch 24/50\n",
      "205146/205146 [==============================] - 12s 59us/step - loss: 12.4311 - r2_keras: -0.0225\n",
      "Epoch 25/50\n",
      "205146/205146 [==============================] - 12s 58us/step - loss: 12.4197 - r2_keras: -0.0212\n",
      "Epoch 26/50\n",
      "205146/205146 [==============================] - 12s 57us/step - loss: 12.4064 - r2_keras: -0.0204\n",
      "Epoch 27/50\n",
      "205146/205146 [==============================] - 12s 58us/step - loss: 12.3971 - r2_keras: -0.0199\n",
      "Epoch 28/50\n",
      "205146/205146 [==============================] - 13s 63us/step - loss: 12.3956 - r2_keras: -0.0197\n",
      "Epoch 29/50\n",
      "205146/205146 [==============================] - 12s 58us/step - loss: 12.3863 - r2_keras: -0.0194\n",
      "Epoch 30/50\n",
      "205146/205146 [==============================] - 12s 58us/step - loss: 12.3806 - r2_keras: -0.0196\n",
      "Epoch 31/50\n",
      "205146/205146 [==============================] - 12s 60us/step - loss: 12.3715 - r2_keras: -0.0169\n",
      "Epoch 32/50\n",
      "205146/205146 [==============================] - 12s 59us/step - loss: 12.3728 - r2_keras: -0.0174\n",
      "Epoch 33/50\n",
      "205146/205146 [==============================] - 12s 60us/step - loss: 12.3704 - r2_keras: -0.0181\n",
      "Epoch 34/50\n",
      "205146/205146 [==============================] - 12s 58us/step - loss: 12.3655 - r2_keras: -0.0169\n",
      "Epoch 35/50\n",
      "205146/205146 [==============================] - 12s 58us/step - loss: 12.3639 - r2_keras: -0.0178\n",
      "Epoch 36/50\n",
      "205146/205146 [==============================] - 14s 68us/step - loss: 12.3574 - r2_keras: -0.0164\n",
      "Epoch 37/50\n",
      "205146/205146 [==============================] - 12s 57us/step - loss: 12.3596 - r2_keras: -0.0165\n",
      "Epoch 38/50\n",
      "205146/205146 [==============================] - 12s 60us/step - loss: 12.3556 - r2_keras: -0.0167\n",
      "Epoch 39/50\n",
      "205146/205146 [==============================] - 13s 61us/step - loss: 12.3565 - r2_keras: -0.0155\n",
      "Epoch 40/50\n",
      "205146/205146 [==============================] - 12s 57us/step - loss: 12.3537 - r2_keras: -0.0164\n",
      "Epoch 41/50\n",
      "205146/205146 [==============================] - 12s 60us/step - loss: 12.3595 - r2_keras: -0.0168\n",
      "Epoch 42/50\n",
      "205146/205146 [==============================] - 12s 58us/step - loss: 12.3535 - r2_keras: -0.0162\n",
      "Epoch 43/50\n",
      "205146/205146 [==============================] - 12s 58us/step - loss: 12.3549 - r2_keras: -0.0161\n",
      "Epoch 44/50\n",
      "205146/205146 [==============================] - 13s 61us/step - loss: 12.3557 - r2_keras: -0.0168\n",
      "Epoch 45/50\n",
      "205146/205146 [==============================] - 14s 68us/step - loss: 12.3557 - r2_keras: -0.0160\n",
      "Epoch 46/50\n",
      "205146/205146 [==============================] - 12s 60us/step - loss: 12.3613 - r2_keras: -0.0165\n",
      "Epoch 47/50\n",
      "205146/205146 [==============================] - 16s 77us/step - loss: 12.3566 - r2_keras: -0.0159\n",
      "Epoch 48/50\n",
      "205146/205146 [==============================] - 12s 61us/step - loss: 12.3535 - r2_keras: -0.0149\n",
      "Epoch 49/50\n",
      "205146/205146 [==============================] - 12s 59us/step - loss: 12.3521 - r2_keras: -0.0161\n",
      "Epoch 50/50\n",
      "205146/205146 [==============================] - 12s 57us/step - loss: 12.3538 - r2_keras: -0.0166\n",
      "final r2 :  0.0003717653852075564\n",
      "Appended model with r2 score  0.0003717653852075564\n"
     ]
    }
   ],
   "source": [
    "model_RRate = get_keras_ANN_vital_signs(0,X_train_scaled, y_train_vital_signs, 3, 100, [50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For vital sign LABEL_ABPm\n",
      "[104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104]\n",
      "Epoch 1/50\n",
      "205146/205146 [==============================] - 17s 85us/step - loss: 542.9726 - r2_keras: -2.4808\n",
      "Epoch 2/50\n",
      "205146/205146 [==============================] - 18s 87us/step - loss: 295.1536 - r2_keras: -0.9073\n",
      "Epoch 3/50\n",
      "205146/205146 [==============================] - 18s 87us/step - loss: 269.0950 - r2_keras: -0.6972\n",
      "Epoch 4/50\n",
      "205146/205146 [==============================] - 18s 88us/step - loss: 248.3092 - r2_keras: -0.5699\n",
      "Epoch 5/50\n",
      "205146/205146 [==============================] - 18s 87us/step - loss: 241.1824 - r2_keras: -0.5154\n",
      "Epoch 6/50\n",
      "205146/205146 [==============================] - 18s 88us/step - loss: 228.5690 - r2_keras: -0.4396\n",
      "Epoch 7/50\n",
      "205146/205146 [==============================] - 20s 96us/step - loss: 218.6331 - r2_keras: -0.3787\n",
      "Epoch 8/50\n",
      "127296/205146 [=================>............] - ETA: 6s - loss: 215.3761 - r2_keras: -0.3554"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-189-5f863795471e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_ABPm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_keras_ANN_vital_signs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_vital_signs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-186-9f153de70d07>\u001b[0m in \u001b[0;36mget_keras_ANN_vital_signs\u001b[1;34m(ind_vital_sign, X_train_scaled, y_train_vital_signs, n_layers, hidden_units, epochs)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"mse\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr2_keras\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mr2s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3740\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3742\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m     \"\"\"\n\u001b[1;32m-> 1081\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_ABPm = get_keras_ANN_vital_signs(1, X_train_scaled, y_train_vital_signs, 3, 150, [50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For vital sign LABEL_SpO2\n",
      "Epoch 1/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 1736.8876 - r2_keras: -512.5916\n",
      "Epoch 2/300\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 660.4639 - r2_keras: -193.2402\n",
      "Epoch 3/300\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 564.4192 - r2_keras: -163.8082\n",
      "Epoch 4/300\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 539.0846 - r2_keras: -162.1317\n",
      "Epoch 5/300\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 459.7267 - r2_keras: -132.4781\n",
      "Epoch 6/300\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 395.6827 - r2_keras: -115.0959\n",
      "Epoch 7/300\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 336.7852 - r2_keras: -97.8160\n",
      "Epoch 8/300\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 299.3357 - r2_keras: -86.0238\n",
      "Epoch 9/300\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 263.2170 - r2_keras: -76.6337\n",
      "Epoch 10/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 245.0307 - r2_keras: -70.0711\n",
      "Epoch 11/300\n",
      "17095/17095 [==============================] - 1s 66us/step - loss: 227.5894 - r2_keras: -65.3896\n",
      "Epoch 12/300\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 218.5915 - r2_keras: -62.5685\n",
      "Epoch 13/300\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 218.4816 - r2_keras: -62.2970\n",
      "Epoch 14/300\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 211.1229 - r2_keras: -61.5042\n",
      "Epoch 15/300\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 209.9905 - r2_keras: -60.3868\n",
      "Epoch 16/300\n",
      "17095/17095 [==============================] - 1s 52us/step - loss: 203.4788 - r2_keras: -58.9237\n",
      "Epoch 17/300\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 202.9529 - r2_keras: -58.6628\n",
      "Epoch 18/300\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 197.3187 - r2_keras: -55.6563\n",
      "Epoch 19/300\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 192.3355 - r2_keras: -54.8823\n",
      "Epoch 20/300\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 189.8517 - r2_keras: -54.1414\n",
      "Epoch 21/300\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 185.4397 - r2_keras: -52.5858\n",
      "Epoch 22/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 185.1797 - r2_keras: -52.8303\n",
      "Epoch 23/300\n",
      "17095/17095 [==============================] - 1s 65us/step - loss: 179.9799 - r2_keras: -51.5667\n",
      "Epoch 24/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 181.7767 - r2_keras: -52.4224\n",
      "Epoch 25/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 174.1235 - r2_keras: -49.5912\n",
      "Epoch 26/300\n",
      "17095/17095 [==============================] - 1s 73us/step - loss: 170.0121 - r2_keras: -48.2356\n",
      "Epoch 27/300\n",
      "17095/17095 [==============================] - 1s 69us/step - loss: 164.3541 - r2_keras: -47.4073\n",
      "Epoch 28/300\n",
      "17095/17095 [==============================] - 1s 76us/step - loss: 160.1867 - r2_keras: -45.2272\n",
      "Epoch 29/300\n",
      "17095/17095 [==============================] - 1s 77us/step - loss: 161.6583 - r2_keras: -46.9476\n",
      "Epoch 30/300\n",
      "17095/17095 [==============================] - 1s 72us/step - loss: 159.9423 - r2_keras: -45.5320\n",
      "Epoch 31/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 153.3100 - r2_keras: -44.1590\n",
      "Epoch 32/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 149.2277 - r2_keras: -42.9379\n",
      "Epoch 33/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 146.7054 - r2_keras: -41.5153\n",
      "Epoch 34/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 143.5693 - r2_keras: -40.7137\n",
      "Epoch 35/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 140.9901 - r2_keras: -39.9507\n",
      "Epoch 36/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 142.8020 - r2_keras: -40.1614\n",
      "Epoch 37/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 136.5459 - r2_keras: -39.1618\n",
      "Epoch 38/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 138.2251 - r2_keras: -38.6994\n",
      "Epoch 39/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 135.6075 - r2_keras: -38.4291\n",
      "Epoch 40/300\n",
      "17095/17095 [==============================] - 1s 64us/step - loss: 135.3663 - r2_keras: -38.0468\n",
      "Epoch 41/300\n",
      "17095/17095 [==============================] - 1s 64us/step - loss: 132.6116 - r2_keras: -38.2654\n",
      "Epoch 42/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 132.1914 - r2_keras: -37.2875\n",
      "Epoch 43/300\n",
      "17095/17095 [==============================] - 1s 70us/step - loss: 132.7797 - r2_keras: -37.7767\n",
      "Epoch 44/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 129.5193 - r2_keras: -36.5887\n",
      "Epoch 45/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 132.2873 - r2_keras: -37.2158\n",
      "Epoch 46/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 129.4872 - r2_keras: -36.4060\n",
      "Epoch 47/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 127.0346 - r2_keras: -37.4254\n",
      "Epoch 48/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 126.8002 - r2_keras: -35.6586\n",
      "Epoch 49/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 126.8552 - r2_keras: -35.7688\n",
      "Epoch 50/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 127.8158 - r2_keras: -36.3846\n",
      "Epoch 51/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 122.3765 - r2_keras: -34.3729\n",
      "Epoch 52/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 124.1203 - r2_keras: -35.0653\n",
      "Epoch 53/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 121.0510 - r2_keras: -35.0525\n",
      "Epoch 54/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 120.1914 - r2_keras: -34.0277\n",
      "Epoch 55/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 118.1540 - r2_keras: -33.2893\n",
      "Epoch 56/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 121.3256 - r2_keras: -34.6178\n",
      "Epoch 57/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 118.1932 - r2_keras: -33.7680\n",
      "Epoch 58/300\n",
      "17095/17095 [==============================] - 1s 63us/step - loss: 119.6347 - r2_keras: -33.5406\n",
      "Epoch 59/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 119.8809 - r2_keras: -33.4585\n",
      "Epoch 60/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 119.7698 - r2_keras: -33.5578\n",
      "Epoch 61/300\n",
      "17095/17095 [==============================] - 1s 72us/step - loss: 117.6435 - r2_keras: -32.8386\n",
      "Epoch 62/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 118.0666 - r2_keras: -32.9946\n",
      "Epoch 63/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 117.1337 - r2_keras: -33.7759\n",
      "Epoch 64/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 116.6042 - r2_keras: -32.6813\n",
      "Epoch 65/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 115.3691 - r2_keras: -33.2164\n",
      "Epoch 66/300\n",
      "17095/17095 [==============================] - 1s 63us/step - loss: 115.0361 - r2_keras: -32.7135\n",
      "Epoch 67/300\n",
      "17095/17095 [==============================] - 1s 71us/step - loss: 114.8999 - r2_keras: -32.5870\n",
      "Epoch 68/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 112.1153 - r2_keras: -31.2926\n",
      "Epoch 69/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 114.2279 - r2_keras: -31.9897\n",
      "Epoch 70/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 111.8522 - r2_keras: -31.4744\n",
      "Epoch 71/300\n",
      "17095/17095 [==============================] - 1s 63us/step - loss: 112.7901 - r2_keras: -31.5412\n",
      "Epoch 72/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 108.3207 - r2_keras: -30.2581\n",
      "Epoch 73/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 109.6735 - r2_keras: -30.8710\n",
      "Epoch 74/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17095/17095 [==============================] - 1s 62us/step - loss: 109.1064 - r2_keras: -30.6315\n",
      "Epoch 75/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 108.3293 - r2_keras: -30.8074\n",
      "Epoch 76/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 106.5762 - r2_keras: -30.0614\n",
      "Epoch 77/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 107.3276 - r2_keras: -30.0037\n",
      "Epoch 78/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 108.0416 - r2_keras: -30.4520\n",
      "Epoch 79/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 107.0758 - r2_keras: -30.4207\n",
      "Epoch 80/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 106.7662 - r2_keras: -30.1006\n",
      "Epoch 81/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 102.6674 - r2_keras: -28.5975\n",
      "Epoch 82/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 102.7593 - r2_keras: -28.5199\n",
      "Epoch 83/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 100.9028 - r2_keras: -28.1706\n",
      "Epoch 84/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 100.4892 - r2_keras: -28.2292\n",
      "Epoch 85/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 99.8036 - r2_keras: -27.9759\n",
      "Epoch 86/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 99.8900 - r2_keras: -28.0512\n",
      "Epoch 87/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 101.2222 - r2_keras: -28.6812\n",
      "Epoch 88/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 98.6721 - r2_keras: -28.0857\n",
      "Epoch 89/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 95.7766 - r2_keras: -26.9255\n",
      "Epoch 90/300\n",
      "17095/17095 [==============================] - 1s 54us/step - loss: 99.7009 - r2_keras: -28.1670\n",
      "Epoch 91/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 97.6795 - r2_keras: -27.5752\n",
      "Epoch 92/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 98.8344 - r2_keras: -27.5780\n",
      "Epoch 93/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 95.9416 - r2_keras: -26.9264\n",
      "Epoch 94/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 94.2413 - r2_keras: -26.6098\n",
      "Epoch 95/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 93.7522 - r2_keras: -26.3629\n",
      "Epoch 96/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 93.0793 - r2_keras: -26.6920\n",
      "Epoch 97/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 93.3014 - r2_keras: -26.2165\n",
      "Epoch 98/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 94.4440 - r2_keras: -26.6016\n",
      "Epoch 99/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 93.1714 - r2_keras: -25.7401\n",
      "Epoch 100/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 93.9696 - r2_keras: -25.8725\n",
      "Epoch 101/300\n",
      "17095/17095 [==============================] - 1s 54us/step - loss: 91.2755 - r2_keras: -25.1511\n",
      "Epoch 102/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 91.4078 - r2_keras: -27.5395\n",
      "Epoch 103/300\n",
      "17095/17095 [==============================] - 1s 79us/step - loss: 91.7672 - r2_keras: -25.7542\n",
      "Epoch 104/300\n",
      "17095/17095 [==============================] - 1s 79us/step - loss: 91.0151 - r2_keras: -25.2872\n",
      "Epoch 105/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 88.8316 - r2_keras: -25.2232\n",
      "Epoch 106/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 91.6110 - r2_keras: -25.8749\n",
      "Epoch 107/300\n",
      "17095/17095 [==============================] - 1s 65us/step - loss: 90.2128 - r2_keras: -25.1026\n",
      "Epoch 108/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 89.9551 - r2_keras: -25.0415\n",
      "Epoch 109/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 91.3774 - r2_keras: -25.5464\n",
      "Epoch 110/300\n",
      "17095/17095 [==============================] - 1s 76us/step - loss: 88.6856 - r2_keras: -24.7147\n",
      "Epoch 111/300\n",
      "17095/17095 [==============================] - 1s 65us/step - loss: 89.7553 - r2_keras: -24.9899\n",
      "Epoch 112/300\n",
      "17095/17095 [==============================] - 1s 77us/step - loss: 87.3865 - r2_keras: -24.3355\n",
      "Epoch 113/300\n",
      "17095/17095 [==============================] - 1s 78us/step - loss: 87.8353 - r2_keras: -24.7448\n",
      "Epoch 114/300\n",
      "17095/17095 [==============================] - 1s 77us/step - loss: 89.0221 - r2_keras: -24.8789\n",
      "Epoch 115/300\n",
      "17095/17095 [==============================] - 1s 68us/step - loss: 85.7495 - r2_keras: -23.8244\n",
      "Epoch 116/300\n",
      "17095/17095 [==============================] - 1s 63us/step - loss: 86.2867 - r2_keras: -24.2955\n",
      "Epoch 117/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 85.9063 - r2_keras: -24.1023\n",
      "Epoch 118/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 85.2274 - r2_keras: -23.5872\n",
      "Epoch 119/300\n",
      "17095/17095 [==============================] - 1s 64us/step - loss: 85.7308 - r2_keras: -23.8543\n",
      "Epoch 120/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 85.7664 - r2_keras: -23.6200\n",
      "Epoch 121/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 84.4299 - r2_keras: -23.5110\n",
      "Epoch 122/300\n",
      "17095/17095 [==============================] - 1s 81us/step - loss: 85.4555 - r2_keras: -23.6739\n",
      "Epoch 123/300\n",
      "17095/17095 [==============================] - 1s 72us/step - loss: 82.6543 - r2_keras: -22.8469\n",
      "Epoch 124/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 82.9947 - r2_keras: -23.1402\n",
      "Epoch 125/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 82.3145 - r2_keras: -22.9115\n",
      "Epoch 126/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 81.7136 - r2_keras: -22.6533\n",
      "Epoch 127/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 82.3212 - r2_keras: -23.3319\n",
      "Epoch 128/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 81.6009 - r2_keras: -23.0394\n",
      "Epoch 129/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 80.9991 - r2_keras: -22.9392\n",
      "Epoch 130/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 80.9171 - r2_keras: -22.3787\n",
      "Epoch 131/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 80.4381 - r2_keras: -22.4729\n",
      "Epoch 132/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 79.0745 - r2_keras: -22.1054\n",
      "Epoch 133/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 79.1853 - r2_keras: -22.3575\n",
      "Epoch 134/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 79.9501 - r2_keras: -22.6003\n",
      "Epoch 135/300\n",
      "17095/17095 [==============================] - 1s 63us/step - loss: 78.7440 - r2_keras: -21.7328\n",
      "Epoch 136/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 78.8838 - r2_keras: -21.5110\n",
      "Epoch 137/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 77.3690 - r2_keras: -21.4336\n",
      "Epoch 138/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 76.4589 - r2_keras: -21.1916\n",
      "Epoch 139/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 76.6265 - r2_keras: -21.0781\n",
      "Epoch 140/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 77.1832 - r2_keras: -21.4474\n",
      "Epoch 141/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 78.3589 - r2_keras: -21.8579\n",
      "Epoch 142/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 76.4495 - r2_keras: -21.2646\n",
      "Epoch 143/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 77.0877 - r2_keras: -21.3465\n",
      "Epoch 144/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 74.7379 - r2_keras: -20.9511\n",
      "Epoch 145/300\n",
      "17095/17095 [==============================] - 1s 63us/step - loss: 75.6211 - r2_keras: -20.8834\n",
      "Epoch 146/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 75.1914 - r2_keras: -20.6735\n",
      "Epoch 147/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 75.2652 - r2_keras: -21.1656\n",
      "Epoch 148/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17095/17095 [==============================] - 1s 56us/step - loss: 74.2139 - r2_keras: -20.6027\n",
      "Epoch 149/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 73.6640 - r2_keras: -20.1461\n",
      "Epoch 150/300\n",
      "17095/17095 [==============================] - 1s 53us/step - loss: 72.5748 - r2_keras: -20.0998\n",
      "Epoch 151/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 72.9636 - r2_keras: -20.3087\n",
      "Epoch 152/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 72.2745 - r2_keras: -19.9582\n",
      "Epoch 153/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 72.0144 - r2_keras: -19.9464\n",
      "Epoch 154/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 70.5609 - r2_keras: -19.5586\n",
      "Epoch 155/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 71.5490 - r2_keras: -19.6873\n",
      "Epoch 156/300\n",
      "17095/17095 [==============================] - 1s 54us/step - loss: 71.2213 - r2_keras: -19.6854\n",
      "Epoch 157/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 71.2279 - r2_keras: -19.5548\n",
      "Epoch 158/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 69.7113 - r2_keras: -19.3501\n",
      "Epoch 159/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 70.7917 - r2_keras: -19.6898\n",
      "Epoch 160/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 70.0497 - r2_keras: -19.7444\n",
      "Epoch 161/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 69.9868 - r2_keras: -19.3030\n",
      "Epoch 162/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 69.0404 - r2_keras: -19.0385\n",
      "Epoch 163/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 69.3372 - r2_keras: -19.1249\n",
      "Epoch 164/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 69.3365 - r2_keras: -18.8344\n",
      "Epoch 165/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 67.5992 - r2_keras: -18.5182\n",
      "Epoch 166/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 67.5713 - r2_keras: -18.5944\n",
      "Epoch 167/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 66.1562 - r2_keras: -18.1710\n",
      "Epoch 168/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 68.3494 - r2_keras: -20.6249\n",
      "Epoch 169/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 67.3636 - r2_keras: -18.7588\n",
      "Epoch 170/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 65.3687 - r2_keras: -18.0766\n",
      "Epoch 171/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 65.9562 - r2_keras: -17.9125\n",
      "Epoch 172/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 66.2528 - r2_keras: -18.2677\n",
      "Epoch 173/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 66.9761 - r2_keras: -18.6030\n",
      "Epoch 174/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 64.7726 - r2_keras: -17.7083\n",
      "Epoch 175/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 64.3361 - r2_keras: -18.0738\n",
      "Epoch 176/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 64.4204 - r2_keras: -17.7943\n",
      "Epoch 177/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 63.8816 - r2_keras: -17.4796\n",
      "Epoch 178/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 62.9557 - r2_keras: -17.4603\n",
      "Epoch 179/300\n",
      "17095/17095 [==============================] - 1s 54us/step - loss: 64.2244 - r2_keras: -17.6392\n",
      "Epoch 180/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 64.2250 - r2_keras: -17.5635\n",
      "Epoch 181/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 62.0535 - r2_keras: -17.1126\n",
      "Epoch 182/300\n",
      "17095/17095 [==============================] - 1s 54us/step - loss: 62.9115 - r2_keras: -17.5995\n",
      "Epoch 183/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 62.4607 - r2_keras: -17.0364\n",
      "Epoch 184/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 62.8179 - r2_keras: -17.5082\n",
      "Epoch 185/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 63.3073 - r2_keras: -17.3875\n",
      "Epoch 186/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 61.4749 - r2_keras: -17.0447\n",
      "Epoch 187/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 62.3604 - r2_keras: -17.1472\n",
      "Epoch 188/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 59.4476 - r2_keras: -16.2857\n",
      "Epoch 189/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 60.7463 - r2_keras: -16.4983\n",
      "Epoch 190/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 60.9150 - r2_keras: -16.5914\n",
      "Epoch 191/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 59.8979 - r2_keras: -16.4098\n",
      "Epoch 192/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 58.5591 - r2_keras: -16.0160\n",
      "Epoch 193/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 59.0482 - r2_keras: -16.1161\n",
      "Epoch 194/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 59.0481 - r2_keras: -16.2899\n",
      "Epoch 195/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 59.3784 - r2_keras: -16.1780\n",
      "Epoch 196/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 58.5956 - r2_keras: -15.8073\n",
      "Epoch 197/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 57.6713 - r2_keras: -15.7124\n",
      "Epoch 198/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 58.3394 - r2_keras: -15.8240\n",
      "Epoch 199/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 57.8202 - r2_keras: -15.9070\n",
      "Epoch 200/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 57.0925 - r2_keras: -15.7767\n",
      "Epoch 201/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 56.2450 - r2_keras: -15.4606\n",
      "Epoch 202/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 57.7725 - r2_keras: -15.5608\n",
      "Epoch 203/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 55.5787 - r2_keras: -15.2414\n",
      "Epoch 204/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 56.6337 - r2_keras: -15.4254\n",
      "Epoch 205/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 56.3650 - r2_keras: -15.1905\n",
      "Epoch 206/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 55.0810 - r2_keras: -14.9201\n",
      "Epoch 207/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 55.3745 - r2_keras: -15.0052\n",
      "Epoch 208/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 54.2631 - r2_keras: -14.6492\n",
      "Epoch 209/300\n",
      "17095/17095 [==============================] - 1s 63us/step - loss: 53.7759 - r2_keras: -14.7990\n",
      "Epoch 210/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 54.2338 - r2_keras: -14.8233\n",
      "Epoch 211/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 54.0624 - r2_keras: -14.6450\n",
      "Epoch 212/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 53.5391 - r2_keras: -14.6338\n",
      "Epoch 213/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 53.3496 - r2_keras: -14.4412\n",
      "Epoch 214/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 54.0087 - r2_keras: -14.8015\n",
      "Epoch 215/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 51.8680 - r2_keras: -14.1714\n",
      "Epoch 216/300\n",
      "17095/17095 [==============================] - 1s 63us/step - loss: 52.5974 - r2_keras: -14.2730\n",
      "Epoch 217/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 52.4972 - r2_keras: -14.0800\n",
      "Epoch 218/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 53.0522 - r2_keras: -14.2830\n",
      "Epoch 219/300\n",
      "17095/17095 [==============================] - 1s 64us/step - loss: 51.6900 - r2_keras: -13.9750\n",
      "Epoch 220/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 51.5310 - r2_keras: -14.0840\n",
      "Epoch 221/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17095/17095 [==============================] - 1s 56us/step - loss: 52.0201 - r2_keras: -14.0563\n",
      "Epoch 222/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 50.8518 - r2_keras: -13.7355\n",
      "Epoch 223/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 51.2379 - r2_keras: -13.8218\n",
      "Epoch 224/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 50.1676 - r2_keras: -13.5117\n",
      "Epoch 225/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 49.0222 - r2_keras: -13.2696\n",
      "Epoch 226/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 50.5353 - r2_keras: -13.4521\n",
      "Epoch 227/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 49.4452 - r2_keras: -13.1759\n",
      "Epoch 228/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 48.0723 - r2_keras: -12.9299\n",
      "Epoch 229/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 48.9413 - r2_keras: -13.0814\n",
      "Epoch 230/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 48.6259 - r2_keras: -13.0738\n",
      "Epoch 231/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 47.7632 - r2_keras: -12.8952\n",
      "Epoch 232/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 48.6107 - r2_keras: -13.0608\n",
      "Epoch 233/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 47.5510 - r2_keras: -12.8340\n",
      "Epoch 234/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 47.5324 - r2_keras: -12.8501\n",
      "Epoch 235/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 48.0630 - r2_keras: -12.8557\n",
      "Epoch 236/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 47.6013 - r2_keras: -12.8264\n",
      "Epoch 237/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 47.5384 - r2_keras: -12.7256\n",
      "Epoch 238/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 46.4903 - r2_keras: -12.4665\n",
      "Epoch 239/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 47.0233 - r2_keras: -12.5327\n",
      "Epoch 240/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 46.6540 - r2_keras: -12.4936\n",
      "Epoch 241/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 46.4693 - r2_keras: -12.2740\n",
      "Epoch 242/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 45.1763 - r2_keras: -12.3100\n",
      "Epoch 243/300\n",
      "17095/17095 [==============================] - 1s 53us/step - loss: 45.0744 - r2_keras: -11.9846\n",
      "Epoch 244/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 44.7609 - r2_keras: -11.8612\n",
      "Epoch 245/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 44.7934 - r2_keras: -12.0374\n",
      "Epoch 246/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 44.8897 - r2_keras: -12.0406\n",
      "Epoch 247/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 44.2854 - r2_keras: -11.7748\n",
      "Epoch 248/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 43.5163 - r2_keras: -11.4927\n",
      "Epoch 249/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 43.4859 - r2_keras: -11.6766\n",
      "Epoch 250/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 43.8358 - r2_keras: -11.9401\n",
      "Epoch 251/300\n",
      "17095/17095 [==============================] - 1s 54us/step - loss: 43.3547 - r2_keras: -11.6068\n",
      "Epoch 252/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 42.9610 - r2_keras: -11.4730\n",
      "Epoch 253/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 42.8790 - r2_keras: -11.3333\n",
      "Epoch 254/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 42.3185 - r2_keras: -11.0340\n",
      "Epoch 255/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 42.4483 - r2_keras: -11.0970\n",
      "Epoch 256/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 41.5729 - r2_keras: -10.9471\n",
      "Epoch 257/300\n",
      "17095/17095 [==============================] - 1s 54us/step - loss: 42.3351 - r2_keras: -11.3074\n",
      "Epoch 258/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 42.2014 - r2_keras: -11.1359\n",
      "Epoch 259/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 41.4785 - r2_keras: -10.9786\n",
      "Epoch 260/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 41.2289 - r2_keras: -10.9972\n",
      "Epoch 261/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 41.8568 - r2_keras: -11.0876\n",
      "Epoch 262/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 40.6564 - r2_keras: -10.8164\n",
      "Epoch 263/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 39.9125 - r2_keras: -10.6193\n",
      "Epoch 264/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 40.0224 - r2_keras: -10.6739\n",
      "Epoch 265/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 39.3564 - r2_keras: -10.2007\n",
      "Epoch 266/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 39.4823 - r2_keras: -10.3726\n",
      "Epoch 267/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 39.8104 - r2_keras: -10.4516\n",
      "Epoch 268/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 39.2650 - r2_keras: -10.1342\n",
      "Epoch 269/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 38.6647 - r2_keras: -10.2505\n",
      "Epoch 270/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 37.9355 - r2_keras: -9.8449\n",
      "Epoch 271/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 38.9705 - r2_keras: -10.2591\n",
      "Epoch 272/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 38.7067 - r2_keras: -10.1200\n",
      "Epoch 273/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 36.7644 - r2_keras: -9.5861\n",
      "Epoch 274/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 37.6600 - r2_keras: -9.8679\n",
      "Epoch 275/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 37.2936 - r2_keras: -9.6365\n",
      "Epoch 276/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 37.4608 - r2_keras: -9.6851\n",
      "Epoch 277/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 37.6810 - r2_keras: -9.8905\n",
      "Epoch 278/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 36.6265 - r2_keras: -9.5494\n",
      "Epoch 279/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 35.8897 - r2_keras: -9.4179\n",
      "Epoch 280/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 36.2840 - r2_keras: -9.3240\n",
      "Epoch 281/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 36.1180 - r2_keras: -9.2600\n",
      "Epoch 282/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 35.5342 - r2_keras: -9.2558\n",
      "Epoch 283/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 36.0974 - r2_keras: -9.3216\n",
      "Epoch 284/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 36.1311 - r2_keras: -9.4789\n",
      "Epoch 285/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 35.1893 - r2_keras: -9.2421\n",
      "Epoch 286/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 35.5989 - r2_keras: -9.3212\n",
      "Epoch 287/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 34.4447 - r2_keras: -9.0288\n",
      "Epoch 288/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 34.5125 - r2_keras: -9.0270\n",
      "Epoch 289/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 34.4472 - r2_keras: -8.9181\n",
      "Epoch 290/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 33.7247 - r2_keras: -8.7763\n",
      "Epoch 291/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 33.5651 - r2_keras: -8.6532\n",
      "Epoch 292/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 33.1590 - r2_keras: -8.5742\n",
      "Epoch 293/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 33.3317 - r2_keras: -8.5342\n",
      "Epoch 294/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 33.0023 - r2_keras: -8.4599\n",
      "Epoch 295/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17095/17095 [==============================] - 1s 55us/step - loss: 32.8063 - r2_keras: -8.4910\n",
      "Epoch 296/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 32.5629 - r2_keras: -8.3715\n",
      "Epoch 297/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 32.0290 - r2_keras: -8.4896\n",
      "Epoch 298/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 32.4041 - r2_keras: -8.3748\n",
      "Epoch 299/300\n",
      "17095/17095 [==============================] - 1s 54us/step - loss: 31.9940 - r2_keras: -8.1032\n",
      "Epoch 300/300\n",
      "17095/17095 [==============================] - 1s 63us/step - loss: 31.9960 - r2_keras: -8.2393\n",
      "final r2 :  0.16983737618729322\n",
      "Appended model with r2 score  0.16983737618729322\n"
     ]
    }
   ],
   "source": [
    "model_SpO2 = get_keras_ANN_vital_signs(2, X_train_scaled, y_train_vital_signs, 4, 100, [300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SpO2 = new_models_vital_signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For vital sign LABEL_Heartrate\n",
      "Epoch 1/300\n",
      "17095/17095 [==============================] - 1s 52us/step - loss: 1219.0200 - r2_keras: -4.8700\n",
      "Epoch 2/300\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 492.5627 - r2_keras: -1.3833\n",
      "Epoch 3/300\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 425.9757 - r2_keras: -1.0468\n",
      "Epoch 4/300\n",
      "17095/17095 [==============================] - 1s 44us/step - loss: 412.8299 - r2_keras: -0.9796\n",
      "Epoch 5/300\n",
      "17095/17095 [==============================] - 1s 44us/step - loss: 369.9628 - r2_keras: -0.7764\n",
      "Epoch 6/300\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 351.3071 - r2_keras: -0.6732\n",
      "Epoch 7/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 334.4452 - r2_keras: -0.6026\n",
      "Epoch 8/300\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 317.0428 - r2_keras: -0.5235\n",
      "Epoch 9/300\n",
      "17095/17095 [==============================] - 1s 44us/step - loss: 301.8962 - r2_keras: -0.4563\n",
      "Epoch 10/300\n",
      "17095/17095 [==============================] - 1s 44us/step - loss: 284.8579 - r2_keras: -0.3713\n",
      "Epoch 11/300\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 279.7032 - r2_keras: -0.3542\n",
      "Epoch 12/300\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 267.6112 - r2_keras: -0.2837\n",
      "Epoch 13/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 254.2988 - r2_keras: -0.2386\n",
      "Epoch 14/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 249.9918 - r2_keras: -0.2019\n",
      "Epoch 15/300\n",
      "17095/17095 [==============================] - 1s 52us/step - loss: 247.2101 - r2_keras: -0.1946\n",
      "Epoch 16/300\n",
      "17095/17095 [==============================] - 1s 69us/step - loss: 241.0662 - r2_keras: -0.1645\n",
      "Epoch 17/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 244.6122 - r2_keras: -0.1801\n",
      "Epoch 18/300\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 238.7436 - r2_keras: -0.1514\n",
      "Epoch 19/300\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 234.4832 - r2_keras: -0.1349\n",
      "Epoch 20/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 232.5747 - r2_keras: -0.1223\n",
      "Epoch 21/300\n",
      "17095/17095 [==============================] - 1s 65us/step - loss: 235.5300 - r2_keras: -0.1354\n",
      "Epoch 22/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 225.2873 - r2_keras: -0.0919\n",
      "Epoch 23/300\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 225.8736 - r2_keras: -0.0879\n",
      "Epoch 24/300\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 222.3226 - r2_keras: -0.0633\n",
      "Epoch 25/300\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 221.3669 - r2_keras: -0.0594\n",
      "Epoch 26/300\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 220.7066 - r2_keras: -0.0564\n",
      "Epoch 27/300\n",
      "17095/17095 [==============================] - 1s 52us/step - loss: 215.1114 - r2_keras: -0.0324\n",
      "Epoch 28/300\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 213.3776 - r2_keras: -0.0290\n",
      "Epoch 29/300\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 210.4603 - r2_keras: -0.0087\n",
      "Epoch 30/300\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 209.3101 - r2_keras: -0.0405\n",
      "Epoch 31/300\n",
      "17095/17095 [==============================] - 1s 50us/step - loss: 208.9078 - r2_keras: 4.0009e-04\n",
      "Epoch 32/300\n",
      "17095/17095 [==============================] - 1s 50us/step - loss: 205.1740 - r2_keras: 0.0172\n",
      "Epoch 33/300\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 203.6074 - r2_keras: 0.0091\n",
      "Epoch 34/300\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 203.1243 - r2_keras: 0.0194\n",
      "Epoch 35/300\n",
      "17095/17095 [==============================] - 1s 52us/step - loss: 201.6616 - r2_keras: 0.0311\n",
      "Epoch 36/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 197.4644 - r2_keras: 0.0546\n",
      "Epoch 37/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 199.2326 - r2_keras: 0.0385\n",
      "Epoch 38/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 194.0313 - r2_keras: 0.0670\n",
      "Epoch 39/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 193.6935 - r2_keras: 0.0727\n",
      "Epoch 40/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 192.3387 - r2_keras: 0.0726\n",
      "Epoch 41/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 190.2621 - r2_keras: 0.0879\n",
      "Epoch 42/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 188.6928 - r2_keras: 0.0918\n",
      "Epoch 43/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 184.9568 - r2_keras: 0.1105\n",
      "Epoch 44/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 184.4399 - r2_keras: 0.1052\n",
      "Epoch 45/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 185.3712 - r2_keras: 0.1113\n",
      "Epoch 46/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 182.0583 - r2_keras: 0.1267\n",
      "Epoch 47/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 181.3297 - r2_keras: 0.1255\n",
      "Epoch 48/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 185.5416 - r2_keras: 0.1043\n",
      "Epoch 49/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 180.9635 - r2_keras: 0.1353\n",
      "Epoch 50/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 184.9233 - r2_keras: 0.1107\n",
      "Epoch 51/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 179.5079 - r2_keras: 0.1399\n",
      "Epoch 52/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 177.1625 - r2_keras: 0.1445\n",
      "Epoch 53/300\n",
      "17095/17095 [==============================] - 1s 63us/step - loss: 178.7290 - r2_keras: 0.1405\n",
      "Epoch 54/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 179.6025 - r2_keras: 0.1354\n",
      "Epoch 55/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 176.8435 - r2_keras: 0.1545\n",
      "Epoch 56/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 175.6834 - r2_keras: 0.1557\n",
      "Epoch 57/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 176.0435 - r2_keras: 0.1568\n",
      "Epoch 58/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 177.7926 - r2_keras: 0.1495\n",
      "Epoch 59/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 176.5280 - r2_keras: 0.1512\n",
      "Epoch 60/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 177.5745 - r2_keras: 0.1484\n",
      "Epoch 61/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 177.0222 - r2_keras: 0.1534\n",
      "Epoch 62/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 176.1312 - r2_keras: 0.1551\n",
      "Epoch 63/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 166.0929 - r2_keras: 0.2099\n",
      "Epoch 64/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 171.7838 - r2_keras: 0.1765\n",
      "Epoch 65/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 171.6853 - r2_keras: 0.1744\n",
      "Epoch 66/300\n",
      "17095/17095 [==============================] - 1s 76us/step - loss: 170.5055 - r2_keras: 0.1805\n",
      "Epoch 67/300\n",
      "17095/17095 [==============================] - 1s 73us/step - loss: 172.0267 - r2_keras: 0.1735\n",
      "Epoch 68/300\n",
      "17095/17095 [==============================] - 1s 82us/step - loss: 169.7656 - r2_keras: 0.1825\n",
      "Epoch 69/300\n",
      "17095/17095 [==============================] - 1s 71us/step - loss: 172.0587 - r2_keras: 0.1695\n",
      "Epoch 70/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 167.0003 - r2_keras: 0.1281\n",
      "Epoch 71/300\n",
      "17095/17095 [==============================] - 1s 63us/step - loss: 166.2566 - r2_keras: 0.2021\n",
      "Epoch 72/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 169.9586 - r2_keras: 0.1905\n",
      "Epoch 73/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 167.1558 - r2_keras: 0.2000\n",
      "Epoch 74/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 166.6182 - r2_keras: 0.1992\n",
      "Epoch 75/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17095/17095 [==============================] - 1s 75us/step - loss: 163.0536 - r2_keras: 0.2219\n",
      "Epoch 76/300\n",
      "17095/17095 [==============================] - 1s 66us/step - loss: 164.6682 - r2_keras: 0.2117\n",
      "Epoch 77/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 165.8784 - r2_keras: 0.2076\n",
      "Epoch 78/300\n",
      "17095/17095 [==============================] - 1s 64us/step - loss: 164.3541 - r2_keras: 0.2093\n",
      "Epoch 79/300\n",
      "17095/17095 [==============================] - 1s 65us/step - loss: 166.0693 - r2_keras: 0.1996\n",
      "Epoch 80/300\n",
      "17095/17095 [==============================] - 1s 66us/step - loss: 162.8675 - r2_keras: 0.2184\n",
      "Epoch 81/300\n",
      "17095/17095 [==============================] - 1s 66us/step - loss: 164.3367 - r2_keras: 0.2101\n",
      "Epoch 82/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 163.1601 - r2_keras: 0.2086\n",
      "Epoch 83/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 161.0324 - r2_keras: 0.2349\n",
      "Epoch 84/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 166.9692 - r2_keras: 0.1902\n",
      "Epoch 85/300\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 162.3583 - r2_keras: 0.2176\n",
      "Epoch 86/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 162.7352 - r2_keras: 0.2231\n",
      "Epoch 87/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 160.1302 - r2_keras: 0.2283\n",
      "Epoch 88/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 161.1606 - r2_keras: 0.2279\n",
      "Epoch 89/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 161.1519 - r2_keras: 0.2244\n",
      "Epoch 90/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 160.6846 - r2_keras: 0.2271\n",
      "Epoch 91/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 159.4647 - r2_keras: 0.2392\n",
      "Epoch 92/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 157.7262 - r2_keras: 0.2382\n",
      "Epoch 93/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 158.7691 - r2_keras: 0.2384\n",
      "Epoch 94/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 157.9021 - r2_keras: 0.2436\n",
      "Epoch 95/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 162.1465 - r2_keras: 0.2248\n",
      "Epoch 96/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 159.9061 - r2_keras: 0.2293\n",
      "Epoch 97/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 161.7047 - r2_keras: 0.2229\n",
      "Epoch 98/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 154.5669 - r2_keras: 0.2129\n",
      "Epoch 99/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 157.4024 - r2_keras: 0.2416\n",
      "Epoch 100/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 156.3528 - r2_keras: 0.2581\n",
      "Epoch 101/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 156.5991 - r2_keras: 0.2446\n",
      "Epoch 102/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 154.5601 - r2_keras: 0.2580\n",
      "Epoch 103/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 157.5523 - r2_keras: 0.2459\n",
      "Epoch 104/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 158.8628 - r2_keras: 0.2383\n",
      "Epoch 105/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 158.1016 - r2_keras: 0.2419\n",
      "Epoch 106/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 156.3993 - r2_keras: 0.2515\n",
      "Epoch 107/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 155.4899 - r2_keras: 0.2486\n",
      "Epoch 108/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 151.6247 - r2_keras: 0.2705\n",
      "Epoch 109/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 153.6880 - r2_keras: 0.2528\n",
      "Epoch 110/300\n",
      "17095/17095 [==============================] - 1s 70us/step - loss: 152.0683 - r2_keras: 0.2666\n",
      "Epoch 111/300\n",
      "17095/17095 [==============================] - 1s 75us/step - loss: 157.0786 - r2_keras: 0.2495\n",
      "Epoch 112/300\n",
      "17095/17095 [==============================] - 1s 77us/step - loss: 152.9989 - r2_keras: 0.2685\n",
      "Epoch 113/300\n",
      "17095/17095 [==============================] - 1s 75us/step - loss: 153.6413 - r2_keras: 0.2602\n",
      "Epoch 114/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 153.1476 - r2_keras: 0.2637\n",
      "Epoch 115/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 155.5807 - r2_keras: 0.2538\n",
      "Epoch 116/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 154.1706 - r2_keras: 0.2610\n",
      "Epoch 117/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 152.8293 - r2_keras: 0.2675\n",
      "Epoch 118/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 151.2111 - r2_keras: 0.2759\n",
      "Epoch 119/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 152.2320 - r2_keras: 0.2695\n",
      "Epoch 120/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 152.1663 - r2_keras: 0.2738\n",
      "Epoch 121/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 151.9565 - r2_keras: 0.2781\n",
      "Epoch 122/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 151.8131 - r2_keras: 0.2727\n",
      "Epoch 123/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 153.4630 - r2_keras: 0.2288\n",
      "Epoch 124/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 152.4515 - r2_keras: 0.2671\n",
      "Epoch 125/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 152.5246 - r2_keras: 0.2717\n",
      "Epoch 126/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 149.7009 - r2_keras: 0.2849\n",
      "Epoch 127/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 148.4717 - r2_keras: 0.2839\n",
      "Epoch 128/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 151.4464 - r2_keras: 0.2697\n",
      "Epoch 129/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 149.9643 - r2_keras: 0.2731\n",
      "Epoch 130/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 148.4708 - r2_keras: 0.2877\n",
      "Epoch 131/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 147.8753 - r2_keras: 0.2902\n",
      "Epoch 132/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 150.1761 - r2_keras: 0.2770\n",
      "Epoch 133/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 148.3674 - r2_keras: 0.2934\n",
      "Epoch 134/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 145.0393 - r2_keras: 0.3035\n",
      "Epoch 135/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 147.9803 - r2_keras: 0.2887\n",
      "Epoch 136/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 145.3123 - r2_keras: 0.2966\n",
      "Epoch 137/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 145.5606 - r2_keras: 0.3001\n",
      "Epoch 138/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 147.7291 - r2_keras: 0.2951\n",
      "Epoch 139/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 143.8277 - r2_keras: 0.3112\n",
      "Epoch 140/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 146.5853 - r2_keras: 0.3000\n",
      "Epoch 141/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 145.5589 - r2_keras: 0.3019\n",
      "Epoch 142/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 147.7471 - r2_keras: 0.2888\n",
      "Epoch 143/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 145.3706 - r2_keras: 0.3006\n",
      "Epoch 144/300\n",
      "17095/17095 [==============================] - 1s 64us/step - loss: 145.1359 - r2_keras: 0.3036\n",
      "Epoch 145/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 143.1705 - r2_keras: 0.3093\n",
      "Epoch 146/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 144.1634 - r2_keras: 0.3095\n",
      "Epoch 147/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 143.9189 - r2_keras: 0.2882\n",
      "Epoch 148/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 144.0617 - r2_keras: 0.3146\n",
      "Epoch 149/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17095/17095 [==============================] - 1s 58us/step - loss: 143.7555 - r2_keras: 0.3087\n",
      "Epoch 150/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 146.1784 - r2_keras: 0.3014\n",
      "Epoch 151/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 142.0893 - r2_keras: 0.3193\n",
      "Epoch 152/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 146.4584 - r2_keras: 0.2957\n",
      "Epoch 153/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 143.4141 - r2_keras: 0.3144\n",
      "Epoch 154/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 142.6556 - r2_keras: 0.3130\n",
      "Epoch 155/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 144.7705 - r2_keras: 0.3108\n",
      "Epoch 156/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 141.9583 - r2_keras: 0.3212\n",
      "Epoch 157/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 141.0034 - r2_keras: 0.3217\n",
      "Epoch 158/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 143.1874 - r2_keras: 0.3075\n",
      "Epoch 159/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 141.5886 - r2_keras: 0.3191\n",
      "Epoch 160/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 143.6880 - r2_keras: 0.3113\n",
      "Epoch 161/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 142.5707 - r2_keras: 0.3156\n",
      "Epoch 162/300\n",
      "17095/17095 [==============================] - 1s 64us/step - loss: 142.1549 - r2_keras: 0.3165\n",
      "Epoch 163/300\n",
      "17095/17095 [==============================] - 1s 73us/step - loss: 142.6892 - r2_keras: 0.3133\n",
      "Epoch 164/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 139.7977 - r2_keras: 0.3312\n",
      "Epoch 165/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 140.9813 - r2_keras: 0.3197\n",
      "Epoch 166/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 138.6490 - r2_keras: 0.3377\n",
      "Epoch 167/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 141.0962 - r2_keras: 0.3239\n",
      "Epoch 168/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 142.1869 - r2_keras: 0.3168\n",
      "Epoch 169/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 140.0765 - r2_keras: 0.3276\n",
      "Epoch 170/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 140.6072 - r2_keras: 0.3258\n",
      "Epoch 171/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 141.4583 - r2_keras: 0.3201\n",
      "Epoch 172/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 138.6652 - r2_keras: 0.3399\n",
      "Epoch 173/300\n",
      "17095/17095 [==============================] - 1s 64us/step - loss: 139.5959 - r2_keras: 0.3356\n",
      "Epoch 174/300\n",
      "17095/17095 [==============================] - 1s 64us/step - loss: 138.9201 - r2_keras: 0.3311\n",
      "Epoch 175/300\n",
      "17095/17095 [==============================] - 1s 66us/step - loss: 140.5061 - r2_keras: 0.3229\n",
      "Epoch 176/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 137.2079 - r2_keras: 0.3427\n",
      "Epoch 177/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 140.3311 - r2_keras: 0.3236\n",
      "Epoch 178/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 137.4518 - r2_keras: 0.3369\n",
      "Epoch 179/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 139.1591 - r2_keras: 0.3344\n",
      "Epoch 180/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 138.6545 - r2_keras: 0.3350\n",
      "Epoch 181/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 137.0999 - r2_keras: 0.3440\n",
      "Epoch 182/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 135.5834 - r2_keras: 0.3526\n",
      "Epoch 183/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 135.0861 - r2_keras: 0.3517\n",
      "Epoch 184/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 137.1710 - r2_keras: 0.3414\n",
      "Epoch 185/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 134.0052 - r2_keras: 0.3601\n",
      "Epoch 186/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 136.4088 - r2_keras: 0.3432\n",
      "Epoch 187/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 135.6637 - r2_keras: 0.3487\n",
      "Epoch 188/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 135.3924 - r2_keras: 0.3510\n",
      "Epoch 189/300\n",
      "17095/17095 [==============================] - 1s 77us/step - loss: 133.7411 - r2_keras: 0.3594\n",
      "Epoch 190/300\n",
      "17095/17095 [==============================] - 1s 78us/step - loss: 136.8199 - r2_keras: 0.3400\n",
      "Epoch 191/300\n",
      "17095/17095 [==============================] - 1s 75us/step - loss: 135.5774 - r2_keras: 0.3539\n",
      "Epoch 192/300\n",
      "17095/17095 [==============================] - 1s 72us/step - loss: 137.3806 - r2_keras: 0.3246\n",
      "Epoch 193/300\n",
      "17095/17095 [==============================] - 1s 63us/step - loss: 133.6454 - r2_keras: 0.3623\n",
      "Epoch 194/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 131.5416 - r2_keras: 0.3695\n",
      "Epoch 195/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 130.9985 - r2_keras: 0.3731\n",
      "Epoch 196/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 134.3114 - r2_keras: 0.3551\n",
      "Epoch 197/300\n",
      "17095/17095 [==============================] - 1s 64us/step - loss: 134.5927 - r2_keras: 0.3495\n",
      "Epoch 198/300\n",
      "17095/17095 [==============================] - 1s 74us/step - loss: 131.2979 - r2_keras: 0.3685\n",
      "Epoch 199/300\n",
      "17095/17095 [==============================] - 1s 79us/step - loss: 132.3846 - r2_keras: 0.3666\n",
      "Epoch 200/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 133.0604 - r2_keras: 0.3644\n",
      "Epoch 201/300\n",
      "17095/17095 [==============================] - 1s 68us/step - loss: 133.1963 - r2_keras: 0.3592\n",
      "Epoch 202/300\n",
      "17095/17095 [==============================] - 1s 64us/step - loss: 132.8969 - r2_keras: 0.3657\n",
      "Epoch 203/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 130.9713 - r2_keras: 0.3773\n",
      "Epoch 204/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 134.4673 - r2_keras: 0.3580\n",
      "Epoch 205/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 130.7224 - r2_keras: 0.3717\n",
      "Epoch 206/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 131.1141 - r2_keras: 0.3696\n",
      "Epoch 207/300\n",
      "17095/17095 [==============================] - 1s 71us/step - loss: 133.2456 - r2_keras: 0.3616\n",
      "Epoch 208/300\n",
      "17095/17095 [==============================] - 1s 68us/step - loss: 131.8998 - r2_keras: 0.3658\n",
      "Epoch 209/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 131.3195 - r2_keras: 0.3705\n",
      "Epoch 210/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 130.6754 - r2_keras: 0.3747\n",
      "Epoch 211/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 130.9296 - r2_keras: 0.3756\n",
      "Epoch 212/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 127.6023 - r2_keras: 0.3909\n",
      "Epoch 213/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 126.4703 - r2_keras: 0.3895\n",
      "Epoch 214/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 128.1488 - r2_keras: 0.3854\n",
      "Epoch 215/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 130.5354 - r2_keras: 0.3713\n",
      "Epoch 216/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 128.1913 - r2_keras: 0.3878\n",
      "Epoch 217/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 129.1415 - r2_keras: 0.3800\n",
      "Epoch 218/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 128.8895 - r2_keras: 0.3877\n",
      "Epoch 219/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 128.5440 - r2_keras: 0.3847\n",
      "Epoch 220/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 129.5688 - r2_keras: 0.3802\n",
      "Epoch 221/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 130.0261 - r2_keras: 0.3789\n",
      "Epoch 222/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 127.4133 - r2_keras: 0.3861\n",
      "Epoch 223/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17095/17095 [==============================] - 1s 58us/step - loss: 128.4326 - r2_keras: 0.3859\n",
      "Epoch 224/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 127.5043 - r2_keras: 0.3907\n",
      "Epoch 225/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 127.3147 - r2_keras: 0.3913\n",
      "Epoch 226/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 129.6533 - r2_keras: 0.3795\n",
      "Epoch 227/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 127.7343 - r2_keras: 0.3886\n",
      "Epoch 228/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 127.6262 - r2_keras: 0.3860\n",
      "Epoch 229/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 128.0864 - r2_keras: 0.3814\n",
      "Epoch 230/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 128.6092 - r2_keras: 0.3873\n",
      "Epoch 231/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 126.5063 - r2_keras: 0.3975\n",
      "Epoch 232/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 127.1696 - r2_keras: 0.3875\n",
      "Epoch 233/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 124.4791 - r2_keras: 0.4026\n",
      "Epoch 234/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 126.0804 - r2_keras: 0.4014\n",
      "Epoch 235/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 128.2587 - r2_keras: 0.3844\n",
      "Epoch 236/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 127.5541 - r2_keras: 0.3855\n",
      "Epoch 237/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 126.3156 - r2_keras: 0.3938\n",
      "Epoch 238/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 123.5361 - r2_keras: 0.4102\n",
      "Epoch 239/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 125.7552 - r2_keras: 0.3951\n",
      "Epoch 240/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 125.9938 - r2_keras: 0.3953\n",
      "Epoch 241/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 124.5798 - r2_keras: 0.3954\n",
      "Epoch 242/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 126.5755 - r2_keras: 0.3950\n",
      "Epoch 243/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 124.8598 - r2_keras: 0.4008\n",
      "Epoch 244/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 125.2484 - r2_keras: 0.3975\n",
      "Epoch 245/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 124.1027 - r2_keras: 0.4079\n",
      "Epoch 246/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 124.2950 - r2_keras: 0.4035\n",
      "Epoch 247/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 124.0174 - r2_keras: 0.4095\n",
      "Epoch 248/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 125.4302 - r2_keras: 0.3936\n",
      "Epoch 249/300\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 124.7235 - r2_keras: 0.4031\n",
      "Epoch 250/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 120.8016 - r2_keras: 0.4222\n",
      "Epoch 251/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 123.2573 - r2_keras: 0.4116\n",
      "Epoch 252/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 121.8362 - r2_keras: 0.4123\n",
      "Epoch 253/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 122.4085 - r2_keras: 0.4158\n",
      "Epoch 254/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 121.9686 - r2_keras: 0.4147\n",
      "Epoch 255/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 120.7923 - r2_keras: 0.4221\n",
      "Epoch 256/300\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 120.1463 - r2_keras: 0.4285\n",
      "Epoch 257/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 123.8821 - r2_keras: 0.4059\n",
      "Epoch 258/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 120.2183 - r2_keras: 0.4227\n",
      "Epoch 259/300\n",
      "17095/17095 [==============================] - 1s 74us/step - loss: 120.5125 - r2_keras: 0.4241\n",
      "Epoch 260/300\n",
      "17095/17095 [==============================] - 1s 68us/step - loss: 121.1586 - r2_keras: 0.4204\n",
      "Epoch 261/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 119.9503 - r2_keras: 0.4264\n",
      "Epoch 262/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 120.3904 - r2_keras: 0.4257\n",
      "Epoch 263/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 122.4982 - r2_keras: 0.4110\n",
      "Epoch 264/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 119.5400 - r2_keras: 0.4239\n",
      "Epoch 265/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 119.2273 - r2_keras: 0.4321\n",
      "Epoch 266/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 118.4178 - r2_keras: 0.4295\n",
      "Epoch 267/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 120.1604 - r2_keras: 0.4137\n",
      "Epoch 268/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 120.6497 - r2_keras: 0.4220\n",
      "Epoch 269/300\n",
      "17095/17095 [==============================] - 1s 64us/step - loss: 118.7914 - r2_keras: 0.4288\n",
      "Epoch 270/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 118.8445 - r2_keras: 0.4295\n",
      "Epoch 271/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 119.8199 - r2_keras: 0.4250\n",
      "Epoch 272/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 118.9858 - r2_keras: 0.4304\n",
      "Epoch 273/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 117.7838 - r2_keras: 0.4278\n",
      "Epoch 274/300\n",
      "17095/17095 [==============================] - 1s 66us/step - loss: 118.2672 - r2_keras: 0.4343\n",
      "Epoch 275/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 117.7103 - r2_keras: 0.4356\n",
      "Epoch 276/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 116.4068 - r2_keras: 0.4447\n",
      "Epoch 277/300\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 117.2979 - r2_keras: 0.4374\n",
      "Epoch 278/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 117.6715 - r2_keras: 0.4363\n",
      "Epoch 279/300\n",
      "17095/17095 [==============================] - 1s 68us/step - loss: 119.2220 - r2_keras: 0.4277\n",
      "Epoch 280/300\n",
      "17095/17095 [==============================] - 1s 80us/step - loss: 117.4205 - r2_keras: 0.4394\n",
      "Epoch 281/300\n",
      "17095/17095 [==============================] - 1s 79us/step - loss: 117.7175 - r2_keras: 0.4320\n",
      "Epoch 282/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 117.6651 - r2_keras: 0.4396\n",
      "Epoch 283/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 116.4685 - r2_keras: 0.4400\n",
      "Epoch 284/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 115.7157 - r2_keras: 0.4458\n",
      "Epoch 285/300\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 116.3474 - r2_keras: 0.4421\n",
      "Epoch 286/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 116.8773 - r2_keras: 0.4373\n",
      "Epoch 287/300\n",
      "17095/17095 [==============================] - 1s 69us/step - loss: 115.6180 - r2_keras: 0.4466\n",
      "Epoch 288/300\n",
      "17095/17095 [==============================] - 1s 68us/step - loss: 115.0187 - r2_keras: 0.4491\n",
      "Epoch 289/300\n",
      "17095/17095 [==============================] - 1s 73us/step - loss: 116.5218 - r2_keras: 0.4454\n",
      "Epoch 290/300\n",
      "17095/17095 [==============================] - 1s 63us/step - loss: 115.0760 - r2_keras: 0.4462\n",
      "Epoch 291/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 115.9564 - r2_keras: 0.4319\n",
      "Epoch 292/300\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 116.5923 - r2_keras: 0.4409\n",
      "Epoch 293/300\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 114.1217 - r2_keras: 0.4536\n",
      "Epoch 294/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 115.7159 - r2_keras: 0.4387\n",
      "Epoch 295/300\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 116.6809 - r2_keras: 0.4398\n",
      "Epoch 296/300\n",
      "17095/17095 [==============================] - 1s 69us/step - loss: 114.3852 - r2_keras: 0.4538\n",
      "Epoch 297/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17095/17095 [==============================] - 1s 74us/step - loss: 114.2607 - r2_keras: 0.4551\n",
      "Epoch 298/300\n",
      "17095/17095 [==============================] - 1s 66us/step - loss: 114.0986 - r2_keras: 0.4506\n",
      "Epoch 299/300\n",
      "17095/17095 [==============================] - 1s 63us/step - loss: 114.5329 - r2_keras: 0.4532\n",
      "Epoch 300/300\n",
      "17095/17095 [==============================] - 1s 64us/step - loss: 115.1898 - r2_keras: 0.4502\n",
      "final r2 :  0.5886519395272307\n",
      "Appended model with r2 score  0.5886519395272307\n"
     ]
    }
   ],
   "source": [
    "model_Heartrate = get_keras_ANN_vital_signs(3, X_train_scaled, y_train_vital_signs, 3, 125, [300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_vital_signs = [model_RRate, model_ABPm, model_SpO2, model_Heartrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for vital signs\n",
    "df_pred_vital_signs = pd.DataFrame(index=val_pids, columns=VITAL_SIGNS)\n",
    "for i, test in enumerate(VITAL_SIGNS):\n",
    "    feature_selector = feature_selectors_vital_signs[i]\n",
    "    X_val_vital_sign = feature_selector.transform(X_val_scaled)\n",
    "    model_for_test = models[i]\n",
    "    y_pred = model_for_test.predict(X_val_vital_sign)\n",
    "    df_pred_vital_signs[test] = y_pred\n",
    "\n",
    "df_pred_vital_signs = df_pred_vital_signs.reset_index().rename(columns={\"index\": \"pid\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_432_input to have shape (95,) but got array with shape (35,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-228-4b3bb449b657>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf_pred_vital_signs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_pids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mVITAL_SIGNS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels_vital_signs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_vital_signs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mdf_pred_vital_signs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m         \u001b[1;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1441\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1442\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    143\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_432_input to have shape (95,) but got array with shape (35,)"
     ]
    }
   ],
   "source": [
    "# Get predictions for vital signs using ANN\n",
    "df_pred_vital_signs = pd.DataFrame(index=val_pids, columns=VITAL_SIGNS)\n",
    "for model in models_vital_signs:\n",
    "    y_pred = model.predict(X_val_vital_signs)\n",
    "    df_pred_vital_signs[test] = y_pred\n",
    "\n",
    "df_pred_vital_signs = df_pred_vital_signs.reset_index().rename(columns={\"index\": \"pid\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to ZIP file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export predictions DataFrame to a zip file\n",
      "           pid  LABEL_BaseExcess  LABEL_Fibrinogen  LABEL_AST  \\\n",
      "0          0.0          0.832393          0.429798   0.896758   \n",
      "1          3.0          0.131074          0.304217   0.463766   \n",
      "2          5.0          0.417147          0.304217   0.443913   \n",
      "3          7.0          0.959387          0.911051   0.944069   \n",
      "4          9.0          0.384726          0.298570   0.566566   \n",
      "...        ...               ...               ...        ...   \n",
      "12659  31647.0          0.338738          0.304217   0.406793   \n",
      "12660  31649.0          0.175901          0.608352   0.760341   \n",
      "12661  31651.0          0.929130          0.325646   0.474042   \n",
      "12662  31652.0          0.082532          0.317979   0.525401   \n",
      "12663  31655.0          0.117321          0.323369   0.573651   \n",
      "\n",
      "       LABEL_Alkalinephos  LABEL_Bilirubin_total  LABEL_Lactate  \\\n",
      "0                0.961712               0.913878       0.849364   \n",
      "1                0.480553               0.439553       0.238957   \n",
      "2                0.433482               0.425904       0.277688   \n",
      "3                0.961004               0.867512       0.672507   \n",
      "4                0.562898               0.542512       0.396905   \n",
      "...                   ...                    ...            ...   \n",
      "12659            0.373973               0.411888       0.300359   \n",
      "12660            0.811736               0.611820       0.340112   \n",
      "12661            0.461187               0.443320       0.697765   \n",
      "12662            0.593110               0.526376       0.207084   \n",
      "12663            0.594063               0.588049       0.225370   \n",
      "\n",
      "       LABEL_TroponinI  LABEL_SaO2  LABEL_Bilirubin_direct  LABEL_EtCO2  \\\n",
      "0             0.150986    0.603449                0.647068     0.434431   \n",
      "1             0.698675    0.274292                0.446310     0.232469   \n",
      "2             0.473744    0.404472                0.446310     0.218275   \n",
      "3             0.352497    0.647528                0.911838     0.269971   \n",
      "4             0.221129    0.206536                0.570115     0.324562   \n",
      "...                ...         ...                     ...          ...   \n",
      "12659         0.094801    0.290623                0.446310     0.232469   \n",
      "12660         0.677199    0.271969                0.739103     0.281824   \n",
      "12661         0.273953    0.724276                0.364428     0.437264   \n",
      "12662         0.635182    0.299705                0.243108     0.341546   \n",
      "12663         0.345621    0.274292                0.446310     0.413635   \n",
      "\n",
      "       LABEL_Sepsis  LABEL_RRate  LABEL_ABPm  LABEL_SpO2  LABEL_Heartrate  \n",
      "0          0.752551    16.098423   85.067841   97.806915        84.746559  \n",
      "1          0.332614    17.577841   84.925156   96.260529        89.948601  \n",
      "2          0.296633    18.384174   72.514221   95.604843        67.635735  \n",
      "3          0.764247    16.582848   88.701813   97.345406        94.783012  \n",
      "4          0.645422    20.150539   88.608429   95.481430        91.113228  \n",
      "...             ...          ...         ...         ...              ...  \n",
      "12659      0.292379    16.750763   69.930702   96.260529        73.083839  \n",
      "12660      0.419551    15.429890   81.475014   96.065491        91.500526  \n",
      "12661      0.522059    17.051477   77.414116   97.674973        84.375969  \n",
      "12662      0.382621    18.422857   96.369110   96.906197       108.435699  \n",
      "12663      0.343168    17.119041   83.600609   97.900909       105.709663  \n",
      "\n",
      "[12664 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "df_predictions = pd.merge(df_pred_medical_test, df_pred_sepsis, on=\"pid\")\n",
    "df_predictions = pd.merge(df_predictions, df_pred_vital_signs, on=\"pid\")\n",
    "print(\"Export predictions DataFrame to a zip file\")\n",
    "print(df_predictions)\n",
    "df_predictions.to_csv(\n",
    "    \"predictions.csv\",\n",
    "    index=None,\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    encoding=\"utf-8-sig\",\n",
    "    float_format=\"%.2f\",\n",
    ")\n",
    "\n",
    "with zipfile.ZipFile(\"predictions.zip\", \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    zf.write(\"predictions.csv\")\n",
    "os.remove(\"predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
