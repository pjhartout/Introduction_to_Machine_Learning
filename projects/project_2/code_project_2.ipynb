{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the solution to project 2 from scratch\n",
    "To note:\n",
    "- For clarity the df_test was renamed to df_val as the test word was used when splitting the labeled data into train and test. \n",
    "    - Val stands for validation\n",
    "\n",
    "To try out:\n",
    "- Preprocessing\n",
    "    - Tweak data imputer\n",
    "    - Tweak scaler (Robust scaler, minmax, etc..)\n",
    "    - Tweak feature selection parameter\n",
    "    - Tweak order of operations above to see the effect\n",
    "- Modelling\n",
    "    - XGBoost\n",
    "    - SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import zipfile\n",
    "import time\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import ADASYN, SMOTE\n",
    "from imblearn.under_sampling import ClusterCentroids, RandomUnderSampler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, chi2, f_classif\n",
    "from sklearn.metrics import f1_score, mean_squared_error, accuracy_score, r2_score, roc_auc_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import label_ranking_average_precision_score as LRAPS\n",
    "from sklearn.metrics import label_ranking_loss as LRL\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Global variables\n",
    "IDENTIFIERS = [\"pid\", \"Time\"]\n",
    "MEDICAL_TESTS = [\n",
    "    \"LABEL_BaseExcess\",\n",
    "    \"LABEL_Fibrinogen\",\n",
    "    \"LABEL_AST\",\n",
    "    \"LABEL_Alkalinephos\",\n",
    "    \"LABEL_Bilirubin_total\",\n",
    "    \"LABEL_Lactate\",\n",
    "    \"LABEL_TroponinI\",\n",
    "    \"LABEL_SaO2\",\n",
    "    \"LABEL_Bilirubin_direct\",\n",
    "    \"LABEL_EtCO2\",\n",
    "]\n",
    "VITAL_SIGNS = [\"LABEL_RRate\", \"LABEL_ABPm\", \"LABEL_SpO2\", \"LABEL_Heartrate\"]\n",
    "SEPSIS = [\"LABEL_Sepsis\"]\n",
    "ESTIMATOR = {\"bayesian\": BayesianRidge(), \"decisiontree\": DecisionTreeRegressor(max_features=\"sqrt\", random_state=0), \n",
    "                \"extratree\": ExtraTreesRegressor(n_estimators=10, random_state=0), \n",
    "                \"knn\": KNeighborsRegressor(n_neighbors=10, weights=\"distance\")}\n",
    "\n",
    "FEATURES_MNAR = [\"EtCO2\", \"PTT\", \"BUN\", \"Lactate\", \"Hgb\", \"HCO3\", \"BaseExcess\",\n",
    "                          \"Fibrinogen\", \"Phosphate\", \"WBC\", \"Creatinine\", \"PaCO2\", \"AST\",\n",
    "                          \"FiO2\", \"Platelets\", \"SaO2\", \"Glucose\", \"Magnesium\", \"Potassium\",\n",
    "                          \"Calcium\", \"Alkalinephos\", \"Bilirubin_direct\", \"Chloride\", \"Hct\",\n",
    "                          \"Bilirubin_total\", \"TroponinI\", \"pH\"]\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_f(x):\n",
    "    \"\"\"To get predictions as confidence level, the model predicts for all 12 sets of measures for\n",
    "    each patient a distance to the hyperplane ; it is then transformed into a confidence level using\n",
    "    the sigmoid function ; the confidence level reported is the mean of all confidence levels for a\n",
    "    single patient\n",
    "\n",
    "    Args:\n",
    "        x (float): input of the sigmoid function\n",
    "\n",
    "    Returns:\n",
    "       float: result of the sigmoid computation.\n",
    "\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r\"data/train_features.csv\")\n",
    "df_train_label = pd.read_csv(r\"data/train_labels.csv\")\n",
    "df_val = pd.read_csv(r\"data/test_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data imputation methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done adding features about MNAR features\n"
     ]
    }
   ],
   "source": [
    "# Adding engineered features\n",
    "mnar_columns = [\n",
    "        sub + \"_presence\" for sub in FEATURES_MNAR\n",
    "    ]\n",
    "pid = df_train[\"pid\"].unique()\n",
    "patient_count = -1\n",
    "for patient in pid:\n",
    "    patient_count += 1\n",
    "    print(patient_count/len(pid)*100, end=\"\\r\")\n",
    "    for column in FEATURES_MNAR:\n",
    "        presence = int(df_train.loc[\n",
    "            df_train[\"pid\"] == patient\n",
    "            ][column].any())\n",
    "        df_train.at[patient, column] = presence\n",
    "print(\"Done adding features about MNAR features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done adding features about MNAR features\n"
     ]
    }
   ],
   "source": [
    "# Adding engineered features\n",
    "pid = df_val[\"pid\"].unique()\n",
    "patient_count = -1\n",
    "for patient in pid:\n",
    "    patient_count += 1\n",
    "    print(patient_count/len(pid)*100, end=\"\\r\")\n",
    "    for column in FEATURES_MNAR:\n",
    "        presence = int(df_val.loc[\n",
    "            df_val[\"pid\"] == patient\n",
    "            ][column].any())\n",
    "        df_val.at[patient, column] = presence\n",
    "print(\"Done adding features about MNAR features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.04735456699135544\r"
     ]
    }
   ],
   "source": [
    "columns_for_regression = [\"Temp\", \"Hgb\", \"RRate\", \"BaseExcess\", \"WBC\", \"PaCO2\", \"FiO2\", \"Glucose\", \"ABPm\", \"ABPd\", \"SpO2\", \"Hct\", \"Heartrate\", \"ABPs\", \"pH\"]\n",
    "columns_for_regression_trend = [\n",
    "        sub + \"_trend\" for sub in columns_for_regression\n",
    "    ]\n",
    "columns_for_regression_std = [\n",
    "        sub + \"_std\" for sub in columns_for_regression\n",
    "    ]\n",
    "columns_for_regression_min = [\n",
    "        sub + \"_min\" for sub in columns_for_regression\n",
    "    ]\n",
    "columns_for_regression_max = [\n",
    "        sub + \"_max\" for sub in columns_for_regression\n",
    "    ]\n",
    "cols_to_add = columns_for_regression_trend + columns_for_regression_std + columns_for_regression_min + columns_for_regression_max\n",
    "patient_count = 0\n",
    "\n",
    "df_train = df_train.reindex(\n",
    "        df_train.columns.tolist() + cols_to_add,\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "pid = df_train[\"pid\"].unique()\n",
    "\n",
    "for patient in pid:\n",
    "    patient_count += 1\n",
    "    print(patient_count/len(pid)*100, end=\"\\r\")\n",
    "    for column in columns_for_regression:\n",
    "        if df_train.loc[df_train[\"pid\"] == patient][column].isna().sum() <= 8:\n",
    "            series = df_train.loc[df_train[\"pid\"] == patient][column]\n",
    "            # Fill missing values between two non nans with their average\n",
    "            series = (series.ffill() + series.bfill()) / 2\n",
    "            # Drop the rest of the value\n",
    "            series = series.dropna()\n",
    "            standard_deviation = series.std()\n",
    "            minimum = series.min()\n",
    "            maximum = series.max()\n",
    "            X = [i for i in range(0, len(series))]\n",
    "            X = np.reshape(X, (len(X), 1))\n",
    "            y = series\n",
    "            model = LinearRegression()\n",
    "            try:\n",
    "                model.fit(X, y)\n",
    "                df_train.at[patient, column + \"_trend\"] = model.coef_\n",
    "            except ValueError:\n",
    "                df_train.at[patient, column + \"_trend\"] = 0\n",
    "            df_train.at[patient, column + \"_std\"] = standard_deviation\n",
    "            df_train.at[patient, column + \"_min\"] = minimum\n",
    "            df_train.at[patient, column + \"_max\"] = maximum\n",
    "\n",
    "    # fill rest of values with 0 for trends col umns\n",
    "    df_train[columns_for_regression_trend] = df_train[\n",
    "        columns_for_regression_trend\n",
    "    ].fillna(value=0)\n",
    "    df_train[columns_for_regression_std] = df_train[\n",
    "        columns_for_regression_std\n",
    "    ].fillna(value=0)\n",
    "    df_train[columns_for_regression_min] = df_train[\n",
    "        columns_for_regression_min\n",
    "    ].fillna(value=0)\n",
    "    df_train[columns_for_regression_max] = df_train[\n",
    "        columns_for_regression_max\n",
    "    ].fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.04735456699135544\r"
     ]
    }
   ],
   "source": [
    "patient_count = 0\n",
    "\n",
    "for patient in pid:\n",
    "    patient_count += 1\n",
    "    print(patient_count/len(pid)*100, end=\"\\r\")\n",
    "    for column in columns_for_regression:\n",
    "        if df_val.loc[df_val[\"pid\"] == patient][column].isna().sum() <= 8:\n",
    "            series = df_val.loc[df_train[\"pid\"] == patient][column]\n",
    "            # Fill missing values between two non nans with their average\n",
    "            series = (series.ffill() + series.bfill()) / 2\n",
    "            # Drop the rest of the value\n",
    "            series = series.dropna()\n",
    "            standard_deviation = series.std()\n",
    "            minimum = series.min()\n",
    "            maximum = series.max()\n",
    "            X = [i for i in range(0, len(series))]\n",
    "            X = np.reshape(X, (len(X), 1))\n",
    "            y = series\n",
    "            model = LinearRegression()\n",
    "            try:\n",
    "                model.fit(X, y)\n",
    "                df_train.at[patient, column + \"_trend\"] = model.coef_\n",
    "            except ValueError:\n",
    "                df_train.at[patient, column + \"_trend\"] = 0\n",
    "            df_val.at[patient, column + \"_std\"] = standard_deviation\n",
    "            df_val.at[patient, column + \"_min\"] = minimum\n",
    "            df_val.at[patient, column + \"_max\"] = maximum\n",
    "\n",
    "    # fill rest of values with 0 for trends col umns\n",
    "    df_val[columns_for_regression_trend] = df_val[\n",
    "        columns_for_regression_trend\n",
    "    ].fillna(value=0)\n",
    "    df_val[columns_for_regression_std] = df_val[\n",
    "        columns_for_regression_std\n",
    "    ].fillna(value=0)\n",
    "    df_val[columns_for_regression_min] = df_val[\n",
    "        columns_for_regression_min\n",
    "    ].fillna(value=0)\n",
    "    df_val[columns_for_regression_max] = df_val[\n",
    "        columns_for_regression_max\n",
    "    ].fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philiphartout/anaconda3/envs/iml/lib/python3.7/site-packages/sklearn/impute/_iterative.py:638: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "imputer = IterativeImputer()\n",
    "columns = df_train.columns\n",
    "df_train = imputer.fit_transform(df_train.values)\n",
    "df_train = pd.DataFrame(df_train, columns=columns)\n",
    "df_train = df_train.groupby([\"pid\"], as_index=False).mean()\n",
    "\n",
    "# Tranform test data according to same imputer\n",
    "pid_val = df_val[\"pid\"].unique()\n",
    "columns = df_val.columns\n",
    "df_val = pd.DataFrame(columns=columns, index=pid_val)\n",
    "df_val = imputer.transform(df_val.values)\n",
    "df_val = pd.DataFrame(df_val, columns=columns)\n",
    "df_val = df_val.groupby([\"pid\"], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"df_train_philip.csv\")\n",
    "df_val.to_csv(\"df_val_philip.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_preprocessed = pd.read_csv(\"df_train_philip.csv\")\n",
    "df_val_preprocessed = pd.read_csv(\"df_val_philip.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.sort_values(by=[\"pid\"])\n",
    "df_train = df_train.drop(columns=IDENTIFIERS)\n",
    "df_val = df_val.sort_values(by=[\"pid\"])\n",
    "df_val = df_val.drop(columns=IDENTIFIERS)\n",
    "df_train_label = df_train_label.sort_values(by=[\"pid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a list of labels for each medical test\n",
      "Creating a list of labels for sepsis\n",
      "Creating a list of labels for each vital sign\n"
     ]
    }
   ],
   "source": [
    "# Data formatting\n",
    "X_train = df_train_preprocessed.drop(columns=['Unnamed: 0']).values\n",
    "X_val = df_val_preprocessed.drop(columns=['Unnamed: 0']).values\n",
    "# Create list with different label for each medical test\n",
    "print(\"Creating a list of labels for each medical test\")\n",
    "y_train_medical_tests = []\n",
    "for test in MEDICAL_TESTS:\n",
    "    y_train_medical_tests.append(df_train_label[test].astype(int).values)\n",
    "\n",
    "# Create list with different label for sepsis\n",
    "print(\"Creating a list of labels for sepsis\")\n",
    "y_train_sepsis = []\n",
    "for sepsis in SEPSIS:\n",
    "    y_train_sepsis.append(df_train_label[sepsis].astype(int).values)\n",
    "\n",
    "# Create list with different label for each vital sign\n",
    "print(\"Creating a list of labels for each vital sign\")\n",
    "y_train_vital_signs = []\n",
    "for sign in VITAL_SIGNS:\n",
    "    y_train_vital_signs.append(df_train_label[sign].astype(int).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data \n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling medical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Modelling of medical tests using logistic regression with cross validation\n",
    "# models = []\n",
    "# losses = []\n",
    "# columns_medical_tests = []\n",
    "# for i, test in enumerate(MEDICAL_TESTS):\n",
    "#     print(f\"Fitting model for {test}.\")\n",
    "\n",
    "#     print(\"Applying feature selection\")\n",
    "#     feature_selector = SelectKBest(score_func=f_classif, k=3)\n",
    "#     X_train = feature_selector.fit_transform(X_train, y_train_medical_tests[i])\n",
    "#     X_test = feature_selector.transform(X_test)\n",
    "#     columns = feature_selector.get_support(indices=True)\n",
    "#     columns_medical_tests.append(columns)\n",
    "\n",
    "#     print(\"Fitting model\")\n",
    "#     clf = LogisticRegressionCV(cv=5, random_state=42).fit(X_train, y_train_medical_tests[i])\n",
    "#     models.append(clf)\n",
    "#     print(roc_auc_score(y_test, clf.predict_proba(X_test)[:,1]))\n",
    "#     print(f\"Finished test for medical tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READY TO ROLL\n"
     ]
    }
   ],
   "source": [
    "print(\"READY TO ROLL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_cuda_tensor(X_train, X_test, y_train, y_test, device):\n",
    "    \"\"\"Converts a number of np.ndarrays to tensors placed on the device specified.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): (n_samples, n_features) array containing training features\n",
    "        X_test (np.ndarray): (n_samples, n_features) array containing testing features\n",
    "        y_train (np.ndarray): (n_samples,) array containing training labels\n",
    "        y_test (np.ndarray): (n_samples,) array containing testing labels\n",
    "        device (torch.device): device on which the tensors should be placed (CPU/CUDA GPU)\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    return (\n",
    "        torch.from_numpy(X_train).to(device).float(),\n",
    "        torch.from_numpy(X_test).to(device).float(),\n",
    "        torch.from_numpy(y_train).to(device).float(),\n",
    "        torch.from_numpy(y_test).to(device).float(),\n",
    "    )\n",
    "\n",
    "\n",
    "class Feedforward(torch.nn.Module):\n",
    "    \"\"\" Definition of the feedfoward neural network. It currently has three layers which can be\n",
    "    modified in the function where the network is trained.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, subtask, p=0.2):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.subtask = subtask\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout = torch.nn.Dropout(p=p)\n",
    "        self.bn = torch.nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        torch.nn.init.xavier_normal_(self.fc1.weight)\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        torch.nn.init.xavier_normal_(self.fc2.weight)\n",
    "        self.fc3 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        torch.nn.init.xavier_normal_(self.fc3.weight)\n",
    "        self.fcout = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "        torch.nn.init.xavier_normal_(self.fcout.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Function where the forward pass is defined. The backward pass is deternmined by the\n",
    "            autograd function built into PyTorch.\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor (n_samples,n_features) tensor containing training input\n",
    "                features\n",
    "            subtask (int): subtask performed (choice: 1,2,3)\n",
    "        Returns:\n",
    "            output (torch.Tensor): (n_samples,n_features) tensor containing\n",
    "                the predicted output for each sample.\n",
    "        \"\"\"\n",
    "        assert self.subtask in [1, 2, 3]\n",
    "        hidden = self.fc1(x)\n",
    "        hidden_bn = self.bn(hidden)\n",
    "        relu = self.sigmoid(hidden_bn)\n",
    "        hidden = self.dropout(self.fc2(relu))\n",
    "        hidden_bn = self.bn(hidden)\n",
    "        relu = self.sigmoid(hidden_bn)\n",
    "        hidden = self.dropout(self.fc3(relu))\n",
    "        hidden_bn = self.bn(hidden)\n",
    "        relu = self.sigmoid(hidden_bn)\n",
    "        output = self.fcout(relu)\n",
    "        if self.subtask == 1 or self.subtask == 2:\n",
    "            output = self.sigmoid(output)\n",
    "        else:\n",
    "            output = self.relu(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Data(Dataset):\n",
    "    \"\"\" Class used to load the data in minibatches to control the neural network stability during\n",
    "        training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.len = self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Modelling using extreme gradient boosting\n",
    "# models = []\n",
    "# losses = []\n",
    "# feature_selector_medical_tests = []\n",
    "# for i, test in enumerate(MEDICAL_TESTS):\n",
    "#     print(f\"Fitting model for {test}.\")\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X_train_scaled, y_train_medical_tests[i], test_size=0.10, random_state=42, shuffle=True\n",
    "#     )\n",
    "#     # Coarse parameter grid not optimized at all yet\n",
    "    \n",
    "#     print(\"Resampling\")\n",
    "#     sampler = RandomUnderSampler(random_state=42)\n",
    "#     X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "#     print(\"Applying feature selection\")\n",
    "#     feature_selector = SelectKBest(score_func=f_classif, k=10)\n",
    "#     X_train = feature_selector.fit_transform(X_train_res, y_train_res)\n",
    "#     X_test = feature_selector.transform(X_test)\n",
    "#     feature_selector_medical_tests.append(feature_selector)\n",
    "    \n",
    "    \n",
    "#     X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = convert_to_cuda_tensor(\n",
    "#             X_train, X_test, y_train_res, y_test, DEVICE\n",
    "#     )\n",
    "    \n",
    "#     model = Feedforward(\n",
    "#             input_size=X_train_tensor.shape[1],\n",
    "#             hidden_size=100,\n",
    "#             output_size=1,\n",
    "#             subtask=1,\n",
    "#             p=0.1,\n",
    "#     )\n",
    "    \n",
    "#     criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# #     if optim == \"SGD\":\n",
    "# #         optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "# #     elif optim == \"Adam\":\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "#     dataset = Data(X_train_tensor, y_train_tensor)\n",
    "#     batch_size = 1024  # Ideally we want powers of 2\n",
    "#     trainloader = DataLoader(dataset=dataset, batch_size=batch_size)\n",
    "\n",
    "#     if torch.cuda.is_available():\n",
    "#         model.cuda()\n",
    "#     model.float()\n",
    "    \n",
    "    \n",
    "#     dirpath = os.path.join(os.getcwd(), \"runs\")\n",
    "#     fileList = os.listdir(dirpath)\n",
    "#     for fileName in fileList:\n",
    "#         shutil.rmtree(dirpath + \"/\" + fileName)\n",
    "    \n",
    "#     now = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "#     writer = SummaryWriter(\n",
    "#         log_dir=f\"runs/ann_network_runs_{200}_epochs_{now}_{test}\"\n",
    "#     )\n",
    "#     for epoch in list(range(500)):\n",
    "#         LOSS = []\n",
    "#         for x, y in trainloader:\n",
    "#             yhat = model(x)\n",
    "#             loss = criterion(yhat.float(), y.reshape((y.shape[0], 1)))\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             LOSS.append(loss)\n",
    "#         X_test_tensor = X_test_tensor.to(DEVICE)\n",
    "#         y_test_pred = model(X_test_tensor).cpu().detach().numpy()\n",
    "#         y_train_pred = model(X_train_tensor).cpu().detach().numpy()\n",
    "#         loss_average = sum(LOSS) / len(LOSS)\n",
    "#         writer.add_scalar(\"Training_loss\", loss_average, epoch)\n",
    "#         ROC_train = roc_auc_score(y_train_res, y_train_pred)\n",
    "#         ROC_test = roc_auc_score(y_test, y_test_pred)\n",
    "# #         writer.add_scalar(\"ROC train\", ROC_train, epoch)\n",
    "# #         writer.add_scalar(\"ROC test\", ROC_test, epoch)\n",
    "#         print(f\"Epoch: {epoch} - ROC train: {ROC_train} - ROC test: {ROC_test}\", end=\"\\r\")\n",
    "#     print(\"\")\n",
    "        \n",
    "# #     models.append(model)\n",
    "# #     model.eval()\n",
    "# #     print(f\"ROC score on test set {roc_auc_score(y_test, model(X_test_tensor))}\")\n",
    "# #     print(f\"CV score {coarse_search.best_score_}\")\n",
    "# print(f\"Finished test for medical tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model for LABEL_BaseExcess.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   30.7s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed: 11.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed: 17.1min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed: 20.1min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed: 23.9min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed: 28.2min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed: 28.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92124844 0.11344181 0.29810002 ... 0.91302997 0.27503392 0.8153258 ]\n",
      "ROC score on test set 0.879664081103796\n",
      "CV score 0.8627901854812372\n",
      "ROC score on test set 0.879664081103796\n",
      "CV score 0.8627901854812372\n",
      "Fitting model for LABEL_Fibrinogen.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:   30.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:   47.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:  6.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.718125   0.5770305  0.52418727 ... 0.4888774  0.32086784 0.36808202]\n",
      "ROC score on test set 0.7243470422535212\n",
      "CV score 0.7329693651574803\n",
      "ROC score on test set 0.7243470422535212\n",
      "CV score 0.7329693651574803\n",
      "Fitting model for LABEL_AST.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   25.7s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed: 15.7min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed: 18.6min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed: 21.7min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed: 22.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8374758  0.6264091  0.33403337 ... 0.58397895 0.38864544 0.4827148 ]\n",
      "ROC score on test set 0.7368214609952413\n",
      "CV score 0.7073899464604403\n",
      "ROC score on test set 0.7368214609952413\n",
      "CV score 0.7073899464604403\n",
      "Fitting model for LABEL_Alkalinephos.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed: 15.5min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed: 18.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed: 22.1min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed: 22.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8032056  0.64130074 0.33422276 ... 0.59836924 0.4610252  0.46021613]\n",
      "ROC score on test set 0.7403043216253443\n",
      "CV score 0.7150990828758821\n",
      "ROC score on test set 0.7403043216253443\n",
      "CV score 0.7150990828758821\n",
      "Fitting model for LABEL_Bilirubin_total.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   21.6s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   51.4s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed: 14.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed: 17.0min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed: 19.7min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed: 22.6min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed: 23.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87397355 0.6922115  0.32102206 ... 0.5634515  0.4186712  0.5080465 ]\n",
      "ROC score on test set 0.7409208937198067\n",
      "CV score 0.7041963403010875\n",
      "ROC score on test set 0.7409208937198067\n",
      "CV score 0.7041963403010875\n",
      "Fitting model for LABEL_Lactate.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   19.3s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   55.3s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed: 17.1min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed: 20.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8033025  0.20566778 0.28735828 ... 0.62399405 0.2143497  0.6532622 ]\n",
      "ROC score on test set 0.7627646524669035\n",
      "CV score 0.750527370586176\n",
      "ROC score on test set 0.7627646524669035\n",
      "CV score 0.750527370586176\n",
      "Fitting model for LABEL_TroponinI.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   28.5s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:   48.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:  9.7min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12231846 0.75927013 0.74577814 ... 0.25730664 0.6032266  0.17911664]\n",
      "ROC score on test set 0.7227693581780539\n",
      "CV score 0.742208626284478\n",
      "ROC score on test set 0.7227693581780539\n",
      "CV score 0.742208626284478\n",
      "Fitting model for LABEL_SaO2.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   52.0s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed: 17.2min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed: 20.0min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed: 23.0min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed: 23.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74861014 0.28393912 0.42730027 ... 0.5937572  0.26378438 0.4900812 ]\n",
      "ROC score on test set 0.7633056578207291\n",
      "CV score 0.7569968114539521\n",
      "ROC score on test set 0.7633056578207291\n",
      "CV score 0.7569968114539521\n",
      "Fitting model for LABEL_Bilirubin_direct.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1528 tasks      | elapsed:   22.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2428 tasks      | elapsed:   38.0s\n",
      "[Parallel(n_jobs=-1)]: Done 3072 tasks      | elapsed:   48.3s\n",
      "[Parallel(n_jobs=-1)]: Done 4132 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5632 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 7332 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 9232 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:  2.7min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8458845  0.62316173 0.28457513 ... 0.4416283  0.3965993  0.6129491 ]\n",
      "ROC score on test set 0.7264207650273226\n",
      "CV score 0.7335745216034641\n",
      "ROC score on test set 0.7264207650273226\n",
      "CV score 0.7335745216034641\n",
      "Fitting model for LABEL_EtCO2.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 248 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=-1)]: Done 498 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=-1)]: Done 848 tasks      | elapsed:   32.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1298 tasks      | elapsed:   48.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1848 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2498 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3248 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4098 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 5048 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6098 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 7248 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 8498 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 9848 tasks      | elapsed:  5.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62657565 0.3886174  0.32917774 ... 0.61635196 0.25950286 0.5672045 ]\n",
      "ROC score on test set 0.8425446428571428\n",
      "CV score 0.801199280844551\n",
      "ROC score on test set 0.8425446428571428\n",
      "CV score 0.801199280844551\n",
      "Finished test for medical tests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:  5.8min finished\n"
     ]
    }
   ],
   "source": [
    "# Modelling using extreme gradient boosting\n",
    "clf = xgb.XGBClassifier(objective=\"binary:logistic\", n_thread=-1)\n",
    "models = []\n",
    "losses = []\n",
    "feature_selector_medical_tests = []\n",
    "for i, test in enumerate(MEDICAL_TESTS):\n",
    "    print(f\"Fitting model for {test}.\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train_scaled, y_train_medical_tests[i], test_size=0.10, random_state=42, shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(\"Resampling\")\n",
    "    sampler = RandomUnderSampler(random_state=42)\n",
    "    X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(\"Applying feature selection\")\n",
    "    feature_selector = SelectKBest(score_func=f_classif, k=5)\n",
    "    X_train_selected = feature_selector.fit_transform(X_train_res, y_train_res)\n",
    "    X_test = feature_selector.transform(X_test)\n",
    "    feature_selector_medical_tests.append(feature_selector)\n",
    "    \n",
    "    print(\"Fitting coarse model\")\n",
    "    # Coarse parameter grid not optimized at all yet\n",
    "    coarse_param_grid = {\n",
    "        \"booster\": [\"dart\"],\n",
    "        \"eta\": np.arange(0,1,0.1),\n",
    "        \"min_child_weight\": range(1, 10, 1),\n",
    "        \"max_depth\": range(4, 10, 1),\n",
    "        \"gamma\": range(0, 100, 1),\n",
    "        \"max_delta_step\": range(1, 10, 1),\n",
    "        \"subsample\": np.arange(0.1, 1, 0.05),\n",
    "        \"colsample_bytree\": np.arange(0.3, 1, 0.05),\n",
    "        \"n_estimators\": range(50, 150, 1),\n",
    "        \"scale_pos_weight\": [1],\n",
    "        \"reg_lambda\": [0, 1], # Ridge regularization\n",
    "        \"reg_alpha\": [0, 1], # Lasso regularization\n",
    "        \"eval_metric\": [\"error\"],\n",
    "        \"verbosity\": [1]\n",
    "    }\n",
    "    coarse_search = RandomizedSearchCV(estimator=clf,\n",
    "            param_distributions=coarse_param_grid, scoring=\"roc_auc\",\n",
    "            n_jobs=-1, cv=10, n_iter=1000, verbose=1)\n",
    "    coarse_search.fit(X_train_selected, y_train_res)\n",
    "    print(coarse_search.best_estimator_.predict_proba(X_test)[:,1])\n",
    "    print(f\"ROC score on test set {roc_auc_score(y_test, coarse_search.best_estimator_.predict_proba(X_test)[:,1])}\")\n",
    "    print(f\"CV score {coarse_search.best_score_}\")\n",
    "    best_params = coarse_search.best_params_\n",
    "#     print(best_params)\n",
    "#     print(\"Fitting fine model\")\n",
    "    # Fine parameter grid not optimized at all yet\n",
    "#     fine_param_grid = {\n",
    "#         \"booster\": best_params[\"booster\"],\n",
    "#         \"eta\": np.arange(best_params[\"eta\"]-0.05, best_params[\"eta\"]+0.05, 0.003),\n",
    "#         \"min_child_weight\": [best_params[\"min_child_weight\"]],\n",
    "#         \"max_depth\": [best_params[\"max_depth\"]],\n",
    "#         \"gamma\": np.arange(best_params[\"gamma\"]-3, best_params[\"gamma\"]+3, 0.05),\n",
    "#         \"max_delta_step\": [best_params[\"max_delta_step\"]],\n",
    "#         \"subsample\": np.arange(best_params[\"subsample\"]-0.05, best_params[\"subsample\"]+0.05, 0.002),\n",
    "#         \"colsample_bytree\": [best_params[\"colsample_bytree\"]],\n",
    "#         \"n_estimators\": range(best_params[\"n_estimators\"]-10, best_params[\"n_estimators\"]+10, 1),\n",
    "#         \"scale_pos_weight\": [1],\n",
    "#         \"reg_lambda\": [best_params[\"reg_lambda\"]], # Ridge regularization\n",
    "#         \"reg_alpha\": [best_params[\"reg_alpha\"]], # Lasso regularization\n",
    "#         \"eval_metric\": [\"error\"],\n",
    "#         \"verbosity\": [2]\n",
    "#     }\n",
    "#     fine_search = RandomizedSearchCV(estimator=clf,\n",
    "#             param_distributions=fine_param_grid, scoring=\"roc_auc\",\n",
    "#             n_jobs=-1, cv=10, n_iter=10, verbose=1)\n",
    "#     fine_search.fit(X_train_selected, y_train_res)\n",
    "#     print(coarse_search.best_estimator_.predict_proba(X_test)[:,1])\n",
    "    print(f\"ROC score on test set {roc_auc_score(y_test, coarse_search.best_estimator_.predict_proba(X_test)[:,1])}\")\n",
    "    print(f\"CV score {coarse_search.best_score_}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    models.append(coarse_search.best_estimator_)\n",
    "    \n",
    "print(f\"Finished test for medical tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "for i, model in enumerate(models):\n",
    "    joblib.dump(models[i], f\"xgboost_fine_{MEDICAL_TESTS[i]}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "val_pids = np.unique(df_val[\"pid\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Get predictions for medical tests\n",
    "df_pred_medical_test = pd.DataFrame(index=val_pids, columns=MEDICAL_TESTS)\n",
    "for i, test in enumerate(MEDICAL_TESTS):\n",
    "    feature_selector = feature_selector_medical_tests[i]\n",
    "    X_val_vital_sign = feature_selector.transform(X_val_scaled)\n",
    "    model_for_test = models[i]\n",
    "#     print(model_for_test.predict_proba(X_val_vital_sign))\n",
    "    y_pred = model_for_test.predict_proba(X_val_vital_sign)[:, 1]\n",
    "    df_pred_medical_test[test] = y_pred\n",
    "\n",
    "df_pred_medical_test = df_pred_medical_test.reset_index().rename(columns={\"index\": \"pid\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>LABEL_BaseExcess</th>\n",
       "      <th>LABEL_Fibrinogen</th>\n",
       "      <th>LABEL_AST</th>\n",
       "      <th>LABEL_Alkalinephos</th>\n",
       "      <th>LABEL_Bilirubin_total</th>\n",
       "      <th>LABEL_Lactate</th>\n",
       "      <th>LABEL_TroponinI</th>\n",
       "      <th>LABEL_SaO2</th>\n",
       "      <th>LABEL_Bilirubin_direct</th>\n",
       "      <th>LABEL_EtCO2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.832393</td>\n",
       "      <td>0.429798</td>\n",
       "      <td>0.896758</td>\n",
       "      <td>0.961712</td>\n",
       "      <td>0.913878</td>\n",
       "      <td>0.849364</td>\n",
       "      <td>0.150986</td>\n",
       "      <td>0.603449</td>\n",
       "      <td>0.647068</td>\n",
       "      <td>0.434431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.131074</td>\n",
       "      <td>0.304217</td>\n",
       "      <td>0.463766</td>\n",
       "      <td>0.480553</td>\n",
       "      <td>0.439553</td>\n",
       "      <td>0.238957</td>\n",
       "      <td>0.698675</td>\n",
       "      <td>0.274292</td>\n",
       "      <td>0.446310</td>\n",
       "      <td>0.232469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.417147</td>\n",
       "      <td>0.304217</td>\n",
       "      <td>0.443913</td>\n",
       "      <td>0.433482</td>\n",
       "      <td>0.425904</td>\n",
       "      <td>0.277688</td>\n",
       "      <td>0.473744</td>\n",
       "      <td>0.404472</td>\n",
       "      <td>0.446310</td>\n",
       "      <td>0.218275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.959387</td>\n",
       "      <td>0.911051</td>\n",
       "      <td>0.944069</td>\n",
       "      <td>0.961004</td>\n",
       "      <td>0.867512</td>\n",
       "      <td>0.672507</td>\n",
       "      <td>0.352497</td>\n",
       "      <td>0.647528</td>\n",
       "      <td>0.911838</td>\n",
       "      <td>0.269971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.384726</td>\n",
       "      <td>0.298570</td>\n",
       "      <td>0.566566</td>\n",
       "      <td>0.562898</td>\n",
       "      <td>0.542512</td>\n",
       "      <td>0.396905</td>\n",
       "      <td>0.221129</td>\n",
       "      <td>0.206536</td>\n",
       "      <td>0.570115</td>\n",
       "      <td>0.324562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12659</th>\n",
       "      <td>31647.0</td>\n",
       "      <td>0.338738</td>\n",
       "      <td>0.304217</td>\n",
       "      <td>0.406793</td>\n",
       "      <td>0.373973</td>\n",
       "      <td>0.411888</td>\n",
       "      <td>0.300359</td>\n",
       "      <td>0.094801</td>\n",
       "      <td>0.290623</td>\n",
       "      <td>0.446310</td>\n",
       "      <td>0.232469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12660</th>\n",
       "      <td>31649.0</td>\n",
       "      <td>0.175901</td>\n",
       "      <td>0.608352</td>\n",
       "      <td>0.760341</td>\n",
       "      <td>0.811736</td>\n",
       "      <td>0.611820</td>\n",
       "      <td>0.340112</td>\n",
       "      <td>0.677199</td>\n",
       "      <td>0.271969</td>\n",
       "      <td>0.739103</td>\n",
       "      <td>0.281824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12661</th>\n",
       "      <td>31651.0</td>\n",
       "      <td>0.929130</td>\n",
       "      <td>0.325646</td>\n",
       "      <td>0.474042</td>\n",
       "      <td>0.461187</td>\n",
       "      <td>0.443320</td>\n",
       "      <td>0.697765</td>\n",
       "      <td>0.273953</td>\n",
       "      <td>0.724276</td>\n",
       "      <td>0.364428</td>\n",
       "      <td>0.437264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12662</th>\n",
       "      <td>31652.0</td>\n",
       "      <td>0.082532</td>\n",
       "      <td>0.317979</td>\n",
       "      <td>0.525401</td>\n",
       "      <td>0.593110</td>\n",
       "      <td>0.526376</td>\n",
       "      <td>0.207084</td>\n",
       "      <td>0.635182</td>\n",
       "      <td>0.299705</td>\n",
       "      <td>0.243108</td>\n",
       "      <td>0.341546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12663</th>\n",
       "      <td>31655.0</td>\n",
       "      <td>0.117321</td>\n",
       "      <td>0.323369</td>\n",
       "      <td>0.573651</td>\n",
       "      <td>0.594063</td>\n",
       "      <td>0.588049</td>\n",
       "      <td>0.225370</td>\n",
       "      <td>0.345621</td>\n",
       "      <td>0.274292</td>\n",
       "      <td>0.446310</td>\n",
       "      <td>0.413635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12664 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           pid  LABEL_BaseExcess  LABEL_Fibrinogen  LABEL_AST  \\\n",
       "0          0.0          0.832393          0.429798   0.896758   \n",
       "1          3.0          0.131074          0.304217   0.463766   \n",
       "2          5.0          0.417147          0.304217   0.443913   \n",
       "3          7.0          0.959387          0.911051   0.944069   \n",
       "4          9.0          0.384726          0.298570   0.566566   \n",
       "...        ...               ...               ...        ...   \n",
       "12659  31647.0          0.338738          0.304217   0.406793   \n",
       "12660  31649.0          0.175901          0.608352   0.760341   \n",
       "12661  31651.0          0.929130          0.325646   0.474042   \n",
       "12662  31652.0          0.082532          0.317979   0.525401   \n",
       "12663  31655.0          0.117321          0.323369   0.573651   \n",
       "\n",
       "       LABEL_Alkalinephos  LABEL_Bilirubin_total  LABEL_Lactate  \\\n",
       "0                0.961712               0.913878       0.849364   \n",
       "1                0.480553               0.439553       0.238957   \n",
       "2                0.433482               0.425904       0.277688   \n",
       "3                0.961004               0.867512       0.672507   \n",
       "4                0.562898               0.542512       0.396905   \n",
       "...                   ...                    ...            ...   \n",
       "12659            0.373973               0.411888       0.300359   \n",
       "12660            0.811736               0.611820       0.340112   \n",
       "12661            0.461187               0.443320       0.697765   \n",
       "12662            0.593110               0.526376       0.207084   \n",
       "12663            0.594063               0.588049       0.225370   \n",
       "\n",
       "       LABEL_TroponinI  LABEL_SaO2  LABEL_Bilirubin_direct  LABEL_EtCO2  \n",
       "0             0.150986    0.603449                0.647068     0.434431  \n",
       "1             0.698675    0.274292                0.446310     0.232469  \n",
       "2             0.473744    0.404472                0.446310     0.218275  \n",
       "3             0.352497    0.647528                0.911838     0.269971  \n",
       "4             0.221129    0.206536                0.570115     0.324562  \n",
       "...                ...         ...                     ...          ...  \n",
       "12659         0.094801    0.290623                0.446310     0.232469  \n",
       "12660         0.677199    0.271969                0.739103     0.281824  \n",
       "12661         0.273953    0.724276                0.364428     0.437264  \n",
       "12662         0.635182    0.299705                0.243108     0.341546  \n",
       "12663         0.345621    0.274292                0.446310     0.413635  \n",
       "\n",
       "[12664 rows x 11 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred_medical_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling sepsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>EtCO2</th>\n",
       "      <th>PTT</th>\n",
       "      <th>BUN</th>\n",
       "      <th>Lactate</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Hgb</th>\n",
       "      <th>HCO3</th>\n",
       "      <th>BaseExcess</th>\n",
       "      <th>RRate</th>\n",
       "      <th>...</th>\n",
       "      <th>PaCO2_max</th>\n",
       "      <th>FiO2_max</th>\n",
       "      <th>Glucose_max</th>\n",
       "      <th>ABPm_max</th>\n",
       "      <th>ABPd_max</th>\n",
       "      <th>SpO2_max</th>\n",
       "      <th>Hct_max</th>\n",
       "      <th>Heartrate_max</th>\n",
       "      <th>ABPs_max</th>\n",
       "      <th>pH_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.0</td>\n",
       "      <td>10.457671</td>\n",
       "      <td>19.379766</td>\n",
       "      <td>8.085624</td>\n",
       "      <td>1.211180</td>\n",
       "      <td>36.853339</td>\n",
       "      <td>4.444106</td>\n",
       "      <td>10.845164</td>\n",
       "      <td>-0.222014</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>0.25</td>\n",
       "      <td>27.125</td>\n",
       "      <td>59.666667</td>\n",
       "      <td>42.333333</td>\n",
       "      <td>49.75</td>\n",
       "      <td>8.225</td>\n",
       "      <td>42.333333</td>\n",
       "      <td>79.916667</td>\n",
       "      <td>1.315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86.0</td>\n",
       "      <td>14.616151</td>\n",
       "      <td>39.361886</td>\n",
       "      <td>22.099093</td>\n",
       "      <td>1.971963</td>\n",
       "      <td>36.309616</td>\n",
       "      <td>8.466490</td>\n",
       "      <td>19.535822</td>\n",
       "      <td>-0.846701</td>\n",
       "      <td>18.023750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66.0</td>\n",
       "      <td>15.791465</td>\n",
       "      <td>37.113623</td>\n",
       "      <td>16.386752</td>\n",
       "      <td>1.874285</td>\n",
       "      <td>36.809351</td>\n",
       "      <td>8.364038</td>\n",
       "      <td>19.664941</td>\n",
       "      <td>-0.539940</td>\n",
       "      <td>14.931951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66.0</td>\n",
       "      <td>15.622747</td>\n",
       "      <td>38.511719</td>\n",
       "      <td>18.504078</td>\n",
       "      <td>2.037579</td>\n",
       "      <td>37.166667</td>\n",
       "      <td>10.195928</td>\n",
       "      <td>19.913676</td>\n",
       "      <td>-2.301866</td>\n",
       "      <td>15.833333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42.0</td>\n",
       "      <td>15.473433</td>\n",
       "      <td>36.413614</td>\n",
       "      <td>15.481142</td>\n",
       "      <td>1.825637</td>\n",
       "      <td>36.512461</td>\n",
       "      <td>8.253512</td>\n",
       "      <td>19.023452</td>\n",
       "      <td>-0.351043</td>\n",
       "      <td>17.255125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18990</th>\n",
       "      <td>52.0</td>\n",
       "      <td>17.530511</td>\n",
       "      <td>38.407358</td>\n",
       "      <td>16.974113</td>\n",
       "      <td>1.817651</td>\n",
       "      <td>37.262373</td>\n",
       "      <td>8.862815</td>\n",
       "      <td>20.913225</td>\n",
       "      <td>-0.813232</td>\n",
       "      <td>15.767475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18991</th>\n",
       "      <td>66.0</td>\n",
       "      <td>14.685580</td>\n",
       "      <td>37.877243</td>\n",
       "      <td>21.038035</td>\n",
       "      <td>1.679079</td>\n",
       "      <td>36.705730</td>\n",
       "      <td>8.602569</td>\n",
       "      <td>19.593550</td>\n",
       "      <td>-0.446349</td>\n",
       "      <td>17.023402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18992</th>\n",
       "      <td>44.0</td>\n",
       "      <td>16.590601</td>\n",
       "      <td>41.245585</td>\n",
       "      <td>18.441434</td>\n",
       "      <td>2.324881</td>\n",
       "      <td>37.162000</td>\n",
       "      <td>8.638379</td>\n",
       "      <td>19.648159</td>\n",
       "      <td>-1.735771</td>\n",
       "      <td>23.791434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18993</th>\n",
       "      <td>70.0</td>\n",
       "      <td>15.375473</td>\n",
       "      <td>37.376038</td>\n",
       "      <td>20.542817</td>\n",
       "      <td>2.067433</td>\n",
       "      <td>36.492035</td>\n",
       "      <td>8.199492</td>\n",
       "      <td>19.450276</td>\n",
       "      <td>-0.858656</td>\n",
       "      <td>16.415617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18994</th>\n",
       "      <td>60.0</td>\n",
       "      <td>15.324444</td>\n",
       "      <td>37.754309</td>\n",
       "      <td>18.638508</td>\n",
       "      <td>2.076506</td>\n",
       "      <td>36.656467</td>\n",
       "      <td>8.751419</td>\n",
       "      <td>19.700195</td>\n",
       "      <td>-0.664748</td>\n",
       "      <td>17.913271</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18995 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age      EtCO2        PTT        BUN   Lactate       Temp        Hgb  \\\n",
       "0      34.0  10.457671  19.379766   8.085624  1.211180  36.853339   4.444106   \n",
       "1      86.0  14.616151  39.361886  22.099093  1.971963  36.309616   8.466490   \n",
       "2      66.0  15.791465  37.113623  16.386752  1.874285  36.809351   8.364038   \n",
       "3      66.0  15.622747  38.511719  18.504078  2.037579  37.166667  10.195928   \n",
       "4      42.0  15.473433  36.413614  15.481142  1.825637  36.512461   8.253512   \n",
       "...     ...        ...        ...        ...       ...        ...        ...   \n",
       "18990  52.0  17.530511  38.407358  16.974113  1.817651  37.262373   8.862815   \n",
       "18991  66.0  14.685580  37.877243  21.038035  1.679079  36.705730   8.602569   \n",
       "18992  44.0  16.590601  41.245585  18.441434  2.324881  37.162000   8.638379   \n",
       "18993  70.0  15.375473  37.376038  20.542817  2.067433  36.492035   8.199492   \n",
       "18994  60.0  15.324444  37.754309  18.638508  2.076506  36.656467   8.751419   \n",
       "\n",
       "            HCO3  BaseExcess      RRate  ...  PaCO2_max  FiO2_max  \\\n",
       "0      10.845164   -0.222014  17.000000  ...   7.333333      0.25   \n",
       "1      19.535822   -0.846701  18.023750  ...   0.000000      0.00   \n",
       "2      19.664941   -0.539940  14.931951  ...   0.000000      0.00   \n",
       "3      19.913676   -2.301866  15.833333  ...   0.000000      0.00   \n",
       "4      19.023452   -0.351043  17.255125  ...   0.000000      0.00   \n",
       "...          ...         ...        ...  ...        ...       ...   \n",
       "18990  20.913225   -0.813232  15.767475  ...   0.000000      0.00   \n",
       "18991  19.593550   -0.446349  17.023402  ...   0.000000      0.00   \n",
       "18992  19.648159   -1.735771  23.791434  ...   0.000000      0.00   \n",
       "18993  19.450276   -0.858656  16.415617  ...   0.000000      0.00   \n",
       "18994  19.700195   -0.664748  17.913271  ...   0.000000      0.00   \n",
       "\n",
       "       Glucose_max   ABPm_max   ABPd_max  SpO2_max  Hct_max  Heartrate_max  \\\n",
       "0           27.125  59.666667  42.333333     49.75    8.225      42.333333   \n",
       "1            0.000   0.000000   0.000000      0.00    0.000       0.000000   \n",
       "2            0.000   0.000000   0.000000      0.00    0.000       0.000000   \n",
       "3            0.000   0.000000   0.000000      0.00    0.000       0.000000   \n",
       "4            0.000   0.000000   0.000000      0.00    0.000       0.000000   \n",
       "...            ...        ...        ...       ...      ...            ...   \n",
       "18990        0.000   0.000000   0.000000      0.00    0.000       0.000000   \n",
       "18991        0.000   0.000000   0.000000      0.00    0.000       0.000000   \n",
       "18992        0.000   0.000000   0.000000      0.00    0.000       0.000000   \n",
       "18993        0.000   0.000000   0.000000      0.00    0.000       0.000000   \n",
       "18994        0.000   0.000000   0.000000      0.00    0.000       0.000000   \n",
       "\n",
       "        ABPs_max  pH_max  \n",
       "0      79.916667   1.315  \n",
       "1       0.000000   0.000  \n",
       "2       0.000000   0.000  \n",
       "3       0.000000   0.000  \n",
       "4       0.000000   0.000  \n",
       "...          ...     ...  \n",
       "18990   0.000000   0.000  \n",
       "18991   0.000000   0.000  \n",
       "18992   0.000000   0.000  \n",
       "18993   0.000000   0.000  \n",
       "18994   0.000000   0.000  \n",
       "\n",
       "[18995 rows x 95 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9  8 23 20  4 33 15 30 16  1 27 12 25  3  2 13 53  5 83 32 46 28 62 44\n",
      " 26 63 18 38 74 60 79 89 43 14 36 21 59 24 48 71 93 52 87 70 69 34 73 88\n",
      " 78 92 58 65 22 80 94 72 57 90 77 82 75 11 37 67 19 85 68 39 17  0 66 10\n",
      " 47 81 45 41 76 35  7 91 84 50  6 56 86 55 61 51 42 40 31 29 54 49 64]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 7200x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feature extraction\n",
    "feature_selector = SelectKBest(score_func=f_regression, k=X_train_scaled.shape[1])\n",
    "fit = feature_selector.fit(X_train_scaled, y_train_sepsis[0])\n",
    "\n",
    "# Get the indices sorted by most important to least important\n",
    "indices = np.argsort(fit.scores_)[::-1]\n",
    "\n",
    "print(indices)\n",
    "# To get your top 10 feature names\n",
    "features = []\n",
    "for i in range(X_train_scaled.shape[1]):\n",
    "    features.append(df_train.columns[indices[i]])\n",
    "\n",
    "# Now plot\n",
    "plt.rcParams['figure.figsize'] = 100, 10\n",
    "plt.figure()\n",
    "plt.bar(features, fit.scores_[indices[range(X_train_scaled.shape[1])]], color='r', align='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting model\n",
      "[0 0 0 ... 1 1 1]\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   22.4s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:   36.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC score on test set 0.6795178031877602\n",
      "CV score 0.6437475757575757\n",
      "Finished test for medical tests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   46.3s finished\n"
     ]
    }
   ],
   "source": [
    "# Model and predict sepsis\n",
    "\n",
    "clf = xgb.XGBClassifier(objective=\"binary:logistic\", n_thread=-1)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train_scaled, y_train_sepsis[0], test_size=0.10, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "        \"booster\": [\"dart\"],\n",
    "        \"eta\": np.arange(0,1,0.1),\n",
    "        \"min_child_weight\": range(1, 10, 1),\n",
    "        \"max_depth\": range(4, 10, 1),\n",
    "        \"gamma\": range(0, 100, 1),\n",
    "        \"max_delta_step\": range(1, 10, 1),\n",
    "        \"subsample\": np.arange(0.1, 1, 0.05),\n",
    "        \"colsample_bytree\": np.arange(0.3, 1, 0.05),\n",
    "        \"n_estimators\": range(50, 150, 1),\n",
    "        \"scale_pos_weight\": [1],\n",
    "        \"reg_lambda\": [0, 1], # Ridge regularization\n",
    "        \"reg_alpha\": [0, 1], # Lasso regularization\n",
    "        \"eval_metric\": [\"error\"],\n",
    "        \"verbosity\": [1]\n",
    "    }\n",
    "\n",
    "print(\"Resampling\")\n",
    "sampler = RandomUnderSampler()\n",
    "X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Applying feature selection\")\n",
    "feature_selector = SelectKBest(score_func=f_classif, k=10)\n",
    "X_train = feature_selector.fit_transform(X_train_res, y_train_res)\n",
    "X_test = feature_selector.transform(X_test)\n",
    "\n",
    "\n",
    "print(\"Fitting model\")\n",
    "coarse_search = RandomizedSearchCV(estimator=clf,\n",
    "        param_distributions=param_grid, scoring=\"roc_auc\",\n",
    "        n_jobs=-1, cv=10, n_iter=100, verbose=1)\n",
    "print(y_train_res)\n",
    "coarse_search.fit(X_train, y_train_res)\n",
    "\n",
    "sepsis_model = coarse_search.best_estimator_\n",
    "print(f\"ROC score on test set {roc_auc_score(y_test, coarse_search.best_estimator_.predict_proba(X_test)[:,1])}\")\n",
    "print(f\"CV score {coarse_search.best_score_}\")\n",
    "print(f\"Finished test for medical tests.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling\n",
      "Applying feature selection\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-634f414c4ecf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0msvc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0msepsis_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ROC score on test set {roc_auc_score(y_test, sigmoid_f(clf.best_estimator_.decision_function(X_test)))}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iml/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train_scaled, y_train_sepsis[0], test_size=0.10, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Resampling\")\n",
    "sampler = ADASYN()\n",
    "X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Applying feature selection\")\n",
    "feature_selector = SelectKBest(score_func=f_classif, k=60)\n",
    "X_train = feature_selector.fit_transform(X_train_res, y_train_res)\n",
    "X_test = feature_selector.transform(X_test)\n",
    "\n",
    "parameters = {\n",
    "        \"C\": np.linspace(0.1, 10, num=3),\n",
    "        \"kernel\": [\"rbf\", \"sigmoid\"],\n",
    "        \"gamma\": np.linspace(0.1, 10, num=3),  # for poly or rbf kernel\n",
    "        \"coef0\": [0],\n",
    "        \"shrinking\": [True],\n",
    "        \"probability\": [False],\n",
    "        \"cache_size\": [1000],\n",
    "        \"class_weight\": [None],\n",
    "        \"verbose\": [2],\n",
    "        \"decision_function_shape\": [\"ovo\"],  # only binary variables are set\n",
    "        \"random_state\": [42],\n",
    "        \"max_iter\": [2000]\n",
    "    }\n",
    "\n",
    "svc = SVC()\n",
    "clf = GridSearchCV(svc, parameters, cv=2, n_jobs=-1)\n",
    "clf.fit(X_train, y_train_res)\n",
    "sepsis_model = clf.best_estimator_\n",
    "print(f\"ROC score on test set {roc_auc_score(y_test, sigmoid_f(clf.best_estimator_.decision_function(X_test)))}\")\n",
    "print(f\"CV score {clf.best_score_}\")\n",
    "print(f\"Finished test for medical tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgboost_fine_sepsis.pkl']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(sepsis_model, f\"xgboost_fine_sepsis.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_sepsis = feature_selector.transform(X_val_scaled)\n",
    "y_pred = sepsis_model.predict_proba(X_val_sepsis)[:,1]\n",
    "df_pred_sepsis = pd.DataFrame(y_pred, index=val_pids, columns=SEPSIS)\n",
    "df_pred_sepsis = df_pred_sepsis.reset_index().rename(columns={\"index\": \"pid\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling vital signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model for LABEL_RRate.\n",
      "Applying feature selection\n",
      "Fitting model\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   18.6s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  8.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score 0.3762834615674595\n",
      "Test score is 0.3651876778962392\n",
      "Finished test for medical tests.\n",
      "Fitting model for LABEL_ABPm.\n",
      "Applying feature selection\n",
      "Fitting model\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  6.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score 0.5772479021545291\n",
      "Test score is 0.5894346373011152\n",
      "Finished test for medical tests.\n",
      "Fitting model for LABEL_SpO2.\n",
      "Applying feature selection\n",
      "Fitting model\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   49.5s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  4.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score 0.30896617018979444\n",
      "Test score is 0.3392388962178333\n",
      "Finished test for medical tests.\n",
      "Fitting model for LABEL_Heartrate.\n",
      "Applying feature selection\n",
      "Fitting model\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   18.3s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  8.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score 0.5974856981684256\n",
      "Test score is 0.6043524041430639\n",
      "Finished test for medical tests.\n"
     ]
    }
   ],
   "source": [
    "# Modelling of vital signs\n",
    "models = []\n",
    "losses = []\n",
    "feature_selectors_vital_signs = []\n",
    "clf = xgb.XGBRegressor(objective=\"reg:squarederror\", n_thread=-1)\n",
    "\n",
    "for i, sign in enumerate(VITAL_SIGNS):\n",
    "    print(f\"Fitting model for {sign}.\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_train_scaled, y_train_vital_signs[i], test_size=0.10, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    print(\"Applying feature selection\")\n",
    "    feature_selector = SelectKBest(score_func=f_classif, k=5)\n",
    "    X_train_selected = feature_selector.fit_transform(X_train, y_train)\n",
    "    X_test_selected = feature_selector.transform(X_test)\n",
    "    feature_selectors_vital_signs.append(feature_selector)\n",
    "\n",
    "    print(\"Fitting model\")\n",
    "    \n",
    "    param_grid = {\n",
    "        \"booster\": [\"dart\"],\n",
    "        \"eta\": np.arange(0,1,0.1),\n",
    "        \"min_child_weight\": range(1, 10, 1),\n",
    "        \"max_depth\": range(4, 10, 1),\n",
    "        \"gamma\": range(0, 100, 1),\n",
    "        \"max_delta_step\": range(1, 10, 1),\n",
    "        \"subsample\": np.arange(0.1, 1, 0.05),\n",
    "        \"colsample_bytree\": np.arange(0.3, 1, 0.05),\n",
    "        \"n_estimators\": range(50, 150, 1),\n",
    "        \"scale_pos_weight\": [1],\n",
    "        \"reg_lambda\": [0, 1], # Ridge regularization\n",
    "        \"reg_alpha\": [0, 1], # Lasso regularization\n",
    "        \"eval_metric\": [\"error\"],\n",
    "        \"verbosity\": [1]\n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "    coarse_search = RandomizedSearchCV(estimator=clf,\n",
    "            param_distributions=param_grid, scoring=\"r2\",\n",
    "            n_jobs=-1, cv=10, n_iter=100, verbose=1)\n",
    "    coarse_search.fit(X_train_selected, y_train)\n",
    "    models.append(coarse_search.best_estimator_)\n",
    "    print(f\"CV score {coarse_search.best_score_}\")\n",
    "    print(f\"Test score is {r2_score(y_test, coarse_search.best_estimator_.predict(X_test_selected))}\")\n",
    "    print(f\"Finished test for medical tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(models):\n",
    "    joblib.dump(models[i], f\"xgboost_fine_{VITAL_SIGNS[i]}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelling vital signs with multitarget ANN \n",
    "class FeedforwardVital(torch.nn.Module):\n",
    "    \"\"\" Definition of the feedfoward neural network. It currently has three layers which can be\n",
    "    modified in the function where the network is trained.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_layers, input_size, hidden_size, output_size, p=0.2):\n",
    "        super(FeedforwardVital, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout = torch.nn.Dropout(p=p)\n",
    "        self.bn = torch.nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.fc = [torch.nn.Linear(self.input_size, self.hidden_size)]\n",
    "        torch.nn.init.xavier_normal_(self.fc[0].weight)\n",
    "        for i in range(1,n_layers):\n",
    "            self.fc.append(torch.nn.Linear(self.hidden_size, self.hidden_size))\n",
    "            torch.nn.init.xavier_normal_(self.fc[i].weight)\n",
    "        self.fcout = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "        torch.nn.init.xavier_normal_(self.fcout.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Function where the forward pass is defined. The backward pass is deternmined by the\n",
    "            autograd function built into PyTorch.\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor (n_samples,n_features) tensor containing training input\n",
    "                features\n",
    "            subtask (int): subtask performed (choice: 1,2,3)\n",
    "        Returns:\n",
    "            output (torch.Tensor): (n_samples,n_features) tensor containing\n",
    "                the predicted output for each sample.\n",
    "        \"\"\"\n",
    "        hidden = self.fc[0](x)\n",
    "        hidden_bn = self.bn(hidden)\n",
    "        relu = self.sigmoid(hidden_bn)\n",
    "        for i in range(1,self.n_layers):\n",
    "            hidden = self.dropout(self.fc[i](relu))\n",
    "            hidden_bn = self.bn(hidden)\n",
    "            relu = self.sigmoid(hidden_bn)\n",
    "        output = self.fcout(relu)\n",
    "        output = self.relu(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    \"\"\" Class used to load the data in minibatches to control the neural network stability during\n",
    "        training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.len = self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_vital_signs(x_input, y_input, device):\n",
    "    models = []\n",
    "    losses = []\n",
    "    y_input = np.array(y_input)\n",
    "    labels = np.array([y_input[:,i] for i in range(y_input.shape[1])])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        x_input, labels, test_size=0.10, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    print(\"Converting arrays to tensors\")\n",
    "    X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = convert_to_cuda_tensor(\n",
    "        X_train, X_test, y_train, y_test, device\n",
    "    )\n",
    "\n",
    "    model = FeedforwardVital(\n",
    "        n_layers=3, input_size=X_train_tensor.shape[1], hidden_size=100, output_size=4, p=0.2\n",
    "    )\n",
    "    \n",
    "    criterion = torch.nn.SmoothL1Loss()\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.09)\n",
    "    dataset = Data(X_train_tensor, y_train_tensor)\n",
    "    batch_size = 2048  # Ideally we want powers of 2\n",
    "    trainloader = DataLoader(dataset=dataset, batch_size=batch_size)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    model.float()\n",
    "\n",
    "    for epoch in tqdm(list(range(251))):\n",
    "        LOSS = []\n",
    "        for x, y in trainloader:\n",
    "            yhat = model(x)\n",
    "            loss = criterion(yhat.float(), y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            LOSS.append(loss)\n",
    "        X_test_tensor = X_test_tensor.to(device)\n",
    "        y_test_pred = model(X_test_tensor).cpu().detach().numpy()\n",
    "        y_train_pred = model(X_train_tensor).cpu().detach().numpy()\n",
    "        loss_average = sum(LOSS) / len(LOSS)\n",
    "        # db = np.vectorize(lambda x: (0 if x < 0.5 else 1))\n",
    "        # y_test_pred_bin = db(y_test_pred)\n",
    "        # y_train_pred_bin = db(y_train_pred)\n",
    "        R2_score_train = r2_score(y_train, y_train_pred)\n",
    "        R2_score_test = r2_score(y_test, y_test_pred)\n",
    "        if epoch%10==0:\n",
    "            print(\"Training_R2 : \", R2_score_train,\"epoch : \", epoch)\n",
    "            print(\"Testing_R2 : \", R2_score_test,\"epoch : \", epoch)\n",
    "\n",
    "\n",
    "    print(\n",
    "        f\"Value of the test sample is {y_test} and value for the predicted \"\n",
    "        f\"sample is {y_test_pred}\"\n",
    "    )\n",
    "    models.append(model)\n",
    "    losses.append(LOSS)\n",
    "    print(f\"Finished test for vital signs.\")\n",
    "    return models, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting arrays to tensors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efc817e20294a5aae18b27f7ed26fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=251.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_R2 :  -533.956498715256 epoch :  0\n",
      "Testing_R2 :  -548.9716743560629 epoch :  0\n",
      "Training_R2 :  -461.8155924478337 epoch :  10\n",
      "Testing_R2 :  -474.8276858372353 epoch :  10\n",
      "Training_R2 :  -392.55915520110244 epoch :  20\n",
      "Testing_R2 :  -403.6482339746674 epoch :  20\n",
      "Training_R2 :  -326.37934018150406 epoch :  30\n",
      "Testing_R2 :  -335.59508170000726 epoch :  30\n",
      "Training_R2 :  -262.2093825395298 epoch :  40\n",
      "Testing_R2 :  -269.68581199493485 epoch :  40\n",
      "Training_R2 :  -199.50153219408014 epoch :  50\n",
      "Testing_R2 :  -205.2015630301718 epoch :  50\n",
      "Training_R2 :  -139.2792788622069 epoch :  60\n",
      "Testing_R2 :  -143.1916968660144 epoch :  60\n",
      "Training_R2 :  -84.01847154717964 epoch :  70\n",
      "Testing_R2 :  -86.46156807238229 epoch :  70\n",
      "Training_R2 :  -38.68559360085561 epoch :  80\n",
      "Testing_R2 :  -39.7798410463508 epoch :  80\n",
      "Training_R2 :  -9.899117143854594 epoch :  90\n",
      "Testing_R2 :  -10.138680817217153 epoch :  90\n",
      "Training_R2 :  -0.49907178326339846 epoch :  100\n",
      "Testing_R2 :  -0.4959300208749709 epoch :  100\n",
      "Training_R2 :  -0.35263837172992774 epoch :  110\n",
      "Testing_R2 :  -0.36821719931721597 epoch :  110\n",
      "Training_R2 :  -0.3308574929170362 epoch :  120\n",
      "Testing_R2 :  -0.340205665985147 epoch :  120\n",
      "Training_R2 :  -0.33873167423720446 epoch :  130\n",
      "Testing_R2 :  -0.33971541753257717 epoch :  130\n",
      "Training_R2 :  -0.3182955623928171 epoch :  140\n",
      "Testing_R2 :  -0.2922806775052445 epoch :  140\n",
      "Training_R2 :  -0.3173026111391881 epoch :  150\n",
      "Testing_R2 :  -0.34025432994689386 epoch :  150\n",
      "Training_R2 :  -0.30992847968185977 epoch :  160\n",
      "Testing_R2 :  -0.30163706849279864 epoch :  160\n",
      "Training_R2 :  -0.30929961965882896 epoch :  170\n",
      "Testing_R2 :  -0.3205595628388761 epoch :  170\n",
      "Training_R2 :  -0.3058625048823502 epoch :  180\n",
      "Testing_R2 :  -0.3070161816291756 epoch :  180\n",
      "Training_R2 :  -0.2925207510607662 epoch :  190\n",
      "Testing_R2 :  -0.2841047248678956 epoch :  190\n",
      "Training_R2 :  -0.2981103166742512 epoch :  200\n",
      "Testing_R2 :  -0.2741747622991443 epoch :  200\n",
      "Training_R2 :  -0.288763778024853 epoch :  210\n",
      "Testing_R2 :  -0.28655542483657837 epoch :  210\n",
      "Training_R2 :  -0.28361887790118523 epoch :  220\n",
      "Testing_R2 :  -0.2738708363522638 epoch :  220\n",
      "Training_R2 :  -0.28232032359073334 epoch :  230\n",
      "Testing_R2 :  -0.27561257654736726 epoch :  230\n",
      "Training_R2 :  -0.27620551838485957 epoch :  240\n",
      "Testing_R2 :  -0.27173175641500424 epoch :  240\n",
      "Training_R2 :  -0.2758064165308986 epoch :  250\n",
      "Testing_R2 :  -0.27783853838080014 epoch :  250\n",
      "\n",
      "Value of the test sample is [[ 16  63  97  87]\n",
      " [ 19 112  99  91]\n",
      " [ 22  64  95 105]\n",
      " ...\n",
      " [ 22  72  94 103]\n",
      " [ 17  84  95  80]\n",
      " [ 18  80  97  81]] and value for the predicted sample is [[17.500923 79.16353  94.99142  81.80519 ]\n",
      " [18.149458 81.57728  97.12741  84.15927 ]\n",
      " [17.63275  78.81653  94.448265 80.60246 ]\n",
      " ...\n",
      " [18.631243 82.681076 99.08991  85.421   ]\n",
      " [18.355938 81.97425  97.933685 84.21968 ]\n",
      " [17.770515 80.79076  96.4942   82.96898 ]]\n",
      "Finished test for vital signs.\n"
     ]
    }
   ],
   "source": [
    "models_vital_signs, losses_vital_signs = get_model_vital_signs(X_train_scaled, y_train_vital_signs, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for vital signs\n",
    "df_pred_vital_signs = pd.DataFrame(index=val_pids, columns=VITAL_SIGNS)\n",
    "for i, test in enumerate(VITAL_SIGNS):\n",
    "    feature_selector = feature_selectors_vital_signs[i]\n",
    "    X_val_vital_sign = feature_selector.transform(X_val_scaled)\n",
    "    model_for_test = models[i]\n",
    "    y_pred = model_for_test.predict(X_val_vital_sign)\n",
    "    df_pred_vital_signs[test] = y_pred\n",
    "\n",
    "df_pred_vital_signs = df_pred_vital_signs.reset_index().rename(columns={\"index\": \"pid\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to ZIP file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export predictions DataFrame to a zip file\n",
      "           pid  LABEL_BaseExcess  LABEL_Fibrinogen  LABEL_AST  \\\n",
      "0          0.0          0.832393          0.429798   0.896758   \n",
      "1          3.0          0.131074          0.304217   0.463766   \n",
      "2          5.0          0.417147          0.304217   0.443913   \n",
      "3          7.0          0.959387          0.911051   0.944069   \n",
      "4          9.0          0.384726          0.298570   0.566566   \n",
      "...        ...               ...               ...        ...   \n",
      "12659  31647.0          0.338738          0.304217   0.406793   \n",
      "12660  31649.0          0.175901          0.608352   0.760341   \n",
      "12661  31651.0          0.929130          0.325646   0.474042   \n",
      "12662  31652.0          0.082532          0.317979   0.525401   \n",
      "12663  31655.0          0.117321          0.323369   0.573651   \n",
      "\n",
      "       LABEL_Alkalinephos  LABEL_Bilirubin_total  LABEL_Lactate  \\\n",
      "0                0.961712               0.913878       0.849364   \n",
      "1                0.480553               0.439553       0.238957   \n",
      "2                0.433482               0.425904       0.277688   \n",
      "3                0.961004               0.867512       0.672507   \n",
      "4                0.562898               0.542512       0.396905   \n",
      "...                   ...                    ...            ...   \n",
      "12659            0.373973               0.411888       0.300359   \n",
      "12660            0.811736               0.611820       0.340112   \n",
      "12661            0.461187               0.443320       0.697765   \n",
      "12662            0.593110               0.526376       0.207084   \n",
      "12663            0.594063               0.588049       0.225370   \n",
      "\n",
      "       LABEL_TroponinI  LABEL_SaO2  LABEL_Bilirubin_direct  LABEL_EtCO2  \\\n",
      "0             0.150986    0.603449                0.647068     0.434431   \n",
      "1             0.698675    0.274292                0.446310     0.232469   \n",
      "2             0.473744    0.404472                0.446310     0.218275   \n",
      "3             0.352497    0.647528                0.911838     0.269971   \n",
      "4             0.221129    0.206536                0.570115     0.324562   \n",
      "...                ...         ...                     ...          ...   \n",
      "12659         0.094801    0.290623                0.446310     0.232469   \n",
      "12660         0.677199    0.271969                0.739103     0.281824   \n",
      "12661         0.273953    0.724276                0.364428     0.437264   \n",
      "12662         0.635182    0.299705                0.243108     0.341546   \n",
      "12663         0.345621    0.274292                0.446310     0.413635   \n",
      "\n",
      "       LABEL_Sepsis  LABEL_RRate  LABEL_ABPm  LABEL_SpO2  LABEL_Heartrate  \n",
      "0          0.752551    16.098423   85.067841   97.806915        84.746559  \n",
      "1          0.332614    17.577841   84.925156   96.260529        89.948601  \n",
      "2          0.296633    18.384174   72.514221   95.604843        67.635735  \n",
      "3          0.764247    16.582848   88.701813   97.345406        94.783012  \n",
      "4          0.645422    20.150539   88.608429   95.481430        91.113228  \n",
      "...             ...          ...         ...         ...              ...  \n",
      "12659      0.292379    16.750763   69.930702   96.260529        73.083839  \n",
      "12660      0.419551    15.429890   81.475014   96.065491        91.500526  \n",
      "12661      0.522059    17.051477   77.414116   97.674973        84.375969  \n",
      "12662      0.382621    18.422857   96.369110   96.906197       108.435699  \n",
      "12663      0.343168    17.119041   83.600609   97.900909       105.709663  \n",
      "\n",
      "[12664 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "df_predictions = pd.merge(df_pred_medical_test, df_pred_sepsis, on=\"pid\")\n",
    "df_predictions = pd.merge(df_predictions, df_pred_vital_signs, on=\"pid\")\n",
    "print(\"Export predictions DataFrame to a zip file\")\n",
    "print(df_predictions)\n",
    "df_predictions.to_csv(\n",
    "    \"predictions.csv\",\n",
    "    index=None,\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    encoding=\"utf-8-sig\",\n",
    "    float_format=\"%.2f\",\n",
    ")\n",
    "\n",
    "with zipfile.ZipFile(\"predictions.zip\", \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    zf.write(\"predictions.csv\")\n",
    "os.remove(\"predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
