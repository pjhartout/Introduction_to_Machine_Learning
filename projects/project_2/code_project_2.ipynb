{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the solution to project 2 from scratch\n",
    "To note:\n",
    "- For clarity the df_test was renamed to df_val as the test word was used when splitting the labeled data into train and test. \n",
    "    - Val stands for validation\n",
    "\n",
    "To try out:\n",
    "- Preprocessing\n",
    "    - Tweak data imputer\n",
    "    - Tweak scaler (Robust scaler, minmax, etc..)\n",
    "    - Tweak feature selection parameter\n",
    "    - Tweak order of operations above to see the effect\n",
    "- Modelling\n",
    "    - XGBoost\n",
    "    - SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import zipfile\n",
    "import time\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import ADASYN, SMOTE\n",
    "from imblearn.under_sampling import ClusterCentroids, RandomUnderSampler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, chi2, f_classif\n",
    "from sklearn.metrics import f1_score, mean_squared_error, accuracy_score, r2_score, roc_auc_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import label_ranking_average_precision_score as LRAPS\n",
    "from sklearn.metrics import label_ranking_loss as LRL\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Global variables\n",
    "IDENTIFIERS = [\"pid\", \"Time\"]\n",
    "MEDICAL_TESTS = [\n",
    "    \"LABEL_BaseExcess\",\n",
    "    \"LABEL_Fibrinogen\",\n",
    "    \"LABEL_AST\",\n",
    "    \"LABEL_Alkalinephos\",\n",
    "    \"LABEL_Bilirubin_total\",\n",
    "    \"LABEL_Lactate\",\n",
    "    \"LABEL_TroponinI\",\n",
    "    \"LABEL_SaO2\",\n",
    "    \"LABEL_Bilirubin_direct\",\n",
    "    \"LABEL_EtCO2\",\n",
    "]\n",
    "VITAL_SIGNS = [\"LABEL_RRate\", \"LABEL_ABPm\", \"LABEL_SpO2\", \"LABEL_Heartrate\"]\n",
    "SEPSIS = [\"LABEL_Sepsis\"]\n",
    "ESTIMATOR = {\"bayesian\": BayesianRidge(), \"decisiontree\": DecisionTreeRegressor(max_features=\"sqrt\", random_state=0), \n",
    "                \"extratree\": ExtraTreesRegressor(n_estimators=10, random_state=0), \n",
    "                \"knn\": KNeighborsRegressor(n_neighbors=10, weights=\"distance\")}\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_f(x):\n",
    "    \"\"\"To get predictions as confidence level, the model predicts for all 12 sets of measures for\n",
    "    each patient a distance to the hyperplane ; it is then transformed into a confidence level using\n",
    "    the sigmoid function ; the confidence level reported is the mean of all confidence levels for a\n",
    "    single patient\n",
    "\n",
    "    Args:\n",
    "        x (float): input of the sigmoid function\n",
    "\n",
    "    Returns:\n",
    "       float: result of the sigmoid computation.\n",
    "\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r\"data/train_features.csv\")\n",
    "df_train_label = pd.read_csv(r\"data/train_labels.csv\")\n",
    "df_val = pd.read_csv(r\"data/test_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data imputation methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philiphartout/anaconda3/envs/iml/lib/python3.7/site-packages/sklearn/impute/_iterative.py:638: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Fit imputer to missing data\n",
    "pid_train = df_train[\"pid\"].unique()\n",
    "columns = df_train.columns\n",
    "df_train_preprocessed = pd.DataFrame(columns=columns, index=pid_train)\n",
    "\n",
    "imputer = IterativeImputer()\n",
    "columns = df_train.columns\n",
    "df_train = imputer.fit_transform(df_train.values)\n",
    "df_train = pd.DataFrame(df_train, columns=columns)\n",
    "df_train = df_train.groupby([\"pid\"], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranform test data according to same imputer\n",
    "pid_val = df_val[\"pid\"].unique()\n",
    "columns = df_val.columns\n",
    "df_val_preprocessed = pd.DataFrame(columns=columns, index=pid_val)\n",
    "\n",
    "df_val = imputer.transform(df_val.values)\n",
    "df_val = pd.DataFrame(df_val, columns=columns)\n",
    "df_val = df_val.groupby([\"pid\"], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"df_train_philip.csv\")\n",
    "df_val.to_csv(\"df_val_philip.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_preprocessed = pd.read_csv(\"df_train_philip.csv\")\n",
    "df_val_preprocessed = pd.read_csv(\"df_val_philip.csv\")\n",
    "df_train_preprocessed = df_train_preprocessed.sort_values(by=[\"pid\"])\n",
    "df_val_preprocessed = df_val_preprocessed.sort_values(by=[\"pid\"])\n",
    "df_train_label = df_train_label.sort_values(by=[\"pid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a list of labels for each medical test\n",
      "Creating a list of labels for sepsis\n",
      "Creating a list of labels for each vital sign\n"
     ]
    }
   ],
   "source": [
    "# Data formatting\n",
    "X_train = df_train_preprocessed.drop(columns=IDENTIFIERS+['Unnamed: 0']).values\n",
    "X_val = df_val_preprocessed.drop(columns=IDENTIFIERS+['Unnamed: 0']).values\n",
    "# Create list with different label for each medical test\n",
    "print(\"Creating a list of labels for each medical test\")\n",
    "y_train_medical_tests = []\n",
    "for test in MEDICAL_TESTS:\n",
    "    y_train_medical_tests.append(df_train_label[test].astype(int).values)\n",
    "\n",
    "# Create list with different label for sepsis\n",
    "print(\"Creating a list of labels for sepsis\")\n",
    "y_train_sepsis = []\n",
    "for sepsis in SEPSIS:\n",
    "    y_train_sepsis.append(df_train_label[sepsis].astype(int).values)\n",
    "\n",
    "# Create list with different label for each vital sign\n",
    "print(\"Creating a list of labels for each vital sign\")\n",
    "y_train_vital_signs = []\n",
    "for sign in VITAL_SIGNS:\n",
    "    y_train_vital_signs.append(df_train_label[sign].astype(int).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data \n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling medical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Modelling of medical tests using logistic regression with cross validation\n",
    "# models = []\n",
    "# losses = []\n",
    "# columns_medical_tests = []\n",
    "# for i, test in enumerate(MEDICAL_TESTS):\n",
    "#     print(f\"Fitting model for {test}.\")\n",
    "\n",
    "#     print(\"Applying feature selection\")\n",
    "#     feature_selector = SelectKBest(score_func=f_classif, k=3)\n",
    "#     X_train = feature_selector.fit_transform(X_train, y_train_medical_tests[i])\n",
    "#     X_test = feature_selector.transform(X_test)\n",
    "#     columns = feature_selector.get_support(indices=True)\n",
    "#     columns_medical_tests.append(columns)\n",
    "\n",
    "#     print(\"Fitting model\")\n",
    "#     clf = LogisticRegressionCV(cv=5, random_state=42).fit(X_train, y_train_medical_tests[i])\n",
    "#     models.append(clf)\n",
    "#     print(roc_auc_score(y_test, clf.predict_proba(X_test)[:,1]))\n",
    "#     print(f\"Finished test for medical tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READY TO ROLL\n"
     ]
    }
   ],
   "source": [
    "print(\"READY TO ROLL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_cuda_tensor(X_train, X_test, y_train, y_test, device):\n",
    "    \"\"\"Converts a number of np.ndarrays to tensors placed on the device specified.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): (n_samples, n_features) array containing training features\n",
    "        X_test (np.ndarray): (n_samples, n_features) array containing testing features\n",
    "        y_train (np.ndarray): (n_samples,) array containing training labels\n",
    "        y_test (np.ndarray): (n_samples,) array containing testing labels\n",
    "        device (torch.device): device on which the tensors should be placed (CPU/CUDA GPU)\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    return (\n",
    "        torch.from_numpy(X_train).to(device).float(),\n",
    "        torch.from_numpy(X_test).to(device).float(),\n",
    "        torch.from_numpy(y_train).to(device).float(),\n",
    "        torch.from_numpy(y_test).to(device).float(),\n",
    "    )\n",
    "\n",
    "\n",
    "class Feedforward(torch.nn.Module):\n",
    "    \"\"\" Definition of the feedfoward neural network. It currently has three layers which can be\n",
    "    modified in the function where the network is trained.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, subtask, p=0.2):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.subtask = subtask\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout = torch.nn.Dropout(p=p)\n",
    "        self.bn = torch.nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        torch.nn.init.xavier_normal_(self.fc1.weight)\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        torch.nn.init.xavier_normal_(self.fc2.weight)\n",
    "        self.fcout = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "        torch.nn.init.xavier_normal_(self.fcout.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Function where the forward pass is defined. The backward pass is deternmined by the\n",
    "            autograd function built into PyTorch.\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor (n_samples,n_features) tensor containing training input\n",
    "                features\n",
    "            subtask (int): subtask performed (choice: 1,2,3)\n",
    "        Returns:\n",
    "            output (torch.Tensor): (n_samples,n_features) tensor containing\n",
    "                the predicted output for each sample.\n",
    "        \"\"\"\n",
    "        assert self.subtask in [1, 2, 3]\n",
    "        hidden = self.fc1(x)\n",
    "        hidden_bn = self.bn(hidden)\n",
    "        relu = self.sigmoid(hidden_bn)\n",
    "        hidden = self.dropout(self.fc2(relu))\n",
    "        hidden_bn = self.bn(hidden)\n",
    "        relu = self.sigmoid(hidden_bn)\n",
    "        output = self.fcout(relu)\n",
    "        if self.subtask == 1 or self.subtask == 2:\n",
    "            output = self.sigmoid(output)\n",
    "        else:\n",
    "            output = self.relu(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Data(Dataset):\n",
    "    \"\"\" Class used to load the data in minibatches to control the neural network stability during\n",
    "        training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.len = self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Modelling using extreme gradient boosting\n",
    "# models = []\n",
    "# losses = []\n",
    "# feature_selector_medical_tests = []\n",
    "# for i, test in enumerate(MEDICAL_TESTS):\n",
    "#     print(f\"Fitting model for {test}.\")\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X_train_scaled, y_train_medical_tests[i], test_size=0.10, random_state=42, shuffle=True\n",
    "#     )\n",
    "#     # Coarse parameter grid not optimized at all yet\n",
    "    \n",
    "#     print(\"Resampling\")\n",
    "#     sampler = RandomUnderSampler(random_state=42)\n",
    "#     X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "#     print(\"Applying feature selection\")\n",
    "#     feature_selector = SelectKBest(score_func=f_classif, k=10)\n",
    "#     X_train = feature_selector.fit_transform(X_train_res, y_train_res)\n",
    "#     X_test = feature_selector.transform(X_test)\n",
    "#     feature_selector_medical_tests.append(feature_selector)\n",
    "    \n",
    "    \n",
    "#     X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = convert_to_cuda_tensor(\n",
    "#             X_train, X_test, y_train_res, y_test, DEVICE\n",
    "#     )\n",
    "    \n",
    "#     model = Feedforward(\n",
    "#             input_size=X_train_tensor.shape[1],\n",
    "#             hidden_size=100,\n",
    "#             output_size=1,\n",
    "#             subtask=1,\n",
    "#             p=0.1,\n",
    "#     )\n",
    "    \n",
    "#     criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# #     if optim == \"SGD\":\n",
    "# #         optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "# #     elif optim == \"Adam\":\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "#     dataset = Data(X_train_tensor, y_train_tensor)\n",
    "#     batch_size = 1024  # Ideally we want powers of 2\n",
    "#     trainloader = DataLoader(dataset=dataset, batch_size=batch_size)\n",
    "\n",
    "#     if torch.cuda.is_available():\n",
    "#         model.cuda()\n",
    "#     model.float()\n",
    "    \n",
    "    \n",
    "#     dirpath = os.path.join(os.getcwd(), \"runs\")\n",
    "#     fileList = os.listdir(dirpath)\n",
    "#     for fileName in fileList:\n",
    "#         shutil.rmtree(dirpath + \"/\" + fileName)\n",
    "    \n",
    "#     now = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "#     writer = SummaryWriter(\n",
    "#         log_dir=f\"runs/ann_network_runs_{200}_epochs_{now}_{test}\"\n",
    "#     )\n",
    "#     for epoch in list(range(500)):\n",
    "#         LOSS = []\n",
    "#         for x, y in trainloader:\n",
    "#             yhat = model(x)\n",
    "#             loss = criterion(yhat.float(), y.reshape((y.shape[0], 1)))\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             LOSS.append(loss)\n",
    "#         X_test_tensor = X_test_tensor.to(DEVICE)\n",
    "#         y_test_pred = model(X_test_tensor).cpu().detach().numpy()\n",
    "#         y_train_pred = model(X_train_tensor).cpu().detach().numpy()\n",
    "#         loss_average = sum(LOSS) / len(LOSS)\n",
    "#         writer.add_scalar(\"Training_loss\", loss_average, epoch)\n",
    "#         ROC_train = roc_auc_score(y_train_res, y_train_pred)\n",
    "#         ROC_test = roc_auc_score(y_test, y_test_pred)\n",
    "# #         writer.add_scalar(\"ROC train\", ROC_train, epoch)\n",
    "# #         writer.add_scalar(\"ROC test\", ROC_test, epoch)\n",
    "#         print(f\"Epoch: {epoch} - ROC train: {ROC_train} - ROC test: {ROC_test}\", end=\"\\r\")\n",
    "#     print(\"\")\n",
    "        \n",
    "# #     models.append(model)\n",
    "# #     model.eval()\n",
    "# #     print(f\"ROC score on test set {roc_auc_score(y_test, model(X_test_tensor))}\")\n",
    "# #     print(f\"CV score {coarse_search.best_score_}\")\n",
    "# print(f\"Finished test for medical tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model for LABEL_BaseExcess.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   30.7s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed: 11.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed: 17.1min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed: 20.1min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed: 23.9min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed: 28.2min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed: 28.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92124844 0.11344181 0.29810002 ... 0.91302997 0.27503392 0.8153258 ]\n",
      "ROC score on test set 0.879664081103796\n",
      "CV score 0.8627901854812372\n",
      "ROC score on test set 0.879664081103796\n",
      "CV score 0.8627901854812372\n",
      "Fitting model for LABEL_Fibrinogen.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:   30.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:   47.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:  6.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.718125   0.5770305  0.52418727 ... 0.4888774  0.32086784 0.36808202]\n",
      "ROC score on test set 0.7243470422535212\n",
      "CV score 0.7329693651574803\n",
      "ROC score on test set 0.7243470422535212\n",
      "CV score 0.7329693651574803\n",
      "Fitting model for LABEL_AST.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   25.7s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed: 15.7min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed: 18.6min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed: 21.7min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed: 22.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8374758  0.6264091  0.33403337 ... 0.58397895 0.38864544 0.4827148 ]\n",
      "ROC score on test set 0.7368214609952413\n",
      "CV score 0.7073899464604403\n",
      "ROC score on test set 0.7368214609952413\n",
      "CV score 0.7073899464604403\n",
      "Fitting model for LABEL_Alkalinephos.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed: 15.5min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed: 18.5min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed: 22.1min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed: 22.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8032056  0.64130074 0.33422276 ... 0.59836924 0.4610252  0.46021613]\n",
      "ROC score on test set 0.7403043216253443\n",
      "CV score 0.7150990828758821\n",
      "ROC score on test set 0.7403043216253443\n",
      "CV score 0.7150990828758821\n",
      "Fitting model for LABEL_Bilirubin_total.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   21.6s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   51.4s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed: 14.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed: 17.0min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed: 19.7min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed: 22.6min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed: 23.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87397355 0.6922115  0.32102206 ... 0.5634515  0.4186712  0.5080465 ]\n",
      "ROC score on test set 0.7409208937198067\n",
      "CV score 0.7041963403010875\n",
      "ROC score on test set 0.7409208937198067\n",
      "CV score 0.7041963403010875\n",
      "Fitting model for LABEL_Lactate.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   19.3s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   55.3s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed: 17.1min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed: 20.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8033025  0.20566778 0.28735828 ... 0.62399405 0.2143497  0.6532622 ]\n",
      "ROC score on test set 0.7627646524669035\n",
      "CV score 0.750527370586176\n",
      "ROC score on test set 0.7627646524669035\n",
      "CV score 0.750527370586176\n",
      "Fitting model for LABEL_TroponinI.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   28.5s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:   48.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:  9.7min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12231846 0.75927013 0.74577814 ... 0.25730664 0.6032266  0.17911664]\n",
      "ROC score on test set 0.7227693581780539\n",
      "CV score 0.742208626284478\n",
      "ROC score on test set 0.7227693581780539\n",
      "CV score 0.742208626284478\n",
      "Fitting model for LABEL_SaO2.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   52.0s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed: 17.2min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed: 20.0min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed: 23.0min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed: 23.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74861014 0.28393912 0.42730027 ... 0.5937572  0.26378438 0.4900812 ]\n",
      "ROC score on test set 0.7633056578207291\n",
      "CV score 0.7569968114539521\n",
      "ROC score on test set 0.7633056578207291\n",
      "CV score 0.7569968114539521\n",
      "Fitting model for LABEL_Bilirubin_direct.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1528 tasks      | elapsed:   22.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2428 tasks      | elapsed:   38.0s\n",
      "[Parallel(n_jobs=-1)]: Done 3072 tasks      | elapsed:   48.3s\n",
      "[Parallel(n_jobs=-1)]: Done 4132 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5632 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 7332 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 9232 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:  2.7min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8458845  0.62316173 0.28457513 ... 0.4416283  0.3965993  0.6129491 ]\n",
      "ROC score on test set 0.7264207650273226\n",
      "CV score 0.7335745216034641\n",
      "ROC score on test set 0.7264207650273226\n",
      "CV score 0.7335745216034641\n",
      "Fitting model for LABEL_EtCO2.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting coarse model\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 248 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=-1)]: Done 498 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=-1)]: Done 848 tasks      | elapsed:   32.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1298 tasks      | elapsed:   48.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1848 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2498 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3248 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4098 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 5048 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6098 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 7248 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 8498 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 9848 tasks      | elapsed:  5.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62657565 0.3886174  0.32917774 ... 0.61635196 0.25950286 0.5672045 ]\n",
      "ROC score on test set 0.8425446428571428\n",
      "CV score 0.801199280844551\n",
      "ROC score on test set 0.8425446428571428\n",
      "CV score 0.801199280844551\n",
      "Finished test for medical tests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:  5.8min finished\n"
     ]
    }
   ],
   "source": [
    "# Modelling using extreme gradient boosting\n",
    "clf = xgb.XGBClassifier(objective=\"binary:logistic\", n_thread=-1)\n",
    "models = []\n",
    "losses = []\n",
    "feature_selector_medical_tests = []\n",
    "for i, test in enumerate(MEDICAL_TESTS):\n",
    "    print(f\"Fitting model for {test}.\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train_scaled, y_train_medical_tests[i], test_size=0.10, random_state=42, shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(\"Resampling\")\n",
    "    sampler = RandomUnderSampler(random_state=42)\n",
    "    X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(\"Applying feature selection\")\n",
    "    feature_selector = SelectKBest(score_func=f_classif, k=5)\n",
    "    X_train_selected = feature_selector.fit_transform(X_train_res, y_train_res)\n",
    "    X_test = feature_selector.transform(X_test)\n",
    "    feature_selector_medical_tests.append(feature_selector)\n",
    "    \n",
    "    print(\"Fitting coarse model\")\n",
    "    # Coarse parameter grid not optimized at all yet\n",
    "    coarse_param_grid = {\n",
    "        \"booster\": [\"dart\"],\n",
    "        \"eta\": np.arange(0,1,0.1),\n",
    "        \"min_child_weight\": range(1, 10, 1),\n",
    "        \"max_depth\": range(4, 10, 1),\n",
    "        \"gamma\": range(0, 100, 1),\n",
    "        \"max_delta_step\": range(1, 10, 1),\n",
    "        \"subsample\": np.arange(0.1, 1, 0.05),\n",
    "        \"colsample_bytree\": np.arange(0.3, 1, 0.05),\n",
    "        \"n_estimators\": range(50, 150, 1),\n",
    "        \"scale_pos_weight\": [1],\n",
    "        \"reg_lambda\": [0, 1], # Ridge regularization\n",
    "        \"reg_alpha\": [0, 1], # Lasso regularization\n",
    "        \"eval_metric\": [\"error\"],\n",
    "        \"verbosity\": [1]\n",
    "    }\n",
    "    coarse_search = RandomizedSearchCV(estimator=clf,\n",
    "            param_distributions=coarse_param_grid, scoring=\"roc_auc\",\n",
    "            n_jobs=-1, cv=10, n_iter=1000, verbose=1)\n",
    "    coarse_search.fit(X_train_selected, y_train_res)\n",
    "    print(coarse_search.best_estimator_.predict_proba(X_test)[:,1])\n",
    "    print(f\"ROC score on test set {roc_auc_score(y_test, coarse_search.best_estimator_.predict_proba(X_test)[:,1])}\")\n",
    "    print(f\"CV score {coarse_search.best_score_}\")\n",
    "    best_params = coarse_search.best_params_\n",
    "#     print(best_params)\n",
    "#     print(\"Fitting fine model\")\n",
    "    # Fine parameter grid not optimized at all yet\n",
    "#     fine_param_grid = {\n",
    "#         \"booster\": best_params[\"booster\"],\n",
    "#         \"eta\": np.arange(best_params[\"eta\"]-0.05, best_params[\"eta\"]+0.05, 0.003),\n",
    "#         \"min_child_weight\": [best_params[\"min_child_weight\"]],\n",
    "#         \"max_depth\": [best_params[\"max_depth\"]],\n",
    "#         \"gamma\": np.arange(best_params[\"gamma\"]-3, best_params[\"gamma\"]+3, 0.05),\n",
    "#         \"max_delta_step\": [best_params[\"max_delta_step\"]],\n",
    "#         \"subsample\": np.arange(best_params[\"subsample\"]-0.05, best_params[\"subsample\"]+0.05, 0.002),\n",
    "#         \"colsample_bytree\": [best_params[\"colsample_bytree\"]],\n",
    "#         \"n_estimators\": range(best_params[\"n_estimators\"]-10, best_params[\"n_estimators\"]+10, 1),\n",
    "#         \"scale_pos_weight\": [1],\n",
    "#         \"reg_lambda\": [best_params[\"reg_lambda\"]], # Ridge regularization\n",
    "#         \"reg_alpha\": [best_params[\"reg_alpha\"]], # Lasso regularization\n",
    "#         \"eval_metric\": [\"error\"],\n",
    "#         \"verbosity\": [2]\n",
    "#     }\n",
    "#     fine_search = RandomizedSearchCV(estimator=clf,\n",
    "#             param_distributions=fine_param_grid, scoring=\"roc_auc\",\n",
    "#             n_jobs=-1, cv=10, n_iter=10, verbose=1)\n",
    "#     fine_search.fit(X_train_selected, y_train_res)\n",
    "#     print(coarse_search.best_estimator_.predict_proba(X_test)[:,1])\n",
    "    print(f\"ROC score on test set {roc_auc_score(y_test, coarse_search.best_estimator_.predict_proba(X_test)[:,1])}\")\n",
    "    print(f\"CV score {coarse_search.best_score_}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    models.append(coarse_search.best_estimator_)\n",
    "    \n",
    "print(f\"Finished test for medical tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "for i, model in enumerate(models):\n",
    "    joblib.dump(models[i], f\"xgboost_fine_{MEDICAL_TESTS[i]}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pids = np.unique(df_val[\"pid\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for medical tests\n",
    "df_pred_medical_test = pd.DataFrame(index=val_pids, columns=MEDICAL_TESTS)\n",
    "for i, test in enumerate(MEDICAL_TESTS):\n",
    "    feature_selector = feature_selector_medical_tests[i]\n",
    "    X_val_vital_sign = feature_selector.transform(X_val_scaled)\n",
    "    model_for_test = models[i]\n",
    "#     print(model_for_test.predict_proba(X_val_vital_sign))\n",
    "    y_pred = model_for_test.predict_proba(X_val_vital_sign)[:, 1]\n",
    "    df_pred_medical_test[test] = y_pred\n",
    "\n",
    "df_pred_medical_test = df_pred_medical_test.reset_index().rename(columns={\"index\": \"pid\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>LABEL_BaseExcess</th>\n",
       "      <th>LABEL_Fibrinogen</th>\n",
       "      <th>LABEL_AST</th>\n",
       "      <th>LABEL_Alkalinephos</th>\n",
       "      <th>LABEL_Bilirubin_total</th>\n",
       "      <th>LABEL_Lactate</th>\n",
       "      <th>LABEL_TroponinI</th>\n",
       "      <th>LABEL_SaO2</th>\n",
       "      <th>LABEL_Bilirubin_direct</th>\n",
       "      <th>LABEL_EtCO2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.832393</td>\n",
       "      <td>0.429798</td>\n",
       "      <td>0.896758</td>\n",
       "      <td>0.961712</td>\n",
       "      <td>0.913878</td>\n",
       "      <td>0.849364</td>\n",
       "      <td>0.150986</td>\n",
       "      <td>0.603449</td>\n",
       "      <td>0.647068</td>\n",
       "      <td>0.434431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.131074</td>\n",
       "      <td>0.304217</td>\n",
       "      <td>0.463766</td>\n",
       "      <td>0.480553</td>\n",
       "      <td>0.439553</td>\n",
       "      <td>0.238957</td>\n",
       "      <td>0.698675</td>\n",
       "      <td>0.274292</td>\n",
       "      <td>0.446310</td>\n",
       "      <td>0.232469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.417147</td>\n",
       "      <td>0.304217</td>\n",
       "      <td>0.443913</td>\n",
       "      <td>0.433482</td>\n",
       "      <td>0.425904</td>\n",
       "      <td>0.277688</td>\n",
       "      <td>0.473744</td>\n",
       "      <td>0.404472</td>\n",
       "      <td>0.446310</td>\n",
       "      <td>0.218275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.959387</td>\n",
       "      <td>0.911051</td>\n",
       "      <td>0.944069</td>\n",
       "      <td>0.961004</td>\n",
       "      <td>0.867512</td>\n",
       "      <td>0.672507</td>\n",
       "      <td>0.352497</td>\n",
       "      <td>0.647528</td>\n",
       "      <td>0.911838</td>\n",
       "      <td>0.269971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.384726</td>\n",
       "      <td>0.298570</td>\n",
       "      <td>0.566566</td>\n",
       "      <td>0.562898</td>\n",
       "      <td>0.542512</td>\n",
       "      <td>0.396905</td>\n",
       "      <td>0.221129</td>\n",
       "      <td>0.206536</td>\n",
       "      <td>0.570115</td>\n",
       "      <td>0.324562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12659</th>\n",
       "      <td>31647.0</td>\n",
       "      <td>0.338738</td>\n",
       "      <td>0.304217</td>\n",
       "      <td>0.406793</td>\n",
       "      <td>0.373973</td>\n",
       "      <td>0.411888</td>\n",
       "      <td>0.300359</td>\n",
       "      <td>0.094801</td>\n",
       "      <td>0.290623</td>\n",
       "      <td>0.446310</td>\n",
       "      <td>0.232469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12660</th>\n",
       "      <td>31649.0</td>\n",
       "      <td>0.175901</td>\n",
       "      <td>0.608352</td>\n",
       "      <td>0.760341</td>\n",
       "      <td>0.811736</td>\n",
       "      <td>0.611820</td>\n",
       "      <td>0.340112</td>\n",
       "      <td>0.677199</td>\n",
       "      <td>0.271969</td>\n",
       "      <td>0.739103</td>\n",
       "      <td>0.281824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12661</th>\n",
       "      <td>31651.0</td>\n",
       "      <td>0.929130</td>\n",
       "      <td>0.325646</td>\n",
       "      <td>0.474042</td>\n",
       "      <td>0.461187</td>\n",
       "      <td>0.443320</td>\n",
       "      <td>0.697765</td>\n",
       "      <td>0.273953</td>\n",
       "      <td>0.724276</td>\n",
       "      <td>0.364428</td>\n",
       "      <td>0.437264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12662</th>\n",
       "      <td>31652.0</td>\n",
       "      <td>0.082532</td>\n",
       "      <td>0.317979</td>\n",
       "      <td>0.525401</td>\n",
       "      <td>0.593110</td>\n",
       "      <td>0.526376</td>\n",
       "      <td>0.207084</td>\n",
       "      <td>0.635182</td>\n",
       "      <td>0.299705</td>\n",
       "      <td>0.243108</td>\n",
       "      <td>0.341546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12663</th>\n",
       "      <td>31655.0</td>\n",
       "      <td>0.117321</td>\n",
       "      <td>0.323369</td>\n",
       "      <td>0.573651</td>\n",
       "      <td>0.594063</td>\n",
       "      <td>0.588049</td>\n",
       "      <td>0.225370</td>\n",
       "      <td>0.345621</td>\n",
       "      <td>0.274292</td>\n",
       "      <td>0.446310</td>\n",
       "      <td>0.413635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12664 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           pid  LABEL_BaseExcess  LABEL_Fibrinogen  LABEL_AST  \\\n",
       "0          0.0          0.832393          0.429798   0.896758   \n",
       "1          3.0          0.131074          0.304217   0.463766   \n",
       "2          5.0          0.417147          0.304217   0.443913   \n",
       "3          7.0          0.959387          0.911051   0.944069   \n",
       "4          9.0          0.384726          0.298570   0.566566   \n",
       "...        ...               ...               ...        ...   \n",
       "12659  31647.0          0.338738          0.304217   0.406793   \n",
       "12660  31649.0          0.175901          0.608352   0.760341   \n",
       "12661  31651.0          0.929130          0.325646   0.474042   \n",
       "12662  31652.0          0.082532          0.317979   0.525401   \n",
       "12663  31655.0          0.117321          0.323369   0.573651   \n",
       "\n",
       "       LABEL_Alkalinephos  LABEL_Bilirubin_total  LABEL_Lactate  \\\n",
       "0                0.961712               0.913878       0.849364   \n",
       "1                0.480553               0.439553       0.238957   \n",
       "2                0.433482               0.425904       0.277688   \n",
       "3                0.961004               0.867512       0.672507   \n",
       "4                0.562898               0.542512       0.396905   \n",
       "...                   ...                    ...            ...   \n",
       "12659            0.373973               0.411888       0.300359   \n",
       "12660            0.811736               0.611820       0.340112   \n",
       "12661            0.461187               0.443320       0.697765   \n",
       "12662            0.593110               0.526376       0.207084   \n",
       "12663            0.594063               0.588049       0.225370   \n",
       "\n",
       "       LABEL_TroponinI  LABEL_SaO2  LABEL_Bilirubin_direct  LABEL_EtCO2  \n",
       "0             0.150986    0.603449                0.647068     0.434431  \n",
       "1             0.698675    0.274292                0.446310     0.232469  \n",
       "2             0.473744    0.404472                0.446310     0.218275  \n",
       "3             0.352497    0.647528                0.911838     0.269971  \n",
       "4             0.221129    0.206536                0.570115     0.324562  \n",
       "...                ...         ...                     ...          ...  \n",
       "12659         0.094801    0.290623                0.446310     0.232469  \n",
       "12660         0.677199    0.271969                0.739103     0.281824  \n",
       "12661         0.273953    0.724276                0.364428     0.437264  \n",
       "12662         0.635182    0.299705                0.243108     0.341546  \n",
       "12663         0.345621    0.274292                0.446310     0.413635  \n",
       "\n",
       "[12664 rows x 11 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred_medical_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling sepsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting model\n",
      "[0 0 0 ... 1 1 1]\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   35.7s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed: 10.0min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed: 11.7min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed: 11.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC score on test set 0.7539753938168123\n",
      "CV score 0.7092163131313131\n",
      "Finished test for medical tests.\n"
     ]
    }
   ],
   "source": [
    "# Model and predict sepsis\n",
    "\n",
    "clf = xgb.XGBClassifier(objective=\"binary:logistic\", n_thread=-1)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train_scaled, y_train_sepsis[0], test_size=0.10, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "        \"booster\": [\"dart\"],\n",
    "        \"eta\": np.arange(0,1,0.1),\n",
    "        \"min_child_weight\": range(1, 10, 1),\n",
    "        \"max_depth\": range(4, 10, 1),\n",
    "        \"gamma\": range(0, 100, 1),\n",
    "        \"max_delta_step\": range(1, 10, 1),\n",
    "        \"subsample\": np.arange(0.1, 1, 0.05),\n",
    "        \"colsample_bytree\": np.arange(0.3, 1, 0.05),\n",
    "        \"n_estimators\": range(50, 150, 1),\n",
    "        \"scale_pos_weight\": [1],\n",
    "        \"reg_lambda\": [0, 1], # Ridge regularization\n",
    "        \"reg_alpha\": [0, 1], # Lasso regularization\n",
    "        \"eval_metric\": [\"error\"],\n",
    "        \"verbosity\": [1]\n",
    "    }\n",
    "\n",
    "print(\"Resampling\")\n",
    "sampler = RandomUnderSampler()\n",
    "X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Applying feature selection\")\n",
    "feature_selector = SelectKBest(score_func=f_classif, k=30)\n",
    "X_train = feature_selector.fit_transform(X_train_res, y_train_res)\n",
    "X_test = feature_selector.transform(X_test)\n",
    "\n",
    "\n",
    "print(\"Fitting model\")\n",
    "coarse_search = RandomizedSearchCV(estimator=clf,\n",
    "        param_distributions=param_grid, scoring=\"roc_auc\",\n",
    "        n_jobs=-1, cv=10, n_iter=1000, verbose=1)\n",
    "print(y_train_res)\n",
    "coarse_search.fit(X_train, y_train_res)\n",
    "\n",
    "sepsis_model = coarse_search.best_estimator_\n",
    "print(f\"ROC score on test set {roc_auc_score(y_test, coarse_search.best_estimator_.predict_proba(X_test)[:,1])}\")\n",
    "print(f\"CV score {coarse_search.best_score_}\")\n",
    "print(f\"Finished test for medical tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgboost_fine_sepsis.pkl']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(sepsis_model, f\"xgboost_fine_sepsis.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_sepsis = feature_selector.transform(X_val_scaled)\n",
    "y_pred = sepsis_model.predict_proba(X_val_sepsis)[:,1]\n",
    "df_pred_sepsis = pd.DataFrame(y_pred, index=val_pids, columns=SEPSIS)\n",
    "df_pred_sepsis = df_pred_sepsis.reset_index().rename(columns={\"index\": \"pid\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling vital signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model for LABEL_RRate.\n",
      "Applying feature selection\n",
      "Fitting model\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   18.6s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  8.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score 0.3762834615674595\n",
      "Test score is 0.3651876778962392\n",
      "Finished test for medical tests.\n",
      "Fitting model for LABEL_ABPm.\n",
      "Applying feature selection\n",
      "Fitting model\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  6.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score 0.5772479021545291\n",
      "Test score is 0.5894346373011152\n",
      "Finished test for medical tests.\n",
      "Fitting model for LABEL_SpO2.\n",
      "Applying feature selection\n",
      "Fitting model\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   49.5s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  4.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score 0.30896617018979444\n",
      "Test score is 0.3392388962178333\n",
      "Finished test for medical tests.\n",
      "Fitting model for LABEL_Heartrate.\n",
      "Applying feature selection\n",
      "Fitting model\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   18.3s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  8.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score 0.5974856981684256\n",
      "Test score is 0.6043524041430639\n",
      "Finished test for medical tests.\n"
     ]
    }
   ],
   "source": [
    "# Modelling of vital signs\n",
    "models = []\n",
    "losses = []\n",
    "feature_selectors_vital_signs = []\n",
    "clf = xgb.XGBRegressor(objective=\"reg:squarederror\", n_thread=-1)\n",
    "\n",
    "for i, sign in enumerate(VITAL_SIGNS):\n",
    "    print(f\"Fitting model for {sign}.\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_train_scaled, y_train_vital_signs[i], test_size=0.10, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    print(\"Applying feature selection\")\n",
    "    feature_selector = SelectKBest(score_func=f_classif, k=5)\n",
    "    X_train_selected = feature_selector.fit_transform(X_train, y_train)\n",
    "    X_test_selected = feature_selector.transform(X_test)\n",
    "    feature_selectors_vital_signs.append(feature_selector)\n",
    "\n",
    "    print(\"Fitting model\")\n",
    "    \n",
    "    param_grid = {\n",
    "        \"booster\": [\"dart\"],\n",
    "        \"eta\": np.arange(0,1,0.1),\n",
    "        \"min_child_weight\": range(1, 10, 1),\n",
    "        \"max_depth\": range(4, 10, 1),\n",
    "        \"gamma\": range(0, 100, 1),\n",
    "        \"max_delta_step\": range(1, 10, 1),\n",
    "        \"subsample\": np.arange(0.1, 1, 0.05),\n",
    "        \"colsample_bytree\": np.arange(0.3, 1, 0.05),\n",
    "        \"n_estimators\": range(50, 150, 1),\n",
    "        \"scale_pos_weight\": [1],\n",
    "        \"reg_lambda\": [0, 1], # Ridge regularization\n",
    "        \"reg_alpha\": [0, 1], # Lasso regularization\n",
    "        \"eval_metric\": [\"error\"],\n",
    "        \"verbosity\": [1]\n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "    coarse_search = RandomizedSearchCV(estimator=clf,\n",
    "            param_distributions=param_grid, scoring=\"r2\",\n",
    "            n_jobs=-1, cv=10, n_iter=100, verbose=1)\n",
    "    coarse_search.fit(X_train_selected, y_train)\n",
    "    models.append(coarse_search.best_estimator_)\n",
    "    print(f\"CV score {coarse_search.best_score_}\")\n",
    "    print(f\"Test score is {r2_score(y_test, coarse_search.best_estimator_.predict(X_test_selected))}\")\n",
    "    print(f\"Finished test for medical tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(models):\n",
    "    joblib.dump(models[i], f\"xgboost_fine_{VITAL_SIGNS[i]}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for medical tests\n",
    "df_pred_vital_signs = pd.DataFrame(index=val_pids, columns=VITAL_SIGNS)\n",
    "for i, test in enumerate(VITAL_SIGNS):\n",
    "    feature_selector = feature_selectors_vital_signs[i]\n",
    "    X_val_vital_sign = feature_selector.transform(X_val_scaled)\n",
    "    model_for_test = models[i]\n",
    "    y_pred = model_for_test.predict(X_val_vital_sign)\n",
    "    df_pred_vital_signs[test] = y_pred\n",
    "\n",
    "df_pred_vital_signs = df_pred_vital_signs.reset_index().rename(columns={\"index\": \"pid\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to ZIP file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export predictions DataFrame to a zip file\n",
      "           pid  LABEL_BaseExcess  LABEL_Fibrinogen  LABEL_AST  \\\n",
      "0          0.0          0.832393          0.429798   0.896758   \n",
      "1          3.0          0.131074          0.304217   0.463766   \n",
      "2          5.0          0.417147          0.304217   0.443913   \n",
      "3          7.0          0.959387          0.911051   0.944069   \n",
      "4          9.0          0.384726          0.298570   0.566566   \n",
      "...        ...               ...               ...        ...   \n",
      "12659  31647.0          0.338738          0.304217   0.406793   \n",
      "12660  31649.0          0.175901          0.608352   0.760341   \n",
      "12661  31651.0          0.929130          0.325646   0.474042   \n",
      "12662  31652.0          0.082532          0.317979   0.525401   \n",
      "12663  31655.0          0.117321          0.323369   0.573651   \n",
      "\n",
      "       LABEL_Alkalinephos  LABEL_Bilirubin_total  LABEL_Lactate  \\\n",
      "0                0.961712               0.913878       0.849364   \n",
      "1                0.480553               0.439553       0.238957   \n",
      "2                0.433482               0.425904       0.277688   \n",
      "3                0.961004               0.867512       0.672507   \n",
      "4                0.562898               0.542512       0.396905   \n",
      "...                   ...                    ...            ...   \n",
      "12659            0.373973               0.411888       0.300359   \n",
      "12660            0.811736               0.611820       0.340112   \n",
      "12661            0.461187               0.443320       0.697765   \n",
      "12662            0.593110               0.526376       0.207084   \n",
      "12663            0.594063               0.588049       0.225370   \n",
      "\n",
      "       LABEL_TroponinI  LABEL_SaO2  LABEL_Bilirubin_direct  LABEL_EtCO2  \\\n",
      "0             0.150986    0.603449                0.647068     0.434431   \n",
      "1             0.698675    0.274292                0.446310     0.232469   \n",
      "2             0.473744    0.404472                0.446310     0.218275   \n",
      "3             0.352497    0.647528                0.911838     0.269971   \n",
      "4             0.221129    0.206536                0.570115     0.324562   \n",
      "...                ...         ...                     ...          ...   \n",
      "12659         0.094801    0.290623                0.446310     0.232469   \n",
      "12660         0.677199    0.271969                0.739103     0.281824   \n",
      "12661         0.273953    0.724276                0.364428     0.437264   \n",
      "12662         0.635182    0.299705                0.243108     0.341546   \n",
      "12663         0.345621    0.274292                0.446310     0.413635   \n",
      "\n",
      "       LABEL_Sepsis  LABEL_RRate  LABEL_ABPm  LABEL_SpO2  LABEL_Heartrate  \n",
      "0          0.752551    16.098423   85.067841   97.806915        84.746559  \n",
      "1          0.332614    17.577841   84.925156   96.260529        89.948601  \n",
      "2          0.296633    18.384174   72.514221   95.604843        67.635735  \n",
      "3          0.764247    16.582848   88.701813   97.345406        94.783012  \n",
      "4          0.645422    20.150539   88.608429   95.481430        91.113228  \n",
      "...             ...          ...         ...         ...              ...  \n",
      "12659      0.292379    16.750763   69.930702   96.260529        73.083839  \n",
      "12660      0.419551    15.429890   81.475014   96.065491        91.500526  \n",
      "12661      0.522059    17.051477   77.414116   97.674973        84.375969  \n",
      "12662      0.382621    18.422857   96.369110   96.906197       108.435699  \n",
      "12663      0.343168    17.119041   83.600609   97.900909       105.709663  \n",
      "\n",
      "[12664 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "df_predictions = pd.merge(df_pred_medical_test, df_pred_sepsis, on=\"pid\")\n",
    "df_predictions = pd.merge(df_predictions, df_pred_vital_signs, on=\"pid\")\n",
    "print(\"Export predictions DataFrame to a zip file\")\n",
    "print(df_predictions)\n",
    "df_predictions.to_csv(\n",
    "    \"predictions.csv\",\n",
    "    index=None,\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    encoding=\"utf-8-sig\",\n",
    "    float_format=\"%.2f\",\n",
    ")\n",
    "\n",
    "with zipfile.ZipFile(\"predictions.zip\", \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    zf.write(\"predictions.csv\")\n",
    "os.remove(\"predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
