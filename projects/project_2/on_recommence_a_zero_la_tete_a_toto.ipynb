{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the solution to project 2 from scratch\n",
    "To note:\n",
    "- For clarity the df_test was renamed to df_val as the test word was used when splitting the labeled data into train and test. \n",
    "    - Val stands for validation\n",
    "\n",
    "To try out:\n",
    "- Preprocessing\n",
    "    - Tweak data imputer\n",
    "    - Tweak scaler (Robust scaler, minmax, etc..)\n",
    "    - Tweak feature selection parameter\n",
    "    - Tweak order of operations above to see the effect\n",
    "- Modelling\n",
    "    - XGBoost\n",
    "    - SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import zipfile\n",
    "import time\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import ADASYN, SMOTE\n",
    "from imblearn.under_sampling import ClusterCentroids, RandomUnderSampler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, chi2, f_classif\n",
    "from sklearn.metrics import f1_score, mean_squared_error, accuracy_score, r2_score, roc_auc_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import label_ranking_average_precision_score as LRAPS\n",
    "from sklearn.metrics import label_ranking_loss as LRL\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Global variables\n",
    "IDENTIFIERS = [\"pid\", \"Time\"]\n",
    "MEDICAL_TESTS = [\n",
    "    \"LABEL_BaseExcess\",\n",
    "    \"LABEL_Fibrinogen\",\n",
    "    \"LABEL_AST\",\n",
    "    \"LABEL_Alkalinephos\",\n",
    "    \"LABEL_Bilirubin_total\",\n",
    "    \"LABEL_Lactate\",\n",
    "    \"LABEL_TroponinI\",\n",
    "    \"LABEL_SaO2\",\n",
    "    \"LABEL_Bilirubin_direct\",\n",
    "    \"LABEL_EtCO2\",\n",
    "]\n",
    "VITAL_SIGNS = [\"LABEL_RRate\", \"LABEL_ABPm\", \"LABEL_SpO2\", \"LABEL_Heartrate\"]\n",
    "SEPSIS = [\"LABEL_Sepsis\"]\n",
    "ESTIMATOR = {\"bayesian\": BayesianRidge(), \"decisiontree\": DecisionTreeRegressor(max_features=\"sqrt\", random_state=0), \n",
    "                \"extratree\": ExtraTreesRegressor(n_estimators=10, random_state=0), \n",
    "                \"knn\": KNeighborsRegressor(n_neighbors=10, weights=\"distance\")}\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_f(x):\n",
    "    \"\"\"To get predictions as confidence level, the model predicts for all 12 sets of measures for\n",
    "    each patient a distance to the hyperplane ; it is then transformed into a confidence level using\n",
    "    the sigmoid function ; the confidence level reported is the mean of all confidence levels for a\n",
    "    single patient\n",
    "\n",
    "    Args:\n",
    "        x (float): input of the sigmoid function\n",
    "\n",
    "    Returns:\n",
    "       float: result of the sigmoid computation.\n",
    "\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r\"data/train_features.csv\")\n",
    "df_train_label = pd.read_csv(r\"data/train_labels.csv\")\n",
    "df_val = pd.read_csv(r\"data/test_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data imputation methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18995/18995 [10:14<00:00, 30.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# Fit imputer to missing data\n",
    "pid_train = df_train[\"pid\"].unique()\n",
    "columns = df_train.columns\n",
    "df_train_preprocessed = pd.DataFrame(columns=columns, index=pid_train)\n",
    "\n",
    "imputer = SimpleImputer()\n",
    "columns = df_train.columns\n",
    "df_train = imputer.fit_transform(df_train.values)\n",
    "df_train = pd.DataFrame(df_train, columns=columns)\n",
    "for patient in tqdm(pid_train):\n",
    "    for column in df_train.columns:\n",
    "        df_train_preprocessed.at[patient, column] = df_train.loc[\n",
    "            df_train[\"pid\"] == patient\n",
    "        ][column].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12664/12664 [06:42<00:00, 31.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tranform test data according to same imputer\n",
    "pid_val = df_val[\"pid\"].unique()\n",
    "columns = df_val.columns\n",
    "df_val_preprocessed = pd.DataFrame(columns=columns, index=pid_val)\n",
    "\n",
    "columns = df_val.columns\n",
    "df_val = imputer.transform(df_val.values)\n",
    "df_val = pd.DataFrame(df_val, columns=columns)\n",
    "for patient in tqdm(pid_val):\n",
    "    for column in df_val.columns:\n",
    "        df_val_preprocessed.at[patient, column] = df_val.loc[\n",
    "            df_val[\"pid\"] == patient\n",
    "        ][column].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_preprocessed.to_csv(\"df_train_philip.csv\")\n",
    "df_val_preprocessed.to_csv(\"df_val_philip.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_preprocessed = pd.read_csv(\"df_train_philip.csv\")\n",
    "df_val_preprocessed = pd.read_csv(\"df_val_philip.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a list of labels for each medical test\n",
      "Creating a list of labels for each medical test\n",
      "Creating a list of labels for each vital sign\n"
     ]
    }
   ],
   "source": [
    "# Data formatting\n",
    "X_train = df_train_preprocessed.drop(columns=IDENTIFIERS).values\n",
    "X_val = df_val_preprocessed.drop(columns=IDENTIFIERS).values\n",
    "# Create list with different label for each medical test\n",
    "print(\"Creating a list of labels for each medical test\")\n",
    "y_train_medical_tests = []\n",
    "for test in MEDICAL_TESTS:\n",
    "    y_train_medical_tests.append(df_train_label[test].astype(int).values)\n",
    "\n",
    "# Create list with different label for sepsis\n",
    "print(\"Creating a list of labels for each medical test\")\n",
    "y_train_sepsis = []\n",
    "for sepsis in SEPSIS:\n",
    "    y_train_sepsis.append(df_train_label[sepsis].astype(int).values)\n",
    "\n",
    "# Create list with different label for each vital sign\n",
    "print(\"Creating a list of labels for each vital sign\")\n",
    "y_train_vital_signs = []\n",
    "for sign in VITAL_SIGNS:\n",
    "    y_train_vital_signs.append(df_train_label[sign].astype(int).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data \n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling medical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Modelling of medical tests using logistic regression with cross validation\n",
    "# models = []\n",
    "# losses = []\n",
    "# columns_medical_tests = []\n",
    "# for i, test in enumerate(MEDICAL_TESTS):\n",
    "#     print(f\"Fitting model for {test}.\")\n",
    "\n",
    "#     print(\"Applying feature selection\")\n",
    "#     feature_selector = SelectKBest(score_func=f_classif, k=3)\n",
    "#     X_train = feature_selector.fit_transform(X_train, y_train_medical_tests[i])\n",
    "#     X_test = feature_selector.transform(X_test)\n",
    "#     columns = feature_selector.get_support(indices=True)\n",
    "#     columns_medical_tests.append(columns)\n",
    "\n",
    "#     print(\"Fitting model\")\n",
    "#     clf = LogisticRegressionCV(cv=5, random_state=42).fit(X_train, y_train_medical_tests[i])\n",
    "#     models.append(clf)\n",
    "#     print(roc_auc_score(y_test, clf.predict_proba(X_test)[:,1]))\n",
    "#     print(f\"Finished test for medical tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model for LABEL_BaseExcess.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting model\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   26.3s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   33.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8566016748025775\n",
      "0.860127562844335\n",
      "Fitting model for LABEL_Fibrinogen.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting model\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   10.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7532895831407709\n",
      "0.7330510798212931\n",
      "Fitting model for LABEL_AST.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting model\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   26.7s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   35.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7181450660343298\n",
      "0.6981686505137089\n",
      "Fitting model for LABEL_Alkalinephos.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting model\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   24.6s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   27.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7239535700658354\n",
      "0.7063167377553694\n",
      "Fitting model for LABEL_Bilirubin_total.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting model\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   20.6s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   23.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7306865846496845\n",
      "0.7154203867818565\n",
      "Fitting model for LABEL_Lactate.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting model\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   20.1s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   25.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7662656725149067\n",
      "0.7545122935767401\n",
      "Fitting model for LABEL_TroponinI.\n",
      "Resampling\n",
      "Applying feature selection\n",
      "Fitting model\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    8.1s\n"
     ]
    }
   ],
   "source": [
    "# Modelling using extreme gradient boosting\n",
    "clf = xgb.XGBClassifier(objective=\"binary:logistic\", n_thread=-1)\n",
    "models = []\n",
    "losses = []\n",
    "columns_medical_tests = []\n",
    "X_train_orig = X_train\n",
    "for i, test in enumerate(MEDICAL_TESTS):\n",
    "    print(f\"Fitting model for {test}.\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train_orig, y_train_medical_tests[i], test_size=0.10, random_state=42, shuffle=True\n",
    "    )\n",
    "    # Coarse parameter grid not optimized at all yet\n",
    "    param_grid = {\n",
    "        \"booster\": [\"dart\", \"gbtree\", \"gblinear\"],\n",
    "        \"eta\": np.arange(0.01, 0.4, 0.015),\n",
    "        \"min_child_weight\": range(1, 10, 1),\n",
    "        \"max_depth\": range(3, 8, 1),\n",
    "        \"gamma\": range(0, 100, 2),\n",
    "        \"max_delta_step\": range(1, 10, 1),\n",
    "        \"subsample\": np.arange(0.1, 1, 0.05),\n",
    "        \"colsample_bytree\": np.arange(0.3, 1, 0.05),\n",
    "        \"n_estimators\": range(50, 120, 2),\n",
    "        \"scale_pos_weight\": [1],\n",
    "        \"reg_lambda\": [0], # Ridge regularization\n",
    "        \"reg_alpha\": [1], # Lasso regularization\n",
    "        \"eval_metric\": [\"error\"],\n",
    "        \"verbosity\": [1]\n",
    "    }\n",
    "    \n",
    "    print(\"Resampling\")\n",
    "    sampler = RandomUnderSampler(random_state=42)\n",
    "    X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(\"Applying feature selection\")\n",
    "    feature_selector = SelectKBest(score_func=f_classif, k=5)\n",
    "    X_train_selected = feature_selector.fit_transform(X_train_res, y_train_res)\n",
    "    X_test = feature_selector.transform(X_test)\n",
    "    columns = feature_selector.get_support(indices=True)\n",
    "    columns_medical_tests.append(columns)\n",
    "\n",
    "    print(\"Fitting model\")\n",
    "    coarse_search = RandomizedSearchCV(estimator=clf,\n",
    "            param_distributions=param_grid, scoring=\"roc_auc\",\n",
    "            n_jobs=-1, cv=5, n_iter=100, verbose=1)\n",
    "    coarse_search.fit(X_train_selected, y_train_res)\n",
    "    \n",
    "    models.append(coarse_search.best_estimator_)\n",
    "    print(roc_auc_score(y_test, coarse_search.best_estimator_.predict_proba(X_test)[:,1]))\n",
    "    print(coarse_search.best_score_)\n",
    "print(f\"Finished test for medical tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for medical tests\n",
    "val_pids = np.unique(df_val[\"pid\"].values)\n",
    "df_pred_medical_test = pd.DataFrame(index=val_pids, columns=MEDICAL_TESTS)\n",
    "X_val = scaler.transform(X_val_scaled)\n",
    "for i, test in enumerate(MEDICAL_TESTS):\n",
    "    col_for_medical_test = columns_medical_tests[i]\n",
    "    X_val_vital_sign = X_val[:, col_for_medical_test]\n",
    "    model_for_test = models[i]\n",
    "    y_pred = model_for_test.predict_proba(X_val_vital_sign)[:, 1]\n",
    "    df_pred_medical_test[test] = y_pred\n",
    "\n",
    "df_pred_medical_test = df_pred_medical_test.reset_index().rename(columns={\"index\": \"pid\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling sepsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and predict sepsis\n",
    "print(\"Resampling data\")\n",
    "sampler = ADASYN()\n",
    "X_train, y_train = sampler.fit_sample(X_train_scaled, y_train_sepsis[0])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train, y_train, test_size=0.10, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Applying feature selection\")\n",
    "feature_selector = SelectKBest(score_func=f_classif, k=3)\n",
    "X_train = feature_selector.fit_transform(X_train, y_train)\n",
    "X_test = feature_selector.transform(X_test)\n",
    "columns_sepsis = feature_selector.get_support(indices=True)\n",
    "\n",
    "models, scores = [], []\n",
    "for i, C in enumerate(np.linspace(0.00001, 0.0005, num=5)):\n",
    "    print(\"Starting SVC for C={}\".format(C))\n",
    "    models.append(\n",
    "        SVC(C=C, kernel=\"poly\", class_weight=\"balanced\").fit(X_train, y_train)\n",
    "    )\n",
    "    y_pred = models[i].decision_function(X_test)\n",
    "    y_pred = [sigmoid_f(y_pred[i]) for i in range(len(y_pred))]\n",
    "    scores.append(roc_auc_score(y_test, y_pred))\n",
    "    recall = recall_score(y_test, np.around(y_pred))\n",
    "    precision = precision_score(y_test, np.around(y_test))\n",
    "    print(\n",
    "        \"The ROC AUC score is {}, the precision is {} and the recall is \"\n",
    "        \"{} for iteration {}\".format(scores[i], precision, recall, i)\n",
    "    )\n",
    "best = np.argmax(scores)\n",
    "model, score = models[best], scores[best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val_scaled[:, columns_sepsis]\n",
    "y_pred = model.decision_function(X_val)\n",
    "y_pred = [sigmoid_f(y_pred[i]) for i in range(len(y_pred))]\n",
    "# y_pred = model.predict_proba(X_test)[:,1]\n",
    "df_pred_sepsis = pd.DataFrame(y_pred, index=test_pids, columns=SEPSIS)\n",
    "df_pred_sepsis = df_pred_sepsis.reset_index().rename(columns={\"index\": \"pid\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling vital signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling of vital signs\n",
    "models = []\n",
    "losses = []\n",
    "columns_vital_signs = []\n",
    "for i, sign in enumerate(VITAL_SIGNS):\n",
    "    print(f\"Fitting model for {sign}.\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_train_scaled, y_train_vital_signs[i], test_size=0.10, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    print(\"Applying feature selection\")\n",
    "    feature_selector = SelectKBest(score_func=f_classif, k=3)\n",
    "    X_train = feature_selector.fit_transform(X_train, y_train)\n",
    "    X_test = feature_selector.transform(X_test)\n",
    "    columns = feature_selector.get_support(indices=True)\n",
    "    columns_vital_signs.append(columns)\n",
    "\n",
    "    print(\"Fitting model\")\n",
    "    model = LinearRegression().fit(X_train, y_train)\n",
    "    models.append(model)\n",
    "    print(r2_score(y_test, model.predict(X_test)))\n",
    "    print(f\"Finished test for medical tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for medical tests\n",
    "val_pids = np.unique(df_val[\"pid\"].values)\n",
    "df_pred_vital_signs = pd.DataFrame(index=val_pids, columns=VITAL_SIGNS)\n",
    "for i, test in enumerate(VITAL_SIGNS):\n",
    "    col_for_vital_sign = columns_vital_signs[i]\n",
    "    X_val_vital_sign = X_val_scaled[:, col_for_vital_sign]\n",
    "    model_for_test = models[i]\n",
    "    y_pred = model_for_test.predict(X_val_vital_sign)\n",
    "    df_pred_vital_signs[test] = y_pred\n",
    "\n",
    "df_pred_vital_signs = df_pred_vital_signs.reset_index().rename(columns={\"index\": \"pid\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to ZIP file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = pd.merge(df_pred_medical_test, df_pred_sepsis, on=\"pid\")\n",
    "df_predictions = pd.merge(df_predictions, df_pred_vital_signs, on=\"pid\")\n",
    "print(\"Export predictions DataFrame to a zip file\")\n",
    "print(df_predictions)\n",
    "df_predictions.to_csv(\n",
    "    \"predictions.csv\",\n",
    "    index=None,\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    encoding=\"utf-8-sig\",\n",
    "    float_format=\"%.2f\",\n",
    ")\n",
    "\n",
    "with zipfile.ZipFile(\"predictions.zip\", \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    zf.write(\"predictions.csv\")\n",
    "os.remove(\"predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
