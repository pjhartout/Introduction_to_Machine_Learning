{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras.applications\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "import keras.layers as kl\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 \n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(keras.__version__)\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_G_WIDTH = 100\n",
    "T_G_HEIGHT = 100\n",
    "T_G_NUMCHANNELS = 3\n",
    "CHUNKSIZE = 16\n",
    "BATCHSIZE = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_triplets = pd.read_csv(\"data/train_triplets.txt\", names=[\"A\", \"B\", \"C\"], sep=\" \")\n",
    "test_triplets = pd.read_csv(\"data/test_triplets.txt\", names=[\"A\", \"B\", \"C\"], sep=\" \")\n",
    "\n",
    "for column in [\"A\", \"B\", \"C\"]:\n",
    "    train_triplets[column] = train_triplets[column].astype(str)\n",
    "    test_triplets[column] = test_triplets[column].astype(str)\n",
    "    train_triplets[column] = train_triplets[column].apply(lambda x: x.zfill(5))\n",
    "    test_triplets[column] = test_triplets[column].apply(lambda x: x.zfill(5))\n",
    "train_triplets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split in test and training set, we take 0.3 of the dataframe and use it for testing and the rest for training\n",
    "train_triplets = train_triplets.sample(frac=1)\n",
    "n_test = 500\n",
    "test_images = train_triplets[:n_test]\n",
    "train_images = train_triplets[n_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def createCNNModel(emb_size):\n",
    "\n",
    "#     # Initialize a ResNet50_ImageNet Model\n",
    "# #     resnet_input = kl.Input(shape=(T_G_WIDTH,T_G_HEIGHT,T_G_NUMCHANNELS))\n",
    "#     model = tf.keras.Sequential([\n",
    "#         tf.keras.layers.Conv2D(filters=10, kernel_size=2, padding='same', activation='relu', input_shape=(T_G_HEIGHT,T_G_WIDTH,3)),\n",
    "#         tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "#         tf.keras.layers.Dropout(0.3),\n",
    "#         tf.keras.layers.Flatten(),\n",
    "#         tf.keras.layers.Dense(emb_size, activation=None), # No activation on final dense layer\n",
    "#         tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)) # L2 normalize embeddings\n",
    "#     ])\n",
    "\n",
    "#     print(type(model))\n",
    "#     # triplet framework, shared weights\n",
    "#     input_shape=(T_G_WIDTH,T_G_HEIGHT,T_G_NUMCHANNELS)\n",
    "#     input_anchor = kl.Input(shape=input_shape, name='input_anchor')\n",
    "#     input_positive = kl.Input(shape=input_shape, name='input_pos')\n",
    "#     input_negative = kl.Input(shape=input_shape, name='input_neg')\n",
    "#     print(type(model))\n",
    "#     print(type(input_positive))\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "#     loss=tfa.losses.TripletSemiHardLoss())\n",
    "#     net_anchor = model(input_anchor)\n",
    "#     net_positive = model(input_positive)\n",
    "#     net_negative = model(input_negative)\n",
    "\n",
    "#     # The Lamda layer produces output using given function. Here its Euclidean distance.\n",
    "#     positive_dist = kl.Lambda(euclidean_distance, name='pos_dist')([net_anchor, net_positive])\n",
    "#     negative_dist = kl.Lambda(euclidean_distance, name='neg_dist')([net_anchor, net_negative])\n",
    "#     tertiary_dist = kl.Lambda(euclidean_distance, name='ter_dist')([net_positive, net_negative])\n",
    "\n",
    "#     # This lambda layer simply stacks outputs so both distances are available to the objective\n",
    "#     stacked_dists = kl.Lambda(lambda vects: K.stack(vects, axis=1), name='stacked_dists')([positive_dist, negative_dist, tertiary_dist])\n",
    "\n",
    "#     model = Model([input_anchor, input_positive, input_negative], stacked_dists, name='triple_siamese')\n",
    "\n",
    "#     v_optimizer = optimizers.Adam(lr=0.001)\n",
    "\n",
    "#     model.compile(optimizer=v_optimizer, loss=triplet_loss, metrics=[accuracy])\n",
    "\n",
    "#     return model\n",
    "# createCNNModel(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def createResNetModel(emb_size):\n",
    "\n",
    "    # Initialize a ResNet50_ImageNet Model\n",
    "    xception_input = kl.Input(shape=(T_G_WIDTH,T_G_HEIGHT,T_G_NUMCHANNELS))\n",
    "    xception_model = keras.applications.Xception(include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_tensor=xception_input,\n",
    "        input_shape=None,\n",
    "        pooling=max,\n",
    "    )\n",
    "\n",
    "    # New Layers over ResNet50\n",
    "    net = xception_model.output\n",
    "    net = kl.GlobalAveragePooling2D(name='gap')(net)\n",
    "    net = kl.Dropout(0.5)(net)\n",
    "    net = kl.Dense(emb_size,activation='relu',name='t_emb_1')(net)\n",
    "    net = kl.Lambda(lambda  x: K.l2_normalize(x,axis=1), name='t_emb_1_l2norm')(net)\n",
    "    \n",
    "    # model creation\n",
    "    base_model = Model(xception_model.input, net, name=\"base_model\")\n",
    "\n",
    "    # triplet framework, shared weights\n",
    "    input_shape=(T_G_WIDTH,T_G_HEIGHT,T_G_NUMCHANNELS)\n",
    "    input_anchor = kl.Input(shape=input_shape, name='input_anchor')\n",
    "    input_positive = kl.Input(shape=input_shape, name='input_pos')\n",
    "    input_negative = kl.Input(shape=input_shape, name='input_neg')\n",
    "    print(type(base_model))\n",
    "    print(type(input_positive))\n",
    "    net_anchor = base_model(input_anchor)\n",
    "    net_positive = base_model(input_positive)\n",
    "    net_negative = base_model(input_negative)\n",
    "\n",
    "    # The Lamda layer produces output using given function. Here its Euclidean distance.\n",
    "    positive_dist = kl.Lambda(euclidean_distance, name='pos_dist')([net_anchor, net_positive])\n",
    "    negative_dist = kl.Lambda(euclidean_distance, name='neg_dist')([net_anchor, net_negative])\n",
    "    tertiary_dist = kl.Lambda(euclidean_distance, name='ter_dist')([net_positive, net_negative])\n",
    "\n",
    "    # This lambda layer simply stacks outputs so both distances are available to the objective\n",
    "    stacked_dists = kl.Lambda(lambda vects: K.stack(vects, axis=1), name='stacked_dists')([positive_dist, negative_dist, tertiary_dist])\n",
    "\n",
    "    model = Model([input_anchor, input_positive, input_negative], stacked_dists, name='triple_siamese')\n",
    "\n",
    "    v_optimizer = optimizers.Adam(lr=0.001)\n",
    "\n",
    "    model.compile(optimizer=v_optimizer, loss=triplet_loss, metrics=[accuracy])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred):\n",
    "    margin = K.constant(1)\n",
    "    return K.mean(K.maximum(K.constant(0), K.square(y_pred[:,0,0]) - 0.5*(K.square(y_pred[:,1,0])+K.square(y_pred[:,2,0])) + margin))\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return K.mean(y_pred[:,0,0] < y_pred[:,1,0])\n",
    "\n",
    "def l2Norm(x):\n",
    "    return  K.l2_normalize(x, axis=-1)\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def t_read_image(loc):\n",
    "    t_image = cv2.imread(loc)\n",
    "    t_image = cv2.resize(t_image, (T_G_HEIGHT,T_G_WIDTH))\n",
    "    t_image = t_image.astype(\"float32\")\n",
    "    t_image = keras.applications.resnet50.preprocess_input(t_image, data_format='channels_last')\n",
    "\n",
    "    return t_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load file names\n",
    "for direc, subdir, file in os.walk(r\"data/food\"):\n",
    "    list_dir = file[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the image\n",
    "img_array = {}\n",
    "for file in tqdm(list_dir):\n",
    "    img = t_read_image(os.path.join(\"data/food\", file))\n",
    "    img_array[file.split(\".jpg\")[0]]=img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "# int is the embedding size \n",
    "# resnet_model = createResNetModel(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_model = createResNetModel(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Getting anchors train ...\")\n",
    "anchors_train = [img_array[img] for img in np.array(train_images[\"A\"])]\n",
    "print(\"Getting positives train ...\")\n",
    "positives_train = [img_array[img] for img in np.array(train_images[\"B\"])]\n",
    "print(\"Getting negatives train ...\")\n",
    "negatives_train = [img_array[img] for img in np.array(train_images[\"C\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Getting anchors test ...\")\n",
    "anchors_test = [img_array[img] for img in np.array(test_images[\"A\"])]\n",
    "print(\"Getting positives test ...\")\n",
    "positives_test = [img_array[img] for img in np.array(test_images[\"B\"])]\n",
    "print(\"Getting negatives test ...\")\n",
    "negatives_test = [img_array[img] for img in np.array(test_images[\"C\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numepochs = 1\n",
    "total_t_ch = int(np.ceil(len(anchors_train) / float(CHUNKSIZE)))\n",
    "total_v_ch = int(np.ceil(len(anchors_test) / float(CHUNKSIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for e in range(0, numepochs):\n",
    "    for t in range(0, total_t_ch):\n",
    "        print (\"Epoch :{}, train chunk {}/{}\".format(e,t+1,total_t_ch))\n",
    "        anchors_t = anchors_train[t*CHUNKSIZE:(t+1)*CHUNKSIZE]\n",
    "        positives_t =positives_train[t*CHUNKSIZE:(t+1)*CHUNKSIZE]\n",
    "        negatives_t = negatives_train[t*CHUNKSIZE:(t+1)*CHUNKSIZE]\n",
    "        Y_train = np.random.randint(2, size=(1,2,len(anchors_t))).T\n",
    "        # This method does NOT use data augmentation\n",
    "        cnn_model.fit([anchors_t, positives_t, negatives_t], Y_train, epochs=1,  batch_size=BATCHSIZE)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In case the validation images don't fit in memory, we load chunks from disk again. \n",
    "val_res = [0.0, 0.0]\n",
    "total_w = 0.0\n",
    "for v in range(0, total_v_ch):\n",
    "\n",
    "    print('Loading validation image lists ...')\n",
    "    print (\"Epoch :{}, train chunk {}/{}...\".format(e,v+1,total_v_ch))\n",
    "    anchors_v = anchors_test[v*CHUNKSIZE:(v+1)*CHUNKSIZE]\n",
    "    positives_v =positives_test[v*CHUNKSIZE:(v+1)*CHUNKSIZE]\n",
    "    negatives_v = negatives_test[v*CHUNKSIZE:(v+1)*CHUNKSIZE]\n",
    "    Y_val = np.random.randint(2, size=(1,2,len(anchors_v))).T\n",
    "\n",
    "    # Weight of current validation measurement. \n",
    "    # if loaded expected number of items, this will be 1.0, otherwise < 1.0, and > 0.0.\n",
    "    w = float(anchors_v.shape[0]) / float(CHUNKSIZE)\n",
    "    total_w = total_w + w\n",
    "\n",
    "    curval = model.evaluate([anchors_v, positives_v, negatives_v], Y_val, batch_size=BATCHSIZE)\n",
    "    val_res[0] = val_res[0] + w*curval[0]\n",
    "    val_res[1] = val_res[1] + w*curval[1]\n",
    "\n",
    "val_res = [x / total_w for x in val_res]\n",
    "\n",
    "print('Validation Results: ', str(val_res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = cnn_model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"Xception.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
