{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\josep\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\josep\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\josep\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import ADASYN, SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data\\\\train.csv\")\n",
    "df_val = pd.read_csv(\"data\\\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>112000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.037616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.190267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Active\n",
       "count  112000.000000\n",
       "mean        0.037616\n",
       "std         0.190267\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         0.000000\n",
       "max         1.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# very imbalanced!\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split strings into amino acids sequences\n",
    "X_train = df_train[\"Sequence\"].values\n",
    "X_train = [list(X_train[i]) for i in range(len(X_train))]\n",
    "y_train = df_train[\"Active\"].values\n",
    "X_val = df_val[\"Sequence\"].values\n",
    "X_val = [list(X_val[i]) for i in range(len(X_val))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage active mutations :  3.76  %\n"
     ]
    }
   ],
   "source": [
    "# percentage active \n",
    "print(\"Percentage active mutations : \",np.around(sum(y_train)/len(y_train)*100,2),\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the mutations, taking into consideration mutation position\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(X_train)\n",
    "X_train_onehot = enc.transform(X_train).toarray()\n",
    "X_val_onehot = enc.transform(X_val).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the input\n",
    "#scaler = StandardScaler()\n",
    "scaler = PowerTransformer()\n",
    "X_train_scaled =scaler.fit_transform(X_train_onehot)\n",
    "X_val_scaled = scaler.transform(X_val_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define f1 score, precision and recall for keras to be able to follow real time\n",
    "# taken from https://medium.com/@aakashgoel12/how-to-add-user-defined-function-get-f1-score-in-keras-metrics-3013f979ce0d\n",
    "# and https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
    "def get_f1(y_true, y_pred): \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct ANN to perform binary classification\n",
    "def get_ANN(X_train_scaled,y_train,n_layers,hidden_units):\n",
    "    \"\"\"Constructs an ANN model to perform binary classification\n",
    "    \n",
    "    Args: X_train_scaled (np.ndarray): scaled array of one hot encoded AA mutation sequences \n",
    "        y_train (np.ndarray): labels (active or inactive)\n",
    "        n_layers (int): number of hidden layers for the ANN\n",
    "        hidden_units (int): number of units per hidden layer\n",
    "        \n",
    "    Returns: model (keras.models.Sequential): trained ANN \n",
    "    \"\"\"\n",
    "    print(\"Starting train_test_split\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_train_scaled, y_train, test_size=0.15, random_state=42, shuffle=True\n",
    "    )   \n",
    "    \n",
    "    print(\"Resampling to account for imbalance in data\")\n",
    "    sampler = ADASYN()\n",
    "    X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # ANN architecture definition\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_units, activation=\"relu\", input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dropout(0.5))\n",
    "    for i in range(1,n_layers):\n",
    "        model.add(Dense(hidden_units, activation=\"relu\"))\n",
    "        model.add(Dropout(0.5))\n",
    "    model.add(Dense(1,activation=\"sigmoid\"))\n",
    "    \n",
    "    # use Adam as optimizer\n",
    "    opt = Adam()\n",
    "    \n",
    "    model.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[precision_m, recall_m, get_f1])\n",
    "    model.fit(X_train_res,y_train_res,epochs=65,batch_size=32)\n",
    "    \n",
    "    # evaluate the model on test set\n",
    "    score = model.evaluate(X_test, y_test, batch_size=64)\n",
    "    print(\"Loss, precision, recall, F1 : \",score)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting train_test_split\n",
      "Resampling to account for imbalance in data\n",
      "Epoch 1/65\n",
      "183211/183211 [==============================] - 9s 49us/step - loss: 0.1265 - precision_m: 0.9412 - recall_m: 0.9665 - get_f1: 0.9522\n",
      "Epoch 2/65\n",
      "183211/183211 [==============================] - 9s 52us/step - loss: 0.0526 - precision_m: 0.9768 - recall_m: 0.9910 - get_f1: 0.9833\n",
      "Epoch 3/65\n",
      "183211/183211 [==============================] - 10s 53us/step - loss: 0.0410 - precision_m: 0.9817 - recall_m: 0.9935 - get_f1: 0.9872\n",
      "Epoch 4/65\n",
      "183211/183211 [==============================] - 10s 53us/step - loss: 0.0351 - precision_m: 0.9840 - recall_m: 0.9948 - get_f1: 0.9891\n",
      "Epoch 5/65\n",
      "183211/183211 [==============================] - 10s 54us/step - loss: 0.0326 - precision_m: 0.9854 - recall_m: 0.9955 - get_f1: 0.9901\n",
      "Epoch 6/65\n",
      "183211/183211 [==============================] - 10s 55us/step - loss: 0.0302 - precision_m: 0.9868 - recall_m: 0.9960 - get_f1: 0.9911\n",
      "Epoch 7/65\n",
      "183211/183211 [==============================] - 10s 55us/step - loss: 0.0276 - precision_m: 0.9878 - recall_m: 0.9962 - get_f1: 0.9917\n",
      "Epoch 8/65\n",
      "183211/183211 [==============================] - 10s 55us/step - loss: 0.0259 - precision_m: 0.9888 - recall_m: 0.9962 - get_f1: 0.9922\n",
      "Epoch 9/65\n",
      "183211/183211 [==============================] - 10s 55us/step - loss: 0.0251 - precision_m: 0.9889 - recall_m: 0.9969 - get_f1: 0.9926\n",
      "Epoch 10/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0240 - precision_m: 0.9896 - recall_m: 0.9970 - get_f1: 0.9931\n",
      "Epoch 11/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0238 - precision_m: 0.9899 - recall_m: 0.9973 - get_f1: 0.9934\n",
      "Epoch 12/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0230 - precision_m: 0.9898 - recall_m: 0.9970 - get_f1: 0.9932\n",
      "Epoch 13/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0221 - precision_m: 0.9905 - recall_m: 0.9974 - get_f1: 0.9937\n",
      "Epoch 14/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0210 - precision_m: 0.9910 - recall_m: 0.9975 - get_f1: 0.9941\n",
      "Epoch 15/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0202 - precision_m: 0.9913 - recall_m: 0.9975 - get_f1: 0.9942\n",
      "Epoch 16/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0206 - precision_m: 0.9912 - recall_m: 0.9975 - get_f1: 0.9942\n",
      "Epoch 17/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0199 - precision_m: 0.9916 - recall_m: 0.9978 - get_f1: 0.9945\n",
      "Epoch 18/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0190 - precision_m: 0.9919 - recall_m: 0.9978 - get_f1: 0.9947\n",
      "Epoch 19/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0202 - precision_m: 0.9916 - recall_m: 0.9980 - get_f1: 0.9946\n",
      "Epoch 20/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0192 - precision_m: 0.9918 - recall_m: 0.9978 - get_f1: 0.9946\n",
      "Epoch 21/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0179 - precision_m: 0.9927 - recall_m: 0.9981 - get_f1: 0.9953\n",
      "Epoch 22/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0190 - precision_m: 0.9922 - recall_m: 0.9981 - get_f1: 0.9950\n",
      "Epoch 23/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0178 - precision_m: 0.9928 - recall_m: 0.9980 - get_f1: 0.9953\n",
      "Epoch 24/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0170 - precision_m: 0.9930 - recall_m: 0.9981 - get_f1: 0.9954\n",
      "Epoch 25/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0172 - precision_m: 0.9929 - recall_m: 0.9981 - get_f1: 0.9953\n",
      "Epoch 26/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0177 - precision_m: 0.9928 - recall_m: 0.9981 - get_f1: 0.9953\n",
      "Epoch 27/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0163 - precision_m: 0.9933 - recall_m: 0.9983 - get_f1: 0.9956\n",
      "Epoch 28/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0163 - precision_m: 0.9935 - recall_m: 0.9982 - get_f1: 0.9957\n",
      "Epoch 29/65\n",
      "183211/183211 [==============================] - 10s 57us/step - loss: 0.0168 - precision_m: 0.9933 - recall_m: 0.9983 - get_f1: 0.9956\n",
      "Epoch 30/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0159 - precision_m: 0.9934 - recall_m: 0.9983 - get_f1: 0.9957\n",
      "Epoch 31/65\n",
      "183211/183211 [==============================] - 10s 57us/step - loss: 0.0158 - precision_m: 0.9938 - recall_m: 0.9984 - get_f1: 0.9960\n",
      "Epoch 32/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0156 - precision_m: 0.9936 - recall_m: 0.9985 - get_f1: 0.9959\n",
      "Epoch 33/65\n",
      "183211/183211 [==============================] - 11s 59us/step - loss: 0.0156 - precision_m: 0.9936 - recall_m: 0.9983 - get_f1: 0.9958\n",
      "Epoch 34/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0159 - precision_m: 0.9934 - recall_m: 0.9983 - get_f1: 0.9957\n",
      "Epoch 35/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0154 - precision_m: 0.9938 - recall_m: 0.9985 - get_f1: 0.9960\n",
      "Epoch 36/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0168 - precision_m: 0.9935 - recall_m: 0.9985 - get_f1: 0.9958\n",
      "Epoch 37/65\n",
      "183211/183211 [==============================] - 10s 57us/step - loss: 0.0150 - precision_m: 0.9943 - recall_m: 0.9986 - get_f1: 0.9963\n",
      "Epoch 38/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0153 - precision_m: 0.9937 - recall_m: 0.9984 - get_f1: 0.9959\n",
      "Epoch 39/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0145 - precision_m: 0.9943 - recall_m: 0.9987 - get_f1: 0.9964\n",
      "Epoch 40/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0145 - precision_m: 0.9943 - recall_m: 0.9986 - get_f1: 0.9963\n",
      "Epoch 41/65\n",
      "183211/183211 [==============================] - 10s 57us/step - loss: 0.0143 - precision_m: 0.9942 - recall_m: 0.9986 - get_f1: 0.9963\n",
      "Epoch 42/65\n",
      "183211/183211 [==============================] - 10s 57us/step - loss: 0.0138 - precision_m: 0.9946 - recall_m: 0.9987 - get_f1: 0.9965\n",
      "Epoch 43/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0148 - precision_m: 0.9946 - recall_m: 0.9985 - get_f1: 0.9964\n",
      "Epoch 44/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0141 - precision_m: 0.9942 - recall_m: 0.9988 - get_f1: 0.9964\n",
      "Epoch 45/65\n",
      "183211/183211 [==============================] - 9s 51us/step - loss: 0.0141 - precision_m: 0.9945 - recall_m: 0.9987 - get_f1: 0.9965\n",
      "Epoch 46/65\n",
      "183211/183211 [==============================] - 9s 51us/step - loss: 0.0138 - precision_m: 0.9948 - recall_m: 0.9989 - get_f1: 0.9967\n",
      "Epoch 47/65\n",
      "183211/183211 [==============================] - 9s 51us/step - loss: 0.0134 - precision_m: 0.9948 - recall_m: 0.9987 - get_f1: 0.9966\n",
      "Epoch 48/65\n",
      "183211/183211 [==============================] - 9s 50us/step - loss: 0.0140 - precision_m: 0.9949 - recall_m: 0.9989 - get_f1: 0.9968\n",
      "Epoch 49/65\n",
      "183211/183211 [==============================] - 9s 50us/step - loss: 0.0154 - precision_m: 0.9942 - recall_m: 0.9989 - get_f1: 0.9964\n",
      "Epoch 50/65\n",
      "183211/183211 [==============================] - 9s 50us/step - loss: 0.0142 - precision_m: 0.9945 - recall_m: 0.9986 - get_f1: 0.9964\n",
      "Epoch 51/65\n",
      "183211/183211 [==============================] - 9s 50us/step - loss: 0.0136 - precision_m: 0.9949 - recall_m: 0.9989 - get_f1: 0.9968\n",
      "Epoch 52/65\n",
      "183211/183211 [==============================] - 9s 51us/step - loss: 0.0142 - precision_m: 0.9944 - recall_m: 0.9988 - get_f1: 0.9965\n",
      "Epoch 53/65\n",
      "183211/183211 [==============================] - 9s 51us/step - loss: 0.0135 - precision_m: 0.9949 - recall_m: 0.9990 - get_f1: 0.9968\n",
      "Epoch 54/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0132 - precision_m: 0.9949 - recall_m: 0.9988 - get_f1: 0.9968\n",
      "Epoch 55/65\n",
      "183211/183211 [==============================] - 11s 59us/step - loss: 0.0130 - precision_m: 0.9948 - recall_m: 0.9989 - get_f1: 0.9968\n",
      "Epoch 56/65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183211/183211 [==============================] - 10s 53us/step - loss: 0.0132 - precision_m: 0.9948 - recall_m: 0.9989 - get_f1: 0.9968\n",
      "Epoch 57/65\n",
      "183211/183211 [==============================] - 10s 57us/step - loss: 0.0133 - precision_m: 0.9951 - recall_m: 0.9989 - get_f1: 0.9969\n",
      "Epoch 58/65\n",
      "183211/183211 [==============================] - 10s 55us/step - loss: 0.0134 - precision_m: 0.9950 - recall_m: 0.9989 - get_f1: 0.9968\n",
      "Epoch 59/65\n",
      "183211/183211 [==============================] - 10s 57us/step - loss: 0.0137 - precision_m: 0.9949 - recall_m: 0.9988 - get_f1: 0.9967\n",
      "Epoch 60/65\n",
      "183211/183211 [==============================] - 10s 55us/step - loss: 0.0133 - precision_m: 0.9950 - recall_m: 0.9988 - get_f1: 0.9968\n",
      "Epoch 61/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0131 - precision_m: 0.9952 - recall_m: 0.9988 - get_f1: 0.9969\n",
      "Epoch 62/65\n",
      "183211/183211 [==============================] - 10s 56us/step - loss: 0.0135 - precision_m: 0.9949 - recall_m: 0.9988 - get_f1: 0.9967\n",
      "Epoch 63/65\n",
      "183211/183211 [==============================] - 11s 58us/step - loss: 0.0127 - precision_m: 0.9952 - recall_m: 0.9990 - get_f1: 0.9970\n",
      "Epoch 64/65\n",
      "183211/183211 [==============================] - 11s 59us/step - loss: 0.0127 - precision_m: 0.9953 - recall_m: 0.9989 - get_f1: 0.9970\n",
      "Epoch 65/65\n",
      "183211/183211 [==============================] - 10s 57us/step - loss: 0.0137 - precision_m: 0.9950 - recall_m: 0.9989 - get_f1: 0.9969\n",
      "16800/16800 [==============================] - 0s 12us/step\n",
      "Loss, precision, recall, F1 :  [0.08547422183353963, 0.8178709149360657, 0.8422324657440186, 0.8168222904205322]\n"
     ]
    }
   ],
   "source": [
    "# train the ANN\n",
    "## beware the real time loss, precision, recall and F1 are calculated on batches so are not accurate\n",
    "model = get_ANN(X_train_scaled, y_train, 3, 125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform predictions\n",
    "y_pred = np.around(model.predict(X_val_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "np.savetxt(\"predictions.csv\", y_pred, fmt=\"%i\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
